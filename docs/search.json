[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AutoDiff",
    "section": "",
    "text": "Vorwort"
  },
  {
    "objectID": "index.html#danksagung",
    "href": "index.html#danksagung",
    "title": "AutoDiff",
    "section": "Danksagung",
    "text": "Danksagung"
  },
  {
    "objectID": "intro.html#ableitungen-von-funktionen",
    "href": "intro.html#ableitungen-von-funktionen",
    "title": "1  Ableitungen und ihre Anwendungen",
    "section": "1.2 Ableitungen von Funktionen",
    "text": "1.2 Ableitungen von Funktionen\nWir kennen Ableitungen von Funktionen \\(f: \\mathbb{R}\\rightarrow\\mathbb{R}\\) aus dem Mathematikunterricht. Sie geben uns darüber Auskunft, wie gross die Steigung der Tangente in einem bestimmten Punkt des Funktionsgraphen ist. Die Tangente stellt dabei die beste lineare Annäherung an den Funktionsgraph dar. Ableitungen beschreiben auch die lokale Änderungsrate der Funktion. Ableitungen erlauben es uns ausserdem, die Extrema und Wendepunkte einer Funktion zu bestimmen.\nDie folgende Tabelle fasst die bekannten Ableitungen der Grundfunktionen zusammen.\n\n\nTabelle 1.1: Ableitungen der Grundfunktionen\n\n\n\n\n\n\n\n\n\n\\(f(x)\\)\n\\(f'(x)\\)\n\n\\(f(x)\\)\n\\(f'(x)\\)\n\n\n\n\n\\(x^n\\)\n\\(n \\cdot x^{n-1} \\quad (n\\in\\mathbb{R})\\)\n\n\\(\\sqrt{x}\\)\n\\(\\frac{1}{2\\cdot\\sqrt{x}}\\)\n\n\n\\(e^x\\)\n\\(e^x\\)\n\n\\(a^x\\)\n\\(a^x \\cdot \\ln(a) \\quad (a&gt;0, a\\ne 1)\\)\n\n\n\\(\\ln(x)\\)\n\\(\\frac{1}{x}\\)\n\n\\(\\log_a(x)\\)\n\\(\\frac{1}{x\\cdot\\ln(a)} \\quad (a&gt;0, a\\ne 1)\\)\n\n\n\\(\\sin(x)\\)\n\\(\\cos(x)\\)\n\n\\(\\arcsin(x)\\)\n\\(\\frac{1}{\\sqrt{1-x^2}}\\)\n\n\n\\(\\cos(x)\\)\n\\(-\\sin(x)\\)\n\n\\(\\arccos(x)\\)\n\\(-\\frac{1}{\\sqrt{1-x^2}}\\)\n\n\n\\(\\tan(x)\\)\n\\(\\frac{1}{\\cos^2(x)} = 1 + \\tan^2(x)\\)\n\n\\(\\arctan(x)\\)\n\\(\\frac{1}{x^2+1}\\)\n\n\n\\(\\sinh(x)\\)\n\\(\\cosh(x)\\)\n\n\\(\\textrm{arsinh}(x)\\)\n\\(\\frac{1}{\\sqrt{x^2+1}}\\)\n\n\n\\(\\cosh(x)\\)\n\\(\\sinh(x)\\)\n\n\\(\\textrm{arcosh(x)}\\)\n\\(\\frac{1}{\\sqrt{x^2-1}}\\)\n\n\n\\(\\tanh(x)\\)\n\\(\\frac{1}{\\cosh^2(x)} = 1 - \\tanh^2(x)\\)\n\n\\(\\textrm{artanh(x)}\\)\n\\(-\\frac{1}{x^2-1}\\)\n\n\n\n\nNeue Funktionen erhält man, indem man die Grundfunktionen aus Tabelle 1.1 addiert, subtrahiert, multipliziert, dividiert und komponiert, d.h. Verkettungen der Form \\((f\\circ g)(x) = f(g(x))\\) bildet. Um solche Funktionen abzuleiten, brauchen wir die Regeln aus Tabelle 1.2. Mit diesen Regeln sind wir dann schon in der Lage, alle differenzierbaren Funktionen abzuleiten.\n\n\nTabelle 1.2: Ableitungsregeln\n\n\n\n\n\n\nRegel\nFormel\n\n\n\n\nSummenregel\n\\(\\frac{d}{dx}(f(x)\\pm g(x)) = f'(x) \\pm g'(x)\\)\n\n\nProduktregel\n\\(\\frac{d}{dx}(f(x)\\cdot g(x)) = f'(x)\\cdot g(x) + f(x) \\cdot g'(x)\\)\n\n\nSpezialfall: Faktorregel\n\\(\\frac{d}{dx}(a\\cdot f(x)) = a\\cdot f'(x)\\)\n\n\nQuotientenregel\n\\(\\frac{d}{dx}\\frac{f(x)}{g(x)} = \\frac{f'(x)\\cdot g(x) - f(x) \\cdot g'(x)}{g(x)^2}\\)\n\n\nKettenregel\n\\(\\frac{d}{dx} f(g(x)) = f'(g(x))\\cdot g'(x)\\)\n\n\n\n\nAn dieser Stelle sei noch angemerkt, dass sich der Begriff der Ableitung sinngemäss auf Funktionen \\(f: \\mathbb{R}^n \\rightarrow\\mathbb{R}^m\\) verallgemeinern lässt. Eine kurze Beschreibung der Grundidee findet sich in Slater (2022). Weitergehende Informationen findet man z.B. in Arens u. a. (2022) oder in jedem Lehrbuch zur Analysis 2. Wir werden im Kapitel 4 darauf zurück kommen."
  },
  {
    "objectID": "intro.html#sec-ProgFunc",
    "href": "intro.html#sec-ProgFunc",
    "title": "1  Ableitungen und ihre Anwendungen",
    "section": "1.3 Programme als Funktionen",
    "text": "1.3 Programme als Funktionen\nProgramme, die numerische Werte einlesen und numerische Werte ausgeben, können als mathematische Funktionen betrachtet werden. Wir beschränken uns zunächst auf Programme, die nur ein Argument erhalten und nur einen Rückgabewert liefern.\n\nBeispiel 1.1 (Eine Funktion als Programm) \nBetrachte das folgende Programm:\n\ndef f(x):\n    y = (2 + x) * (x - 3)\n    return y\n\nx0 = 2\nprint( f(x0) )\n\nDiese Python-Funktion entspricht der Funktion \\(f:\\mathbb{R}\\rightarrow\\mathbb{R} , x \\mapsto y=(2+x)(x-3)\\) im Sinne der Mathematik. Natürlich kann der Funktionskörper viel komplizierter aufgebaut sein und z.B. Schleifen und Bedingungen enthalten.\nUm zu verstehen, wie der Computer einen Ausdruck wie y = (2 + x) * (x - 3) auswertet, ist es hilfreich, ihn als Baum (im Sinne der Graphentheorie) darzustellen. Ausdrucksbäume sind ein Spezialfall von so genannten computational graphs und werden z.B. in Hromkovic u. a. (2021) erklärt.\n\n\n\n\n\n\n   \n\nnx\n\nx   \n\nnPlus\n\n +   \n\nnx-&gt;nPlus\n\n    \n\nnMinus\n\n -   \n\nnx-&gt;nMinus\n\n    \n\nn2\n\n2   \n\nn2-&gt;nPlus\n\n    \n\nn3\n\n3   \n\nn3-&gt;nMinus\n\n    \n\nnMult\n\n *   \n\nnPlus-&gt;nMult\n\n    \n\nnMinus-&gt;nMult\n\n    \n\nny\n\ny   \n\nnMult-&gt;ny\n\n   \n\n\nAbbildung 1.1: Ausdrucksbaum zum Ausdruck y = (2 + x) * (x - 3).\n\n\n\n\nWir wollen nun unsere Python-Funktion so umschreiben, dass diese Struktur auch im Funktionskörper sichtbar wird. Dazu führen wir drei Hilfsvariablen v0, v1, v2 ein.\n\ndef f(x):\n    v0 = x\n    v1 = 2 + v0\n    v2 = v0 - 3\n    y = v1 * v2\n    return y\n\n\n\n\n\n\n\n\n\nKonvention\n\n\n\nEine Funktion berechnet aus einem Argument x einen Rückgabewert y über eine Reihe von Hilfsvariablen v, die mit aufsteigenden Indizes versehen sind. Dabei setzen wir am Anfang immer v0 = x.\n\n\n\nÜbungsaufgabe 1.2 (Kostenfunktin gemäss Konvention) \nSchreibe das Programm aus Übungsaufgabe 1.1 gemäss der obigen Konvention um.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\nimport math\n\ndef kosten(x):\n    v0 = x       # sX\n    v1 = 16 - v0 # d\n    v2 = v1**2 + 12**2\n    v3 = math.sqrt(v2) # sLand\n    v4 = 15000 * v0 # kKueste\n    v5 = 25000 * v3 # kLand\n    y = v4 + v5\n    return y\n    \nx0 = 8\nkGesamt = kosten(x0)\nprint(\"Mit x =\", x0, \"betragen die Kosten\", kGesamt, \"Euro.\")\n\n\nMit x = 8 betragen die Kosten 480555.1275463989 Euro.\n\n\n\n\n\n\nÜbungsaufgabe 1.3 (Programm in Funktion übersetzen) \nSchreibe die mathematische Funktion auf, die durch das folgende Programm berechnet wird.\n\nimport math\n\ndef f(x):\n    v0 = x\n    v1 = v0 ** 2\n    v2 = v1 + 2\n    v3 = -v1 / 2\n    v4 = math.cos(v2)\n    v5 = math.exp(v3)\n    v6 = v4 * v5\n    y = v6 + 1 / v0\n    return y \n\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\\[\\begin{align*}\nv_1 & = x^2 \\\\\nv_2 & = x^2 + 2 \\\\\nv_3 & = - \\frac{x^2}{2} \\\\\nv_4 & = \\cos(x^2 + 2) \\\\\nv_5 & = e^{- \\frac{x^2}{2}} \\\\\nv_6 & = \\cos(x^2 + 2) \\cdot e^{- \\frac{x^2}{2}} \\\\\ny & = f(x) = \\cos(x^2 + 2) \\cdot e^{- \\frac{x^2}{2}} + \\frac{1}{x}\n\\end{align*}\\]\n\n\n\n\nÜbungsaufgabe 1.4 (Funktion in Graph und Programm übersetzen) \nSchreibe zur mathematischen Funktion \\(y = f(x) = \\frac{\\ln(x^2 + 1)}{\\sqrt{x^2 + 1 + x}}\\) den Ausdrucksbaum auf. Übersetze den Ausdruck anschliessend in eine Python-Funktion gemäss der Konvention.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\n\n\n\n\n   \n\nnx\n\nx   \n\nnPlus2\n\n +   \n\nnx-&gt;nPlus2\n\n    \n\nnPow\n\n ^   \n\nnx-&gt;nPow\n\n    \n\nn2\n\n2   \n\nn2-&gt;nPow\n\n    \n\nn1\n\n1   \n\nnPlus1\n\n +   \n\nn1-&gt;nPlus1\n\n    \n\nnPlus1-&gt;nPlus2\n\n    \n\nnLog\n\n ln( )   \n\nnPlus1-&gt;nLog\n\n    \n\nnSqrt\n\n sqrt( )   \n\nnPlus2-&gt;nSqrt\n\n    \n\nnPow-&gt;nPlus1\n\n    \n\nnFrac\n\n /   \n\nny\n\ny   \n\nnFrac-&gt;ny\n\n    \n\nnLog-&gt;nFrac\n\n    \n\nnSqrt-&gt;nFrac\n\n   \n\n\nAbbildung 1.2: Computational Graph zum Ausdruck y = ln(x^2 + 1) / sqrt(x^2 + 1 + x).\n\n\n\n\n\n\nCode\nimport math\n\ndef f(x):\n    v0 = x\n    v1 = v0 ** 2\n    v2 = v1 + 1\n    v3 = v2 + v0\n    v4 = math.log(v2)\n    v5 = math.sqrt(v3)\n    y = v4 / v5\n    return y\n\n\nNatürlich hätte man z.B. v3 und v4 auch vertauschen können.\n\n\n\n\nÜbungsaufgabe 1.5 (Ein Programm mit einer Schleife) \nBetrachte das folgende Programm:\n\ndef f(x):\n    v0 = x\n    for i in range(2):\n        v0 = v0 ** 2 + 1\n    y = v0\n    return y\n\nErsetze im Funktionskörper die Schleife durch mehrere Befehle, so dass immer noch der gleiche mathematische Ausdruck berechnet wird und unsere Konvention eingehalten wird. Welche mathematische Funktion wird durch die Python-Funktion berechnet? Was ändert sich, wenn stattdessen for i in range(3) oder for i in range(4) stehen würde?\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nFür jeden Schleifendurchgang benötigen wir eine neue Hilfsvariable. Die Funktion, die dabei entsteht, kann geschrieben werden als \\(f(x) = (\\ell \\circ \\ell \\circ \\ldots \\circ \\ell)(x)\\), wobei \\(\\ell(x) = x^2 + 1\\) ist.\n\nrange(2)range(3)range(4)\n\n\n\ndef f(x):\n    v0 = x\n    v1 = v0 ** 2 + 1\n    v2 = v1 ** 2 + 1\n    y = v2\n    return y\n\n\\[\\begin{align*}\n    f(x) &= \\ell(\\ell(x)) \\\\\n         &= (x^2 + 1)^2 + 1 = x^4 + 2x^2 + 2\n\\end{align*}\\]\n\n\n\ndef f(x):\n    v0 = x\n    v1 = v0 ** 2 + 1\n    v2 = v1 ** 2 + 1\n    v3 = v2 ** 2 + 1\n    y = v3\n    return y\n\n\\[\\begin{align*}\n    f(x) &= \\ell(\\ell(\\ell(x))) \\\\\n         &= ((x^2 + 1)^2 + 1)^2 + 1 = x^8 + 4x^6 + 8x^4 + 8x^2 + 5\n\\end{align*}\\]\n\n\n\ndef f(x):\n    v0 = x\n    v1 = v0 ** 2 + 1\n    v2 = v1 ** 2 + 1\n    v3 = v2 ** 2 + 1\n    v4 = v3 ** 2 + 1\n    y = v4\n    return y\n\n\\[\\begin{align*}\n    f(x) &= \\ell(\\ell(\\ell(\\ell(x)))) \\\\\n         &= (((x^2 + 1)^2 + 1)^2 + 1)^2 + 1 \\\\\n         &= x^{16} + 8x^{14} + 32x^{12} + 80x^{10} + 138x^8 + 168x^6 + 144x^4 + 80x^2 + 26\n\\end{align*}\\]"
  },
  {
    "objectID": "intro.html#unser-ziel-programme-ableiten",
    "href": "intro.html#unser-ziel-programme-ableiten",
    "title": "1  Ableitungen und ihre Anwendungen",
    "section": "1.4 Unser Ziel: Programme ableiten",
    "text": "1.4 Unser Ziel: Programme ableiten\nWie eingangs erwähnt wurde, haben Ableitungen viele nützliche Anwendungen.\nWir möchten nun Ableitungen von Funktionen berechnen, die durch Programme beschrieben werden, die wie oben einen numerischen Parameter x als Input erhalten und einen numerischen Wert y zurückliefern. Unser Ziel wird es sein, die Programme so zu modifizieren, dass der Funktionsaufruf f(x0) nicht nur den Funktionswert \\(f(x_0)\\) zurückgibt, sondern auch den Wert der Ableitung \\(f'(x_0)\\). Wir sind dabei nicht an einer symbolischen Ableitung interessiert, wie das z.B. GeoGebra oder Mathematica machen (s. Kapitel 2.2), sondern nur an einer punktweisen Auswertung. Natürlich wollen wir die Ableitungsfunktion auch nicht von Hand bestimmen. Wir wollen uns aber auch nicht bloss mit einer Annhäerung des Wertes der Ableitung zufrieden geben (s. Kapitel 2.1), sondern den Wert von \\(f'(x_0)\\) bis auf Maschinengenauigkeit exakt berechnen. In Kapitel 3 werden wir eine Methode kennen lernen, die all dies leistet und dabei die Laufzeit eines Programms nicht wesentlich erhöht. Der Name dieser Methode: Algorithmische Differentiation (AD), obwohl die Namensgebung hier nicht eindeutig ist:\n\nOne of the obstacles in this area [of computing derivatives], which involves “symbolic” and “numerical” methods, has been a confusion in terminology […]. There is not even general agreement on the best name for the field, which is frequently referred to as automatic or computational differentiation in the literature. For this book the adjective algorithmic seemed preferable, because much of the material emphasizes algorithmic structure, sometimes glossing over the details and pitfalls of actual implementations. (Aus dem Vorwort zu Griewank und Walther (2008))\n\nBevor wir uns aber der Methode der algorithmischen Differentiation zuwenden, wollen wir sie durch einige Beispiele motivieren, bei denen die Berechnung von Ableitung von Funktionen und Programmen eine zentrale Rolle spielt: Das Newtonverfahren und das Gradient Descent Verfahren.\n\n1.4.1 Das Newtonverfahren zur Berechnung von Nullstellen\nIn vielen Anwendungen steht man vor der Aufgabe, die Gleichung \\(f(x) = 0\\) nach \\(x\\) aufzulösen, d.h. eine Nullstelle \\(\\bar{x}\\) der Funktion zu finden. Oft ist es aber nicht möglich, die Lösung einer solchen Gleichung in geschlossener Form darzustellen. Um dennoch eine Lösung zumindest näherungsweise berechnen zu können, kann man folgendermassen vorgehen:\n\nWähle einen Startwert \\(x_0\\), der in der Nähe einer Nullstelle \\(\\bar{x}\\) von \\(f\\) liegt.\nIm Kurvenpunkt \\((x_0 | y_0)\\) wird die Tangente an die Kurve \\(f\\) gelegt. Deren Schnittpunkt \\(x_1\\) mit der \\(x\\)-Achse liegt in der Regel näher bei \\(\\bar{x}\\) als \\(x_0\\).\nNun wiederholt man das Verfahren, indem man bei \\(x_1\\) die Tangente an die Kurve legt, usw. Auf diese Weise erhält man eine Folge von Näherungen \\(x_0, x_1, x_2, \\ldots\\), deren Grenzwert die Nullstelle \\(\\bar{x}\\) ist.\n\nDieser Algorithmus ist als Newtonverfahren bekannt.\n\n\n\n\n\n\nDie Gleichung der Tangente im Punkt \\((x_n | y_n) = (x_n | f(x_n))\\) ist bekanntlich \\(t(x) = f(x_n) + f'(x_n) \\cdot (x - x_n)\\). Die Nullstelle der Tangente ist der Näherungswert \\(x_{n+1}\\). Aus \\(t(x_{n+1}) = 0\\) ergibt sich nun die Iterationsvorschrift des Newtonverfahrens: \\[\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n\\tag{1.1}\\]\n\nÜbungsaufgabe 1.5 (Das Newtonverfahren programmieren) \nSchreibe ein Programm, das mit Hilfe des Newtonverfahrens (Gleichung 1.1) eine Nullstelle der Funktion \\(f(x) = \\frac{1}{31} x^3 -\\frac{1}{20} x^2 -x + 1\\) berechnet. Verwende den Startwert \\(x_0 = -2\\). Du kannst abbrechen, wenn die Differenz \\(|x_{n+1} - x_n|\\) kleiner als eine bestimmte Toleranz wird, z.B. kleiner als tol = 1e-6. Wie flexibel ist dein Programm einsetzbar? Überlege dir z.B., wie viele Änderungen du vornehmen müsstest, wenn du die Nullstelle einer anderen Funktion berechnen müsstest.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nWelche der folgenden Lösungsvorschläge kommt deinem Programm am nächsten?\n\nVersion 1Version 2Version 3Version 4\n\n\n\nfrom math import fabs\n\nx0 = -2\ntol = 1e-6\n# Erster Schritt berechnen\nx1 = x0 - (1/31 * x0**3 - 1/20 * x0**2 - x0 + 1) / (3/31 * x0**2 - 1/10 * x0 - 1)\nwhile fabs(x1 - x0) > tol:\n    x0 = x1\n    x1 = x0 - (1/31 * x0**3 - 1/20 * x0**2 - x0 + 1) / (3/31 * x0**2 - 1/10 * x0 - 1)\nprint(x1)\n\n5.908619865450271\n\n\nDas Newtonverfahren wird als main-Funktion (d.h. im Hauptprogramm) ausgeführt. Braucht man jedoch die Nullstelle einer anderen Funktion, dann muss ein neues Programm geschrieben werden. Die Ableitung wurde von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef f(x):\n    y = 1/31 * x**3 - 1/20 * x**2 - x + 1\n    return y\n\ndef fdot(x):\n    ydot = 3/31 * x**2 - 1/10 * x - 1\n    return ydot\n\nx0 = -2\ntol = 1e-6\n# Erster Schritt berechnen\nx1 = x0 - f(x0) / fdot(x0)\nwhile fabs(x1 - x0) > tol:\n    x0 = x1\n    x1 = x0 - f(x0) / fdot(x0)\nprint(x1)\n\n5.908619865450271\n\n\nDas Newtonverfahren wird als main-Funktion (d.h. im Hauptprogramm) ausgeführt, aber die Berechnung von \\(f\\) und ihrer Ableitung \\(f'\\) wurde in zwei Funktionen f und fdot ausgelagert. Das macht das Programm übersichtlicher und flexibler. Die Ableitung wurde wieder von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef newton(f, fdot, x0):\n    tol = 1e-6\n    # Erster Schritt berechnen\n    x1 = x0 - f(x0) / fdot(x0)\n    while fabs(x1 - x0) > tol:\n        x0 = x1\n        x1 = x0 - f(x0) / fdot(x0)\n    return x1\n\ndef f(x):\n    y = 1/31 * x**3 - 1/20 * x**2 - x + 1\n    return y\n\ndef fdot(x):\n    ydot = 3/31 * x**2 - 1/10 * x - 1\n    return ydot\n\nx0 = -2\nxbar = newton(f, fdot, x0)\nprint(xbar)\n\n5.908619865450271\n\n\nDas Newtonverfahren wird als eigene Funktion newton(f, fdot, x0) implementiert. Dieser werden die Funktion \\(f\\) und ihre Ableitung \\(f'\\), sowie der Startwert \\(x_0\\) als Argumente übergeben. Sie kann dann im Hauptprogramm aufgerufen werden. Die Ableitung wurde aber immer noch von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef newton(f, x0):\n    tol = 1e-6\n    # Erster Schritt berechnen\n    # Ableitung von f an der Stelle x0 annähern\n    h = 1e-6\n    ydot = ( f(x0 + h) - f(x0) ) / h\n    x1 = x0 - f(x0) / ydot\n    while fabs(x1 - x0) > tol:\n        x0 = x1\n        ydot = ( f(x0 + h) - f(x0) ) / h\n        x1 = x0 - f(x0) / ydot\n    return x1\n\ndef f(x):\n    y = 1/31 * x**3 - 1/20 * x**2 - x + 1\n    return y\n\nx0 = -2\nxbar = newton(f, x0)\nprint(xbar)\n\n5.90861986545027\n\n\nHier wird das Newtonverfahren in einer Funktion implementiert. Die Ableitung wird nicht mehr von Hand berechnet, sondern innerhalb der Funktion mit \\(f'(x_0)\\approx \\frac{f(x_0 + h) - f(x_0)}{h}\\) angenähert. Dabei wird einfach h = 1e-6 gesetzt und gehofft, dass der entstehende Rundungsfehler klein genug ist. Beachte aber, dass sich der berechnete Wert von der Ausgabe in den anderen Versionen leicht unterscheidet.\n\n\n\n\n\n\nAuch die Version 4 der vorgestellten Lösung ist noch nicht befriedigend. Als wir die Ableitung von Hand berechnet hatten, musste nur die Funktion fdot and der Stelle x0 ausgewertet werden, um den (bis auf Maschinengenauigkeit) exakten Wert von \\(f'(x_0)\\) zu erhalten. Bei der letzten Methode muss man sich mit einem Näherungswert der Ableitung zufrieden geben. Auch wenn der Wert in diesem Beispiel gut genug war 1, so haben wir doch keine Garantie, dass wir für alle Funktionen einen vernünftigen Wert erhalten. Auf die Probleme, die mit dieser Annäherung von \\(f'(x_0)\\) auftreten, wird in Kapitel 2.1 näher eingegangen.1 Das Newton-Verfahren hat die angenehme Eigenschaft, dass kleine Rundungsfehler automatisch ausgeglichen werden. Auf andere numerische Verfahren, die die Ableitung verwenden, trifft dies aber nicht zu.\n\nBeispiel 1.2 (Billard auf einem runden Tisch) \nWir betrachten ein Beispiel aus Gander (2015). Platziere die weisse und die blaue Billardkugel auf dem runden Tisch. Das Ziel ist es, die weisse Kugel so anzustossen, dass sie die blaue Kugel trifft, nachdem sie vorher genau einmal an die Bande gespielt wurde.\n\n\n\n\n\n\nAus Symmetriegründen dürfen wir annehmen, dass der Rand des Billardtisches der Einheitskreis ist und dass die weisse Kugel auf der \\(x\\)-Achse liegt. Die blaue Kugel habe die Koordinaten \\((x_P|y_P)\\). Weiter sei \\(X\\) der Punkt auf dem Einheitskreis, an dem die weisse Kugel abprallt. Wir beschreiben diesen Punkt mit seinen Polarkoordinaten \\(X=(\\cos(x)|\\sin(x))\\). Unser Ziel ist es, \\(x\\) so zu berechnen, dass die weisse Kugel die blaue trifft, nachdem sie bei \\(X\\) an die Bande gestossen ist. Dabei verhält sie sich so, als ob sie an der Kreistangente in \\(X\\) reflektiert wird. Der Tangentenvektor im Punkt \\(X\\) lautet \\(\\vec{t} = \\begin{pmatrix} -\\sin(x) \\\\ \\cos(x) \\end{pmatrix}\\).\n\n\n\n\n\n\nBillard auf einem runden Tisch\n\n\n\nWir betrachten nun die Einheitsvektoren \\(\\vec{e}_Q\\) in Richtung \\(\\overrightarrow{XQ}\\) und \\(\\vec{e}_P\\) in Richtung \\(\\overrightarrow{XP}\\). Wenn die weisse Kugel die blaue treffen soll, dann müssen die Winkel zwischen der Tangente und diesen Vektoren gleich sein. Das ist genau dann der Fall, wenn \\(\\vec{t}\\) senkrecht steht auf \\(\\vec{e}_Q + \\vec{e}_P\\). Wir müssen also \\(x\\) so bestimmen, dass \\(\\vec{t} \\cdot (\\vec{e}_Q + \\vec{e}_P) = 0\\) ist.\nDas folgende Programm berechnet das Skalarprodukt der linken Seite dieser Gleichung.\n\n\nCode\nimport math\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    # Parameter\n    a = -0.8           # Position von Q = (a|0)\n    px, py = 0.5, 0.5  # Position von P = (px|py)\n\n    # Berechnung des Skalarprodukts\n    v0 = x\n    v1 = math.cos(v0)  # x-Koordinate von X\n    v2 = math.sin(v0)  # y-Koordinate von X\n    v3 = px - v1       # x-Komponente des Vektors XP\n    v4 = py - v2       # y-Komponente des Vektors XP\n    v5 = math.sqrt(v3**2 + v4**2)  # Länge des Vektors XP\n    v6 = v3 / v5       # x-Komponente des Einheitsvektors eP\n    v7 = v4 / v5       # y-Komponente des Einheitsvektors eP\n    \n    v8 = a - v1        # x-Komponente des Vektors XQ\n    v9 = -v2           # y-Komponente des Vektors XQ\n    v10 = math.sqrt(v8**2 + v9**2)  # Länge des Vektors XQ\n    v11 = v8 / v10     # x-Komponente des Vektors eQ    \n    v12 = v9 / v10     # y-Komponente des Vektors eQ   \n    y = (v6 + v11) * v2 - (v7 + v12) * v1  # Skalarprodukt\n    return y   \n\n# Graph der Funktion f(x) plotten\nfig = plt.figure()\nax = plt.gca()\nax.set_xlim((0,2*math.pi))\nax.set_ylim((-1.5,1.5))\nX = [2*math.pi * k / 1000 for k in range(1001)]\nY = [f(x) for x in X]\nplt.plot([0, 2*math.pi], [0, 0], 'k--') # x-Achse\nplt.plot(X,Y)\nplt.xticks([0, math.pi/2, math.pi, 3*math.pi/2, 2*math.pi],\n           ['0', 'π/2', 'π', '3π/2', '2π'])\nplt.show()  \n\n\n\n\n\nAbbildung 1.3: Graph des Skalarprodukts als Funktion des Polarwinkels \\(x\\) des Punktes \\(X = (cos(x) | sin(x))\\). Die Nullstellen entsprechen den Winkeln, bei denen die weisse Kugel die blaue Kugel trifft, nachdem sie genau einmal an die Bande gespielt wurde.\n\n\n\n\nWir möchten die Nullstellen der Funktion f(x) mit unserer Funtion newton bestimmmen. Dazu müssen wir jedoch die Ableitung von f berechnen.\n\n\n\n\n1.4.2 Gradient Descent zum Auffinden lokaler Minima\nEine weitere wichtige Aufgabe besteht darin, ein Minimum einer Funktion zu finden. Auch hier wollen wir mit Hilfe der Ableitung eine Folge von Näherungswerten \\(x_0, x_1, x_2, \\ldots\\) finden, deren Grenzwert die \\(x\\)-Koordinate eines (lokalen) Minimums von \\(f\\) ist.\nWenn \\(f'(x_n)>0\\) ist, dann wissen wir, dass die Funktion \\(f\\) an der Stelle \\(x_0\\) streng monoton wachsend ist. D.h., dass die Funktionswerte links von \\(x_n\\) kleiner sind, als an der Stelle \\(x_n\\). Analog gilt, dass wenn \\(f'(x_n)<0\\) ist, die Funktion monoton fallend ist und wir uns nach rechts bewegen sollten, um ein Minimum zu finden. In der Nähe eines Minimums ist ausserdem \\(|f'(x)|\\) sehr klein und wir können entsprechend kleinere Schritte machen, um uns diesem anzunähern. Um also von \\(x_n\\) zu \\(x_{n+1}\\) zu kommen, machen wir einen Schritt, der proportional zu \\(-f'(x_n)\\) ist. Mit dem Proporionalitätsfaktor \\(\\lambda\\in\\mathbb{R}\\) und einem geeignet gewählten Startwert \\(x_0\\) erhalten wir die Iterationsvorschrift \\[\nx_{n+1} = x_n - \\lambda\\cdot f'(x_n)\n\\tag{1.2}\\]\n\n\n\n\n\n\n\nÜbungsaufgabe 1.6 (Eigenschaften der Gradient Descent Methode)  Experimentiere mit verschiedenen Funktionen und verschiedenen Schrittweiten \\(\\lambda\\). Was passiert, wenn die Schrittweite zu klein bzw. zu gross gewählt wird? Was passiert, wenn \\(f\\) an der Stelle \\(x_0\\) ein lokales Maximum aufweist? Was passiert in der Nähe eines Sattelpunktes?\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nIst \\(\\lambda\\) zu klein, dann konvergiert das Verfahren nur sehr langsam. Ist \\(\\lambda\\) dagegen zu gross, dann kann es passieren, dass die Iteration zwischen zwei oder mehr Werten hin- und herspringt oder sogar nach \\(\\pm\\infty\\) divergiert.\nFalls \\(x_0\\) gerade mit der Stelle eines lokalen Maximums oder eines Sattelpunktes zusammenfällt, gilt auch \\(f'(x_0)=0\\) und damit auch \\(x_n = x_0\\) für alle \\(n\\in\\mathbb{N}\\). Maxima sind aber labile Gleichgewichtspunkte in dem Sinn, dass sich \\(x_n\\) von ihnen wegbewegt, wenn \\(x_0\\) auch nur ein bisschen links oder rechts davon liegt. Ähnlich verhält es sich bei Sattelpunkten. Die Folge konvergiert gegen die Stelle des Sattelpunktes, wenn \\(f(x_0)\\) grösser als der \\(y\\)-Wert des Sattelpunktes ist und \\(\\lambda\\) nicht zu gross ist.\n\n\n\n\nÜbungsaufgabe 1.7 (Gradient Descent programmieren) \nSchreibe ein Programm, das mit Hilfe des Gradient Descent Verfahrens (Gleichung 1.2) ein lokales Minimums der Funktion \\(f(x) = \\frac{1}{16}x^4 - \\frac{1}{3}x^3 + \\frac{1}{8}x^2 + x + 2\\) berechnet. Verwende den Startwert \\(x_0 = 1.5\\) und die Schrittweite \\(\\lambda = 0.5\\). Du kannst abbrechen, wenn die Differenz \\(|x_{n+1} - x_n|\\) kleiner als eine bestimmte Toleranz wird, z.B. kleiner als tol = 1e-6. Wie flexibel ist dein Programm einsetzbar? Überlege dir z.B., wie viele Änderungen du vornehmen müsstest, wenn du ein lokales Minimum einer anderen Funktion berechnen müsstest.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nWelche der folgenden Lösungsvorschläge kommt deinem Programm am nächsten?\n\nVersion 1Version 2Version 3Version 4\n\n\n\nfrom math import fabs\n\nx0 = 1.5\nlam = 0.5\ntol = 1e-6\n# Erster Schritt berechnen\nx1 = x0 - lam * (1/4 * x0**3 - x0**2 + 1/4 * x0 + 1)\nwhile fabs(x1-x0) > tol:\n    x0 = x1\n    x1 = x0 - lam * (1/4 * x0**3 - x0**2 + 1/4 * x0 + 1)\nprint(x1)\n\n3.3429230748530196\n\n\nDas Gradient Descent Verfahren wird als main-Funktion (d.h. im Hauptprogramm) ausgeführt. Um das Minimum einer anderen Funktion zu bestimmen, muss ein neues Programm geschrieben werden. Die Ableitung wurde von Hand berechnet\n\n\n\nfrom math import fabs\n\ndef fdot(x):\n    ydot = 1/4 * x**3 - x**2 + 1/4 * x + 1\n    return ydot\n\nx0 = 1.5\nlam = 0.5\ntol = 1e-6\n# Erster Schritt berechnen\nx1 = x0 - lam * fdot(x0)\nwhile fabs(x1-x0) > tol:\n    x0 = x1\n    x1 = x0 - lam * fdot(x0)\nprint(x1)\n\n3.3429230748530196\n\n\nDas Gradient Descent Verfahren wird als main-Funktion (d.h. im Hauptprogramm) ausgeführt, aber die Berechnung von \\(f'\\) wurde in die Funktion fdot(x) ausgelagert. Das macht das Programm etwas flexibler. Die Ableitung wurde wieder von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef gradient_descent(fdot, x0, lam):\n    tol = 1e-6\n    # Erster Schritt berechnen\n    x1 = x0 - lam * fdot(x0)\n    while fabs(x1-x0) > tol:\n        x0 = x1\n        x1 = x0 - lam * fdot(x0)\n    return x1\n\ndef fdot(x):\n    ydot = 1/4 * x**3 - x**2 + 1/4 * x + 1\n    return ydot\n\nx0 = 1.5\nlam = 0.5\nxmin = gradient_descent(fdot, x0, lam)\nprint(xmin)\n\n3.3429230748530196\n\n\nDas Gradient Descent Verfahren wird als eigene Funktion gradient_descent(fdot, x0, lam) implementiert. Dieser Funktion werden die Ableitung \\(f'\\), der Startwert \\(x_0\\), sowie die Schrittweite \\(\\lambda\\) als Argumente übergeben. Sie kann dann im Hauptprogramm aufgerufen werden. Die Ableitung wurde aber immer noch von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef gradient_descent(f, x0, lam):\n    tol = 1e-6\n    # Erster Schritt berechnen\n    # Ableitung an der Stelle x0 annähern\n    h = 1e-6\n    ydot = ( f(x0 + h) - f(x0) ) / h\n    x1 = x0 - lam * ydot\n    while fabs(x1-x0) > tol:\n        x0 = x1\n        ydot = ( f(x0 + h) - f(x0) ) / h\n        x1 = x0 - lam * ydot\n    return x1\n\ndef f(x):\n    y = 1/16 * x**4 - 1/3 * x**3 + 1/8 * x**2 + x + 2\n    return y\n\nx0 = 1.5\nlam = 0.5\nxmin = gradient_descent(fdot, x0, lam)\nprint(xmin)\n\n2.535183236464121\n\n\nDas Gradient Descent Verfahren wird als eigene Funktion gradient_descent(f, x0, lam) implementiert. Dieser Funktion werden die ursprüngliche Funktion \\(f\\), der Startwert \\(x_0\\), sowie die Schrittweite \\(\\lambda\\) als Argumente übergeben. Die Ableitung wird nicht mehr von Hand berechnet, sondern durch den Differenzenquotienten \\(f'(x_0) \\approx \\frac{f(x_0 + h) - f(x_0)}{h}\\) angenähert. Dabei wird einfach h = 1e-6 gesetzt und gehofft, dass der entstehende Rundungsfehler klein genug ist. Offensichtlich ist diese Annahme jedoch nicht gerechtfertigt.\n\n\n\n\n\n\nDie Übungsaufgabe 1.7 verdeutlicht nochmals das Problem, welches wir bereits in Übungsaufgabe 1.5 gesehen haben. Wir müssen für den Algorithmus die Ableitung \\(f'\\) an mehreren Stellen auswerten. Wir möchten aber die Ableitung einerseits nicht von Hand berechnen und andererseits können wir uns auch nicht mit einer Approximation zufrieden geben.\nWir beschliessen dieses Kapitel mit einer praktischen Anwendung der Gradient Descent Methode.\n\nBeispiel 1.3 (Minimaler Abstand) \nDie Punkte \\(P\\) und \\(Q\\) bewegen sich auf Ellipsen im Raum. Die Position des Punktes \\(P\\) zur Zeit \\(t\\) ist gegeben durch\n\\[\\begin{align*}\n    x_P(t) &= 2 \\cos(t) - 1 \\\\\n    y_P(t) &= 1.5 \\sin(t)   \\\\\n    z_P(t) &= 0             \n\\end{align*}\\]\nund die Position von \\(Q\\) zum Zeitpunkt \\(t\\) lässt sich durch\n\\[\\begin{align*}\n    x_Q(t) &= -3 \\sin(2t)     \\\\\n    y_Q(t) &= 2 \\cos(2t) + 1  \\\\\n    z_Q(t) &= 2 \\sin(2t) + 1  \n\\end{align*}\\]\nbestimmen.\n\n\n\n\n\n\nDer Abstand zwischen den beiden Punkten lässt sich zu jedem Zeitpunkt \\(t\\) berechnen durch \\(d = d(t) = |\\overrightarrow{PQ}|\\). Das folgende Programm berechnet diese Funktion und zeichnet ihren Graph.\n\n\nCode\nimport math\nimport matplotlib.pyplot as plt\n\ndef d(t):\n    v0 = t\n    v1 = 2 * math.cos(v0) - 1    # x-Koordinate von P\n    v2 = 1.5 * math.sin(v0)      # y-Koordinate von P\n    v3 = 0                       # z-Koordinate von P\n    v4 = -3 * math.sin(2*v0)     # x-Koordinate von Q\n    v5 = 2 * math.cos(2*v0) + 1  # y-Koordinate von Q\n    v6 = 2 * math.sin(2*v0) + 1  # z-Koordinate von Q\n    y = math.sqrt((v4-v1)**2 + (v5-v2)**2 + (v6-v3)**2)\n    return y\n\n# Graph der Funktion d(t) plotten\nfig = plt.figure()\nax = plt.gca()\nax.set_xlim((0,2*math.pi))\nax.set_ylim((0,6))\nT = [2*math.pi * k / 1000 for k in range(1001)]\nY = [d(t) for t in T]\nplt.plot(T,Y)\nplt.xticks([0, math.pi/2, math.pi, 3*math.pi/2, 2*math.pi],\n           ['0', 'π/2', 'π', '3π/2', '2π'])\nplt.show()  \n\n\n\n\n\nAbbildung 1.4: Graph der Abstandsfunktion \\(d(t)\\).\n\n\n\n\nWir möchten das Minimum der Funktion \\(d(t)\\) mit Hilfe der Gradient Descent Methode finden. Dazu müssen wir aber \\(d\\) ableiten können.\n\n\n\n\n\n\nArens, Tilo, Frank Hettlich, Christian Karpfinger, Ulrich Kockelkorn, Klaus Lichtenegger, und Hellmuth Stachel. 2022. Mathematik. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nGander, Walter. 2015. Learning MATLAB: A Problem Solving Approach. 1. Aufl. UNITEXT. Cham, Switzerland: Springer International Publishing.\n\n\nGriewank, Andreas, und Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2. Aufl. Other Titles in Applied Mathematics 105. Philadelphia, PA: SIAM. http://bookstore.siam.org/ot105/.\n\n\nHromkovic, Juraj, Jarka Arnold, Cédric Donner, Urs Hauser, Matthias Hauswirth, Tobias Kohn, Dennis Komm, David Maletinsky, und Nicole Roth. 2021. INFORMATIK, Programmieren und Robotik: Grundlagen der Informatik für Schweizer Maturitätsschulen. Klett.\n\n\nSlater, Max. 2022. „Differentiable programming from scratch“. Juli 2022. https://thenumb.at/Autodiff/."
  },
  {
    "objectID": "notAD.html#sec-ADnotNumDiff",
    "href": "notAD.html#sec-ADnotNumDiff",
    "title": "2  AD ist nicht …",
    "section": "2.1 AD ist nicht numerisches Ableiten",
    "text": "2.1 AD ist nicht numerisches Ableiten\nEine Funktion \\(y = f(x)\\) ist bekanntlich differenzierbar an der Stelle \\(x_0 \\in \\mathbb{D}\\), wenn der Grenzwert \\[ \\lim_{h\\rightarrow 0} \\frac{f(x_0 + h) - f(x_0)}{h} \\] existiert. In dem Fall ist \\(f'(x_0)\\) einfach der Wert dieses Grenzwerts.\nEin erster Ansatz zur numerischen Berechnung könnte also sein, den Differenzenquotienten für kleine \\(h\\) auszuwerten1.1 Dieser Ansatz kann verbessert werden indem man z.B. \\(f'(x_0) \\approx \\frac{f(x_0 + h) - f(x_0 - h)}{2h}\\) verwendet. Die im Beispiel beschriebenen Probleme bleiben aber auch dann bestehen.\n\nBeispiel 2.1 (Numerische Ableitung) \nLeite die Funktion \\(f(x) = x^2\\) an der Stelle \\(x_0 = 0.2\\) ab.\n\n\nCode\ndef f(x):\n    y = x ** 2\n    return y\n\ndef fdot(f, x0, h):\n    df = (f(x0 + h) - f(x0)) / h\n    return df\n\nx0 = 0.2\nH = [0.1, 0.01, 0.001, 0.0001]\nfor h in H:\n    ydot = fdot(f, x0, h)\n    print(\"h = \" + str(h) + \" \\t=&gt; f'(x0) = \" + str(ydot))\n\n\nh = 0.1     =&gt; f'(x0) = 0.5000000000000001\nh = 0.01    =&gt; f'(x0) = 0.4099999999999999\nh = 0.001   =&gt; f'(x0) = 0.4009999999999986\nh = 0.0001  =&gt; f'(x0) = 0.40009999999993107\n\n\nEs scheint zunächst, als ob die Werte für kleiner werdende \\(h\\) zum korrekten Wert \\(f'(0.2)=0.4\\) konvergieren. Wenn wir aber an sehr genauen Werten interessiert sind und entsprechend \\(h\\) sehr klein wählen, beobachten wir folgendes:\n\n\nCode\ndef f(x):\n    y = x ** 2\n    return y\n\ndef fdot(f, x0, h):\n    df = (f(x0 + h) - f(x0)) / h\n    return df\n\nx0 = 0.2\nH = [10 ** -8, 10 ** -9, 10 ** -10, 10 ** -11]\nfor h in H:\n    ydot = fdot(f, x0, h)\n    print(\"h = \" + str(h) + \"\\t=&gt; f'(x0) = \" + str(ydot))\n\n\nh = 1e-08   =&gt; f'(x0) = 0.4000000095039091\nh = 1e-09   =&gt; f'(x0) = 0.3999999984016789\nh = 1e-10   =&gt; f'(x0) = 0.4000000330961484\nh = 1e-11   =&gt; f'(x0) = 0.3999994779846361\n\n\nMit kleiner werdendem \\(h\\) scheint sich der Näherungswert für die Ableitung zu verschlechtern. Das Phänomen wird noch deutlicher, wenn wir den Fehler \\(E(h) = \\lvert\\frac{f(x_0+h)-f(x_0)}{h} - f'(x_0)\\rvert\\) als Funktion von \\(h\\) plotten. Beachte die doppelt logarithmische Skala.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport math\n\ndef f(x):\n    y = x ** 2\n    return y\n\ndef fdot(f, x0, h):\n    df = (f(x0 + h) - f(x0)) / h\n    return df\n\nx0 = 0.2\nH = [10**(k/100) for k in range(-1800, -300)]\nE = [math.fabs(fdot(f, x0, h) - 2*x0) for h in H]\n\n# Plot\nfig = plt.figure()\nax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\nax.set(xlim=(10**-18, 10**-3), ylim=(10**-12, 10**0))\nax.set_xscale('log')\nax.set_xlabel('h')\nax.set_yscale('log')\nax.set_ylabel('Fehler E(h)')\nplt.plot(H,E)\nplt.show()\n\n\n\n\n\nAbbildung 2.1: Grösse des Fehlers \\(E(h)\\) als Funktion der Schrittweite \\(h\\). Ist \\(h\\) zu gross, dann ist der Näherungswert für \\(f'(x_0)\\) ungenau. Bei kleiner werdendem \\(h\\) nimmt der Fehler zunächst ab, aber ab einem gewissen Wert dominiert die Auslöschung und der Fehler nimmt wieder zu.\n\n\n\n\n\n\n\n2.1.1 Auslöschung\nIm vorherigen Beispiel haben wir das Phänomen der Auslöschung beobachtet. Zunächst ist dir sicher aufgefallen, dass der Näherungswert für \\(f'(x_0)\\) mit \\(h=0.01\\) nicht \\[ \\frac{f(x_0 + h) - f(x_0)}{h} = \\frac{0.21^2 - 0.2^2}{0.01}=0.41\\] ergab, sondern \\(f'(x_0)\\approx 0.40999...\\). Das liegt daran, dass Dezimalzahlen nicht exakt als Binärzahl dargestellt werden können. Da nun die Werte von \\(f(x_0 + h)\\) und \\(f(x_0)\\) für kleine \\(h\\) fast gleich sind, besteht ihre Differenz \\(f(x_0 + h) - f(x_0)\\) nur noch aus diesen Rundungsfehlern. Diese (sinnlose) Differenz ist zwar sehr klein, wird aber im nächsten Schritt mit der sehr grossen Zahl \\(\\frac{1}{h}\\) multipliziert, wodurch die Rundungsfehler die gleiche Grössenordnung annehmen, wie die ursprünglichen Funktionswerte. Mehr über Rundungsfehler und Auslöschung kann in Weitz (2021) ab S. 117 nachgelesen werden."
  },
  {
    "objectID": "notAD.html#sec-ADnotSymbDiff",
    "href": "notAD.html#sec-ADnotSymbDiff",
    "title": "2  AD ist nicht …",
    "section": "2.2 AD ist nicht symbolisches Ableiten",
    "text": "2.2 AD ist nicht symbolisches Ableiten\nComputer Algebra Systeme (CAS) sind Programme zur Bearbeitung algebraischer Ausdrücke. Mit solchen Programmen lassen sich auch Ableitungen symbolisch bestimmen. Wie das funktioniert, wird in Slater (2022) kurz angedeutet. Bekannte Beispiele für CAS sind etwa Wolfram Alpha, Maxima oder Sage. Letzteres kann man hier auch online ausprobieren. Gib z.B. den folgenden Code ein, welcher die Ableitung der Funktion aus Übungsaufgabe 1.5 bestimmt:\nl(x) = x^2 + 1\nf(x) = l(l(l(x)))\nfdot = diff(f,x)\nexpand(fdot)\n\n\n\n\n\n\nTipp\n\n\n\nAuf der Website kannst du rechts unter Language auch Maxima auswählen und Maxima-Code ausführen. Maxima ist in Sage integriert.\n\n\nFür Python gibt es die Bibliothek sympy, die ein CAS für Python zur Verfügung stellt. Damit können wir die Funktion aus Übungsaufgabe 1.5 direkt in Python ableiten:\n\nBeispiel 2.2 (Symbolische Ableitung)  Leite die Funktion \\(f(x) = l(l(l(x)))\\), wobei \\(l(x) = x^2 + 1\\) ist, an der Stelle \\(x_0 = 1\\) ab.\n\n\nCode\nfrom sympy import symbols, diff\n\ndef f(x):\n    v0 = x\n    v1 = v0 ** 2 + 1\n    v2 = v1 ** 2 + 1\n    v3 = v2 ** 2 + 1\n    y = v3\n    return y\n\nx = symbols('x')\nprint(\"f(x) =\", f(x))\n\ndf = diff(f(x),x)\nprint(\"f'(x) =\", df)\n\nx0 = 1\nprint(\"f'(\" + str(x0) + \") =\", df.evalf(subs={x:x0}))\n\n\nf(x) = ((x**2 + 1)**2 + 1)**2 + 1\nf'(x) = 8*x*(x**2 + 1)*((x**2 + 1)**2 + 1)\nf'(1) = 80.0000000000000\n\n\n\n\nDamit erhält man die (bis auf Maschinengenauigkeit) exakten Werte der Ableitungen. Der Grund, warum wir nicht auf symbolische Ausdrücke für Ableitungen zurückgreifen wollen, liegt darin, dass diese Methode bei komplizierten Funktionsausdrücken sehr ineffizient ist, insbesondere dann, wenn wir auch Ableitungen von Funktionen \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) berechnen wollen.\n\n\n\n\nSlater, Max. 2022. „Differentiable programming from scratch“. Juli 2022. https://thenumb.at/Autodiff/.\n\n\nWeitz, Edmund. 2021. Konkrete Mathematik (nicht nur) für Informatiker. 2. Aufl. Wiesbaden, Germany: Springer Spektrum."
  },
  {
    "objectID": "ADOneDimManually.html#sec-SadManualImplementation",
    "href": "ADOneDimManually.html#sec-SadManualImplementation",
    "title": "3  Standard Algorithmische Differentiation für eindimensionale Funktionen",
    "section": "3.1 Manuelle Implementation der SAD",
    "text": "3.1 Manuelle Implementation der SAD\nBeginnen wir mit einem Beispiel:\nWir möchten den Funktionswert und die Ableitung der Funktion \\(y=f(x)=\\sin(x^2)\\) an der Stelle \\(x_0=\\frac{\\pi}{2}\\) bestimmen. Das folgende Programm berechnet den Funktionswert.\n\n\nCode\nimport math\n\ndef f(x):\n    v0 = x\n    v1 = v0**2\n    v2 = math.sin(v1)\n    y = v2\n    return y\n\nx0 = math.pi / 2\nprint(f(x0))\n\n\n0.6242659526396992\n\n\n\\(f\\) ist eine zusammengesetzte Funktion, die wir mit den Funktionen\n\\[\\begin{align*}\n    v_0(x)   &= x \\\\\n    v_1(v_0) &= v_0 ^2 \\\\\n    v_2(v_1) &= \\sin(v_1)\n\\end{align*}\\]\nschreiben können als \\(y=f(x)=v_2(v_1(v_0(x)))\\). Die Ableitung berechnet sich dann mit der Kettenregel zu \\[\nf'(x) = \\frac{dv_2}{dv_1} \\cdot \\frac{dv_1}{dv_0} \\cdot \\frac{dv_0}{dx} = \\cos(v_1)\\cdot 2v_0 \\cdot 1 = \\cos(x^2) \\cdot 2x \\cdot 1\n\\] Wir können also die Ableitung von f(x) berechnen, indem wir jede Zeile des Programms gemäss den bekannten Regeln ableiten:\nv0dot = 1\nv1dot = 2 * v0 * v0dot\nv2dot = math.cos(v1) * v1dot\nMan beachte, dass durch die Konvention, dass immer v0 = x gesetzt wird, auch immer v0dot = 1 ist. Nun können wir unsere Funktion so ergänzen, dass nicht nur der Funktionswert, sondern auch die Ableitung an der Stelle \\(x_0\\) berechnet wird:\n\n\nCode\nimport math\n\ndef f(x):\n    v0dot = 1\n    v0 = x\n    v1dot = 2 * v0 * v0dot\n    v1 = v0**2\n    v2dot = math.cos(v1) * v1dot\n    v2 = math.sin(v1)\n    ydot = v2dot\n    y = v2\n    return [y, ydot]\n\nx0 = math.pi / 2\nprint(f(x0))\n\n\n[0.6242659526396992, -2.4542495411512917]\n\n\nDie Korrektheit des Programms können wir mit GeoGebra überprüfen, welches Ableitungen symbolisch berechnet.\n\n\n\n\nBeachte, dass wir konsequent die Kettenregel verwendet haben. So wird aus v1 = v0**2 etwa v1dot = 2 * v0 * v0dot oder aus v2 = sin(v1) wird v2dot = cos(v1) * v1dot.\n\nÜbungsaufgabe 3.1 (Programm ableiten) \nÄndere das vorherige Programm so ab, dass der Funktionswert und die Ableitung der Funktion \\(y = f(x) = \\ln(\\sin(x^2))\\) an der Stelle \\(x_0 = \\frac{\\pi}{2}\\) berechnet wird. Überprüfe deine Lösung mit GeoGebra.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nEs müssen lediglich zwei weitere Zeilen eingefügt werden und zwar für die Berechnung von v3 und v3dot. Vergiss nicht, die richtigen Werte zurückzugeben.\n\n\nCode\nimport math\n\ndef f(x):\n    v0dot = 1\n    v0 = x\n    v1dot = 2 * v0 * v0dot\n    v1 = v0**2\n    v2dot = math.cos(v1) * v1dot\n    v2 = math.sin(v1)\n    v3dot = 1 / v2 * v2dot\n    v3 = math.log(v2)\n    ydot = v3dot\n    y = v3\n    return [y, ydot]\n\nx0 = math.pi / 2\nprint(f(x0))\n\n\n[-0.4711787952593891, -3.9314166194288416]\n\n\n\n\n\n\n\n\n\n\n\nKonvention\n\n\n\nEin Programm, welches gemäss der Konvention in Kapitel 1.3 geschrieben ist, wird folgendermassen abgeleitet:\n\nFür jede Variable v wird eine neue Variable vdot für den Wert der Ableitung definiert, angefangen bei v0dot = 1.\nJede Programmzeile wird gemäss den bekannten Regeln aus Tabelle 1.1 und Tabelle 1.2 abgeleitet. Dabei wird insbesondere in jeder Zeile die Kettenregel verwendet.\nDie abgeleitete Anweisung wird jeweils vor (!) die zu ableitende Anweisung eingeschoben.\n\n\n\n\nÜbungsaufgabe 3.2 (Kostenfunktion mit Ableitung) \nLeite das Programm aus Übungsaufgabe 1.2 gemäss der Konvention ab.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\nimport math\n\ndef kosten(x):\n    v0dot = 1\n    v0 = x       # sX\n    v1dot = -v0dot\n    v1 = 16 - v0 # d\n    v2dot = 2*v1 * v1dot\n    v2 = v1**2 + 12**2\n    v3dot = 1/(2*math.sqrt(v2)) * v2dot\n    v3 = math.sqrt(v2) # sLand\n    v4dot = 15000 * v0dot\n    v4 = 15000 * v0 # kKueste\n    v5dot = 25000 * v3dot\n    v5 = 25000 * v3 # kLand\n    ydot = v4dot + v5dot\n    y = v4 + v5\n    return [y, ydot]\n\nx0 = 8\n[k, kdot] = kosten(x0)\nprint(\"Kosten\", k)\nprint(\"Ableitung\", kdot)\n\n\nKosten 480555.1275463989\nAbleitung 1132.495094369271\n\n\nDie Ableitung ist positiv, was uns zeigt, dass die Kostenfunktion in der Nähe der Stelle \\(x_0=8\\) wachsend ist und sich das lokale Minimum links davon befinden muss.\n\n\n\nIn Übungsaufgabe 3.8 werden wir die Position des Minimums mit Hilfe der Gradient Descent Verfahren bestimmen.\n\nÜbungsaufgabe 3.3 (Produktregel) \nDas folgende Programm berechnet die Funktion \\(y = f(x) = (2+x)(x-3)\\):\n\ndef f(x):\n    v0 = x\n    v1 = 2 + v0\n    v2 = v0 - 3\n    y = v1 * v2\n    return y\n\nLeite dieses Programm ab. Dein Programm soll die Gleichung der Tangente \\(t(x) = f(x_0) + f'(x_0)\\cdot (x-x_0)\\) an der Stelle \\(x_0 = 2\\) ausgeben.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef f(x):\n    v0dot = 1\n    v0 = x\n    v1dot = v0dot\n    v1 = 2 + v0\n    v2dot = v0dot\n    v2 = v0 - 3\n    ydot = v1dot * v2 + v1 * v2dot # Produktregel\n    y = v1 * v2\n    return [y, ydot]\n\nx0 = 2\n[y0, y0dot] = f(x0)\nprint(\"t(x) =\", y0, \"+\", y0dot, \"* ( x -\", x0, \")\")\n\n\nt(x) = -4 + 3 * ( x - 2 )\n\n\n\n\n\n\nÜbungsaufgabe 3.4 (Programm ableiten) \nLeite die Funktion aus Übungsaufgabe 1.3 ab. Gib den Funktionswert und den Wert der Ableitung an der Stelle \\(x_0=-2\\) aus.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\nimport math\n\ndef f(x):\n    v0dot = 1\n    v0 = x\n    v1dot = 2 * v0 * v0dot\n    v1 = v0 ** 2\n    v2dot = v1dot\n    v2 = v1 + 2\n    v3dot = -1/2 * v1dot\n    v3 = -v1 / 2\n    v4dot = -math.sin(v2) * v2dot\n    v4 = math.cos(v2)\n    v5dot = math.exp(v3) * v3dot\n    v5 = math.exp(v3)\n    v6dot = v4dot * v5 + v4 * v5dot\n    v6 = v4 * v5\n    ydot = v6dot - 1 / v0**2 * v0dot\n    y = v6 + 1 / v0\n    return [y, ydot]\n\nx0 = -2\nprint(f(x0))\n\n\n[-0.3700550823007931, -0.14136926695938976]\n\n\n\n\n\n\nÜbungsaufgabe 3.5 (Programm ableiten) \nLeite die Funktion aus Übungsaufgabe 1.4 ab.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\nimport math\n\ndef f(x):\n    v0dot = 1\n    v0 = x\n    v1dot = 2 * v0 * v0dot\n    v1 = v0 ** 2\n    v2dot = v1dot\n    v2 = v1 + 1\n    v3dot = v2dot + v0dot\n    v3 = v2 + v0\n    v4dot = 1 / v2 * v2dot\n    v4 = math.log(v2)\n    v5dot = 1 / (2 * math.sqrt(v3)) * v3dot\n    v5 = math.sqrt(v3)\n    ydot = (v4dot * v5 - v4 * v5dot) / v5**2\n    y = v4 / v5\n    return [y, ydot]\n\n\n\n\n\nBei all diesen Beispielen könnten wir auch die Reihenfolge der Anweisungen für vdot und v vertauschen, d.h. zuerst die Variable v berechnen und erst danach das zugehörige vdot. Die folgende Übung zeigt aber, warum der 3. Punkt unserer Konvention wichtig ist.\n\nÜbungsaufgabe 3.6 (Ein Programm mit einer Schleife) \nBetrachte die Funktion aus Übungsaufgabe 1.5, welche aus \\(\\ell(x) = x^2 + 1\\) die Funktion \\(y = f(x) = \\ell(\\ell(\\ell(x)))\\) berechnet. Aus Beispiel 2.2 wissen wir, dass \\(f'(1) = 80\\) ist. Vergleiche nun die beiden Varianten für die Ableitung des Programms:\n\nvdot vor vv vor vdot\n\n\n\ndef f(x):\n    v0dot = 1\n    v0 = x\n    for i in range(3):\n        v0dot = 2 * v0 * v0dot\n        v0 = v0 ** 2 + 1\n    ydot = v0dot\n    y = v0\n    return [y, ydot]\n\nprint(f(1))\n\n[26, 80]\n\n\n\n\n\ndef f(x):\n    v0 = x\n    v0dot = 1\n    for i in range(3):\n        v0 = v0 ** 2 + 1\n        v0dot = 2 * v0 * v0dot\n    y = v0\n    ydot = v0dot\n    return [y, ydot]\n\nprint(f(1))\n\n[26, 2080]\n\n\n\n\n\nWarum wird bei der 2. Variante der Wert der Ableitung falsch berechnet?\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nDas Problem tritt in der Schleife auf. In der 2. Variante überschreiben wir den Wert von v0 bereits mit dem neuen Wert der Iteration. Bei der Berechnung von v0dot hätten wir aber noch den alten Wert gebraucht. Die Reihenfolge ist also nur in der 1. Version korrekt. Würden wir die Schleife eliminieren und dafür wie in der Lösung zu Übungsaufgabe 1.5 für jeden Schleifendurchgang fortlaufend numerierte Variablen für die v und vdot verwenden, dann wäre die Reihenfolge wieder egal.\n\n\n\nIn der nächsten Übungsaufgabe verwenden wir die Technik der AD, um das Billardproblem aus Kapitel 1.4.1 mit dem Newtonverfahren zu lösen. Da uns die Funktion f(x) nun nicht mehr nur der Funktionswert, sondern auch die Ableitung berechnet, können wir das Newtonverfahren ohne die Probleme aus Übungsaufgabe 1.6 implementieren.\n\nÜbungsaufgabe 3.7 (Das Billard-Problem) \nLeite das Programm aus Beispiel 1.2 ab. Schreibe danach eine Funktion newton(f, x0), welche ausnutzt, dass der Funktionsaufruf f(x0) auch den exakten Wert der Ableitung zurückgibt. Stelle alle gefundenen Lösungen grafisch dar.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\nimport math\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    # Parameter werden im global space gefunden\n    # Berechnung des Skalarprodukts und dessen Ableitung\n    v0dot = 1\n    v0 = x\n    v1dot = -math.sin(v0) * v0dot  # Ableitung von ...\n    v1 = math.cos(v0)  # x-Koordinate von X\n    v2dot = math.cos(v0) * v0dot   # Ableitung von ...\n    v2 = math.sin(v0)  # y-Koordinate von X\n    v3dot = - v1dot    # Ableitung von ...\n    v3 = px - v1       # x-Komponente des Vektors XP\n    v4dot = - v2dot    # Ableitung von ...\n    v4 = py - v2       # y-Komponente des Vektors XP\n    v5dot = 1 / (2*math.sqrt(v3**2 + v4**2)) \\\n        * (2*v3*v3dot + 2*v4*v4dot)  # Ableitung von ...\n    v5 = math.sqrt(v3**2 + v4**2)  # Länge des Vektors XP\n    v6dot = (v3dot * v5 - v3 * v5dot) / v5**2  # Ableitung von ...\n    v6 = v3 / v5       # x-Komponente des Einheitsvektors eP\n    v7dot = (v4dot * v5 - v4 * v5dot) / v5**2  # Ableitung von ...\n    v7 = v4 / v5       # y-Komponente des Einheitsvektors eP\n    v8dot = -v1dot     # Ableitung von ...\n    v8 = a - v1        # x-Komponente des Vektors XQ\n    v9dot = -v2dot     # Ableitung von ...\n    v9 = -v2           # y-Komponente des Vektors XQ\n    v10dot = 1 / (2*math.sqrt(v8**2 + v9**2)) \\\n        * (2*v8*v8dot + 2*v9*v9dot)  # Ableitung von ...\n    v10 = math.sqrt(v8**2 + v9**2)  # Länge des Vektors XQ\n    v11dot = (v8dot * v10 - v8 * v10dot) / v10**2  # Ableitung von ...\n    v11 = v8 / v10     # x-Komponente des Vektors eQ    \n    v12dot = (v9dot * v10 - v9 * v10dot) / v10**2  # Ableitung von ... \n    v12 = v9 / v10     # y-Komponente des Vektors eQ   \n    ydot = (v6dot + v11dot) * v2 + (v6 + v11) * v2dot \\\n        - ( (v7dot + v12dot) * v1 + (v7 + v12) * v1dot )  # Ableitung von ...\n    y = (v6 + v11) * v2 - (v7 + v12) * v1\n    return [y, ydot]   \n\ndef newton(f, x0):\n    tol = 1e-8\n    # Erster Schritt berechnen\n    [y0, y0dot] = f(x0)\n    x1 = x0 - y0 / y0dot\n    while math.fabs(x1 - x0) &gt; tol:\n        x0 = x1\n        [y0, y0dot] = f(x0)\n        x1 = x0 - y0 / y0dot\n    return x1 \n\n\nif __name__ == \"__main__\":\n\n    # Parameter definieren\n    a = -0.8           # Position von Q = (a|0)\n    px, py = 0.5, 0.5  # Position von P = (px|py)\n\n    # Lösung des Billardproblems berechnen\n    sol = set({}) # leere Menge, in der die gefundenen Lösungen gespeichert werden\n    X = [2*math.pi * k / 10 for k in range(10)]  # Liste der Startwerte für Newton\n    for x0 in X:\n        x = newton(f, x0)\n        sol.add(x)\n\n    # Lösungen grafisch darstellen\n    fig = plt.figure()\n    ax = plt.gca()\n    ax.set_xlim((-1.2, 1.2))\n    ax.set_ylim((-1.2, 1.2))\n    ax.set_aspect('equal')\n    circle = plt.Circle((0,0), 1, color='b', fill=False)\n    qBall = plt.Circle((a,0), 0.02, color='k')\n    pBall = plt.Circle([px, py], 0.02, color='k')\n    ax.add_patch(circle)\n    ax.add_patch(qBall)\n    ax.add_patch(pBall)\n    for x in sol:\n        xcoords = [a, math.cos(x), px]\n        ycoords = [0, math.sin(x), py]\n        plt.plot(xcoords, ycoords, linewidth=1, linestyle='--')\n    plt.show()\n\n\n\n\n\nAbbildung 3.1: Lösung des Billardproblems.\n\n\n\n\n\n\n\nIn den folgenden beiden Aufgaben wollen wir das Gradient Descent Verfahren aus Kapitel 1.4.2 anwenden, um eine Funktion zu minimieren. Auch hier nutzen wir aus, dass uns der Funktionsaufruf jeweils den exakten Wert der Ableitung mitliefert.\n\nÜbungsaufgabe 3.8 (Minimale Kosten mit Gradient Descent) \nBenutze das Gradient Descent Verfahren, um das Minimum der Funktion kosten(x) aus Übungsaufgabe 3.2 zu bestimmen. Starte bei x0 = 8 und führe die Iteration so lange aus, bis sich zwei aufeinanderfolgende Werte um nicht mehr als eine bestimmte Toleranz, z.B. tol = 1e-4. Experimentiere mit verschiedenen Werten für \\(\\lambda\\)\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\nimport math\n\ndef kosten(x):\n    v0dot = 1\n    v0 = x       # sX\n    v1dot = -v0dot\n    v1 = 16 - v0 # d\n    v2dot = 2*v1 * v1dot\n    v2 = v1**2 + 12**2\n    v3dot = 1/(2*math.sqrt(v2)) * v2dot\n    v3 = math.sqrt(v2) # sLand\n    v4dot = 15000 * v0dot\n    v4 = 15000 * v0 # kKueste\n    v5dot = 25000 * v3dot\n    v5 = 25000 * v3 # kLand\n    ydot = v4dot + v5dot\n    y = v4 + v5\n    return [y, ydot]\n\nx0 = 8\nlam = 0.0001\ntol = 1e-4\n# 1. Schritt\n[k, kdot] = kosten(x0)\nx1 = x0 - lam * kdot\nwhile math.fabs(x1 - x0) &gt; tol:\n    x0 = x1\n    [k, kdot] = kosten(x0)\n    x1 = x0 - lam * kdot\nprint(\"Bei x =\", x1, \"sind die Kosten minimal.\")\nprint(\"Sie belaufen sich auf\", k)\n\n\nBei x = 7.000767589818459 sind die Kosten minimal.\nSie belaufen sich auf 480000.000393777\n\n\n\n\n\n\nÜbungsaufgabe 3.9 (Minimaler Abstand) \nLeite das Programm aus Beispiel 1.3 ab. Schreibe danach eine Funktion gradient_descent(f, x0, lam), welche ausnutzt, dass der Funktionsaufruf f(x0) auch den exakten Wert der Ableitung zurückgibt. Stelle die gefundene Lösung grafisch dar.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\nimport math\nimport matplotlib.pyplot as plt\n\ndef d(t):\n    v0dot = 1                    \n    v0 = t\n    v1dot = -2 * math.sin(v0) * v0dot   # Ableitung von...\n    v1 = 2 * math.cos(v0) - 1    # x-Koordinate von P\n    v2dot = 1.5 * math.cos(v0) * v0dot  # Ableitung von...\n    v2 = 1.5 * math.sin(v0)      # y-Koordinate von P\n    v3dot = 0                    # Ableitung von...\n    v3 = 0                       # z-Koordinate von P\n    v4dot = -6 * math.cos(2*v0) * v0dot # Ableitung von...\n    v4 = -3 * math.sin(2*v0)     # x-Koordinate von Q\n    v5dot = -4 * math.sin(2*v0) * v0dot # Ableitung von...\n    v5 = 2 * math.cos(2*v0) + 1  # y-Koordinate von Q\n    v6dot = 4 * math.cos(2*v0) * v0dot  # Ableitung von...\n    v6 = 2 * math.sin(2*v0) + 1  # z-Koordinate von Q\n    ydot = (2*(v4-v1)*(v4dot-v1dot) + 2*(v5-v2)*(v5dot-v2dot) + 2*(v6-v3)*(v6dot-v3dot)) \\\n         / (2 * math.sqrt((v4-v1)**2 + (v5-v2)**2 + (v6-v3)**2))\n    y = math.sqrt((v4-v1)**2 + (v5-v2)**2 + (v6-v3)**2)\n    return [y, ydot]\n\ndef gradient_descent(f, x0, lam):\n    tol = 1e-9\n    # Erster Schritt berechnen\n    [y0, y0dot] = f(x0)\n    x1 = x0 - lam * y0dot\n    while math.fabs(x1-x0) &gt; tol:\n        x0 = x1\n        [y0, y0dot] = f(x0)\n        x1 = x0 - lam * y0dot\n    return x1\n\nif __name__ == \"__main__\":\n    t0 = 3\n    tmin = gradient_descent(d, t0, 0.01)\n    [dmin, dmindot] = d(tmin)\n    print(\"Minimum bei (\", tmin, dmin, \")\")\n\n    fig = plt.figure()\n    ax = plt.gca()\n    ax.set_xlim((0,2*math.pi))\n    ax.set_ylim((0,6))\n    T = [2*math.pi * k / 1000 for k in range(1001)]\n    Y = [d(t)[0] for t in T]  # nur Funktionswert plotten\n    plt.plot(T,Y)\n    plt.xticks([0, math.pi/2, math.pi, 3*math.pi/2, 2*math.pi],\n               ['0', 'π/2', 'π', '3π/2', '2π'])\n    plt.plot(tmin,dmin,color='r', marker='o')\n    plt.show()      \n\n\nMinimum bei ( 4.712388977478413 1.5 )\n\n\n\n\n\nAbbildung 3.2: Kürzester Abstand mit Gradient Descent.\n\n\n\n\nBeachte, dass beim Zeichnen des Funktionsgraphen neu Y = [d(t)[0] for t in T] steht. Der Grund dafür ist, dass d(t) nun eine Liste mit zwei Elementen ist (Funktionswert und Ableitung) und wir nur den Funktionswert zeichnen wollen. Schreibt man stattdessen Y = [d(t) for t in T], dann wird zusätzlich auch der Graph der Ableitung gezeichnet."
  },
  {
    "objectID": "ADOneDimManually.html#sec-SadImplementationOperatorOverloading",
    "href": "ADOneDimManually.html#sec-SadImplementationOperatorOverloading",
    "title": "3  Standard Algorithmische Differentiation für eindimensionale Funktionen",
    "section": "3.2 Implementation der SAD mit Operator Overloading",
    "text": "3.2 Implementation der SAD mit Operator Overloading\nNach dem letzten Abschnitt könnte man einwenden, dass wir die Ableitungen der Funktionen ja doch von Hand berechnet haben, denn wir haben jede Programmzeile, in der eine Variable v berechnet wird, um eine weitere Zeile ergänzt, in der wir vdot nach den bekannten Ableitungsregeln berechnet haben. Dieser Einwand ist auch berechtigt - oder wie es Henrard ausdrückt:\n\n“The bad news is that it [calculating the derivatives] has to be done; it will not appear magically. It is not only a figure of speech that ‘something has to be done’ but that to have it working everything has to be done”. (Henrard 2017, 17)\n\nDie gute Nachricht ist, dass wir diesen Prozess weiter automatisieren können. Wir kennen die Ableitungsregeln für die elementaren Operationen (+,-,*,/), sowie für die Grundfunktionen. In diesem Abschnitt werden wir eine Klasse FloatSad schreiben, deren Instanzen Funktionswerte und Werte der Ableitung speichern. Da solche Werte in der Regel vom Typ float sind und wir die Standard-AD implementieren, nennen wir die Klasse FloatSad. Die Arbeit besteht dann darin, die Ableitungsregeln richtig in den Operatoren dieser Klasse zu kodieren. Da Python Operator Overloading kennt, werden wir dann nach getaner Arbeit die Ableitungen wirklich ohne zusätzlichen Programmieraufwand erhalten.\nDer Grundstein für unsere Klasse wurde bereits im 19. Jahrhundert gelegt, wie die folgende Infobox zeigt:\n\n\n\n\n\n\nHintergrund: Duale Zahlen\n\n\n\n\n\nDuale Zahlen wurden 1873 durch William Clifford eingeführt und sind ähnlich definiert, wie komplexe Zahlen. Zur Erinnerung: Eine komplexe Zahl ist eine Zahl der Form \\(a + bi\\), wobei \\(a,b \\in \\mathbb{R}\\) sind und \\(i\\) die Eigenschaft \\(i^2 = -1\\) hat. Eine duale Zahl ist eine Zahl der Form \\(a + a'\\epsilon\\), wobei wieder \\(a,a' \\in \\mathbb{R}\\) gilt, aber \\(\\epsilon\\) die Eigenschaft \\(\\epsilon^2 = 0\\) hat. Nun kann man nach dem Permanenzprinzip die folgenden Operationen für duale Zahlen definieren:\n\\[\\begin{alignat*}{3}\n    &\\textrm{Addition:} && (a+a'\\epsilon) + (b+b'\\epsilon) &&= (a+b) + (a'+b')\\epsilon \\\\ \\\\\n    &\\textrm{Subtraktion:} && (a+a'\\epsilon) - (b+b'\\epsilon) &&= (a-b) + (a'-b')\\epsilon \\\\ \\\\\n    &\\textrm{Multiplikation:}\\quad && (a+a'\\epsilon) \\cdot (b+b'\\epsilon) &&= ab + a'b\\epsilon + ab'\\epsilon + a'b'\\epsilon^2 \\\\\n    & && &&= ab + (a'b + ab')\\epsilon \\\\ \\\\\n    &\\textrm{Division:} && (\\textrm{für }b\\ne 0) \\quad \\frac{a+a'\\epsilon}{b+b'\\epsilon} &&= \\frac{(a+a'\\epsilon)(b-b'\\epsilon)}{(b+b'\\epsilon)(b-b'\\epsilon)} \\\\\n    & && &&= \\frac{ab+a'b\\epsilon-ab'\\epsilon-a'b'\\epsilon^2}{b^2 - (b')^2\\epsilon^2} \\\\\n    & && &&= \\frac{ab + (a'b-ab')\\epsilon}{b^2} \\\\\n    & && &&= \\frac{a}{b} + \\frac{a'b - ab'}{b^2} \\epsilon\n\\end{alignat*}\\]\nWir sehen, dass der reelle Teil den Wert der Operation und der duale Teil den Wert der zugehörigen Ableitung enthält. Dies gilt auch für Potenzen, wie man unter Anwendung des binomischen Satzes sieht:\n\\[\\begin{align*}\n    (a+a'\\epsilon)^n &= \\sum_{k=0}^n \\binom n k a^{n-k} (a'\\epsilon)^k  \\\\\n    &= a^n + n \\cdot a^{n-1} \\cdot a'\\epsilon + (\\textrm{Terme mit }\\epsilon^2) \\\\\n    &= a^n + n \\cdot a^{n-1} \\cdot a' \\epsilon\n\\end{align*}\\]\nIm dualen Teil erkennen wir die Kettenregel \\((a^n)' = n\\cdot a^{n-1}\\cdot a'\\). Damit können wir duale Zahlen auch in Polynome \\(p(x) = p_0 + p_1 x + p_2 x^2 + \\ldots + p_n x^n\\) einsetzen. Wir erhalten dann\n\\[\\begin{align*}\n    p(a+a'\\epsilon) &= p_0 + p_1 (a+a'\\epsilon) + p_2 (a+a'\\epsilon)^2 + \\ldots + p_n (a+a'\\epsilon)^n \\\\\n    &= p_0 + p_1 a + p_1 a'\\epsilon + p_2 a^2 + p_2 \\cdot 2a a' \\epsilon + ... + p_n a^n + p_n \\cdot n a^{n-1} a' \\epsilon \\\\\n    &= p_0 + p_1 a + p_2 a^2 + \\ldots p_n a^n + (p_1 + p_2 \\cdot 2a + \\ldots + p_n \\cdot n a^{n-1}) \\cdot a' \\epsilon \\\\\n    &= p(a) + p'(a) \\cdot a'\\epsilon\n\\end{align*}\\]\nDieses Resultat lässt sich auf allgemeine Funktionen \\(f\\) verallgemeinern (für den Beweis entwickelt man \\(f\\) in eine Taylorreihe und macht die gleichen Überlegungen wie für ein Polynom): \\[\nf(a+a'\\epsilon) = f(a) + f'(a)\\cdot a'\\epsilon\n\\]\n(Wikipedia: Dual number und Slater (2022))\n\n\n\n\n3.2.1 Die Klasse FloatSad\nBeginnen wir nun mit der Implementation unserer Klasse FloatSad. Analog zu den dualen Zahlen enthält jedes FloatSad-Objekt zwei Attribute. Das Attribut value speichert den Funktionswert und das Attribut derivative speichert den Wert der Ableitung. Im Konstruktor der Klasse setzen wir für derivative den Standardwert 1. Damit können wir eine gewöhnliche Float-Zahl korrekt in ein FloatSad umwandeln. Dies wird im main Programm demonstriert.\n\n\nCode\nimport math\n\nclass FloatSad:\n\n    def __init__(self, value, derivative = 1.0):\n        self.value = float(value)\n        self.derivative = derivative\n\n\nif __name__ == '__main__':\n\n    def f(x):\n        v0 = FloatSad(x)\n        y = v0\n        return y\n\n    x0 = 2\n    resultat = f(x0)\n    print(\"Funktionswert:\", resultat.value)\n    print(\"Ableitung:\", resultat.derivative)\n\n\nFunktionswert: 2.0\nAbleitung: 1.0\n\n\nIn der Funktion f haben wir nun unsere Konvention, dass v0 = x sein soll, dazu verwendet, den Zahlenwert x in ein FloatSad-Objekt umzuwandeln. Die Konvention v0dot = 1 ist im Konstruktor kodiert. Von nun an machen wir also die folgende Konvention:\n\n\n\n\n\n\nKonvention\n\n\n\nEine Funktion berechnet aus einem Argument x vom Typ float oder int einen Rückgabewert y vom Typ FloatSad über eine Reihe von Hilfsvariablen v, die alle vom Typ FloatSad sind. Insbesondere setzen wir am Anfang immer v0 = FloatSad(x).\n\n\nDas obige Programm berechnet also den Funktionswert und den Wert der Ableitung von \\(f(x) = x\\) an der Stelle \\(x_0 = 2\\).\nUm die Ausgabe etwas einfacher zu gestalten implementieren wir als nächstes die print Methode für unsere Klasse. Wir geben ein FloatSad-Objekt einfach in der Form &lt; value ; derivative &gt; aus.\n\ndef __repr__(self):\n        return \"&lt; \" + str(self.value) + \" ; \" + str(self.derivative) + \" &gt;\"\n\nDa wir nun die Funktion \\(f(x) = x\\) programmieren können, wollen wir als nächstes auch die Funktion \\(f(x) = -x\\) programmieren können. Wir müssen unsere FloatSad-Objekte also mit Vorzeichen versehen.\n\n\n3.2.2 Vorzeichen\nNatürlich wollen wir nicht nur das negative Vorzeichen, sondern auch das positive Vorzeichen implementieren, damit wir in unseren Programmen z.B. v1 = +v0 oder v2 = -v0 schreiben können. Beim positiven Vorzeichen müssen wir nichts machen, wir geben also ein FloatSad-Objekt mit den gleichen Attributen zurück. Beim negativen Vorzeichen ändern beide Attribute ihr Vorzeichen.\n\ndef __pos__(self):\n        return FloatSad(self.value, self.derivative)\n    \ndef __neg__(self):\n    newValue = -self.value\n    newDerivative = -self.derivative\n    return FloatSad(newValue, newDerivative)\n\nNun gehen wir daran, die Grundoperationen für FloatSad-Objekte zu implementieren.\n\n\n3.2.3 Die Operatoren + und -\nWir möchten in unseren Programmen Anweisungen wie v2 = v0 + v1 verwenden können. Gemäss der Summenregel können wir dazu einfach die Funktionswerte und auch die Werte der Ableitungen addieren.\n\ndef __add__(self, other):\n        newValue = self.value + other.value\n        newDerivative = self.derivative + other.derivative\n        return FloatSad(newValue, newDerivative)\n\nNun können wir zwei FloatSad-Objekte miteinander addieren. Manchmal möchten wir aber auch ein float- oder int-Wert zu einem FloatSad-Objekt addieren, z.B. v1 = v0 + 2. Dazu machen wir eine Typabfrage und passen den Wert der Ableitung entsprechend der Konstantenregel an:\n\ndef __add__(self, other):\n    if type(other) in (float, int):\n        newValue = self.value + other\n        newDerivative = self.derivative + 0.0\n    else:\n        newValue = self.value + other.value\n        newDerivative = self.derivative + other.derivative\n    return FloatSad(newValue, newDerivative)\n\nJetzt funktioniert zwar die Anweisung v1 = v0 + 2, aber die Anweisung v1 = 2 + v0 erzeugt immer noch eine Fehlermeldung. Um dieses Problem zu beheben, müssen wir als nächstes den reverse-add-Operator implementieren.\n\ndef __radd__(self, other):\n    if type(other) in (float, int):\n        newValue = other + self.value\n        newDerivative = 0.0 + self.derivative\n    else:\n        newValue = other.value + self.value\n        newDerivative = other.derivative + self.derivative\n    return FloatSad(newValue, newDerivative)\n\nHier ist die bisher implementierte Klasse zusammen mit einem kleinen Testprogramm.\n\n\nCode\nimport math\n\nclass FloatSad:\n\n    def __init__(self, value, derivative = 1.0):\n        self.value = float(value)\n        self.derivative = derivative\n\n    def __repr__(self):\n        return \"&lt; \" + str(self.value) + \" ; \" + str(self.derivative) + \" &gt;\"\n\n\n    # unäre Operatoren\n\n    def __pos__(self):\n        return FloatSad(self.value, self.derivative)\n    \n    def __neg__(self):\n        newValue = -self.value\n        newDerivative = -self.derivative\n        return FloatSad(newValue, newDerivative)\n    \n\n    # binäre Operatoren\n\n    def __add__(self, other):\n        if type(other) in (float, int):\n            newValue = self.value + other\n            newDerivative = self.derivative + 0.0\n        else:\n            newValue = self.value + other.value\n            newDerivative = self.derivative + other.derivative\n        return FloatSad(newValue, newDerivative)\n\n    def __radd__(self, other):\n        if type(other) in (float, int):\n            newValue = other + self.value\n            newDerivative = 0.0 + self.derivative\n        else:\n            newValue = other.value + self.value\n            newDerivative = other.derivative + self.derivative\n        return FloatSad(newValue, newDerivative)\n    \n\n\nif __name__ == '__main__':\n\n    def f(x):\n        v0 = FloatSad(x)\n        v1 = -v0 \n        v2 = 3 + v1\n        v3 = v2 + v1\n        y  = +v3\n        return y\n\n    resultat = f(2)\n    print(resultat)\n\n\n&lt; -1.0 ; -2.0 &gt;\n\n\n\nÜbungsaufgabe 3.10 (Korrektheit überprüfen) \nWelche Funktion berechnet f im main Programm? Stimmt die Ausgabe?\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nEs handelt sich um die Funktion \\(f(x) = 3 - 2x\\). Die Ausgabe \\(f(2) = -1, f'(2) = -2\\) ist also korrekt.\n\n\n\nFür die nächste Übung musst du das obige Programm kopieren und in einer Datei mit dem Namen floatsad.py abspeichern. Speichere die Datei im gleichen Ordner wie die anderen Dateien.\n\nÜbungsaufgabe 3.11 (Den Operator - implementieren) \nImplementiere auf die gleiche Weise den --Operator. Die Methoden lauten __sub__(self, other) bzw. __rsub__(self, other). Schreibe auch eine Testfunktion f, welche die neuen Operatoren verwendet.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef __sub__(self, other):\n    if type(other) in (float, int):\n        newValue = self.value - other\n        newDerivative = self.derivative - 0.0\n    else:\n        newValue = self.value - other.value\n        newDerivative = self.derivative - other.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef __rsub__(self, other):\n    if type(other) in (float, int):\n        newValue = other - self.value\n        newDerivative = 0.0 - self.derivative\n    else:\n        newValue = other.value - self.value\n        newDerivative = other.derivative - self.derivative\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\n\n\n3.2.4 Die Operatoren * und /\nAls nächstes wollen wir die Multiplikation implementieren, um Anweisungen der Form v2 = v0 * v1 ausführen zu können. Dazu müssen wir die Produktregel verwenden. Wie bei der Addition und der Subtraktion soll unser *-Operator aber auch Anweisungen der Form v1 = v0 * 2 oder v1 = -3 * v0 richtig auswerten, bei denen die Faktorregel angewendet wird. Dazu ist wieder eine Typabfrage nötig.\n\nÜbungsaufgabe 3.12 (Den Operator * implementieren) \nErgänze die Datei floatsad.py mit den Methoden __mul__(self, other) und __rmul__(self, other). Überlege dir verschiedene Testfälle und überzeuge dich von der Korrektheit deines Programms.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef __mul__(self, other):\n    if type(other) in (float, int):\n        newValue = self.value * other\n        newDerivative = self.derivative * other\n    else:\n        newValue = self.value * other.value\n        newDerivative = self.derivative * other.value + self.value * other.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef __rmul__(self, other):\n    if type(other) in (float, int):\n        newValue = other * self.value\n        newDerivative = other * self.derivative\n    else:\n        newValue = other.value * self.value\n        newDerivative =  other.derivative * self.value + other.value * self.derivative\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\nEs fehlt noch der Divisionsoperator, damit wir Anweisungen wie v2 = v1 / v0 verwenden können. Da wir es bei differenzierbaren Funktionen immer mit float- bzw. FloatSad-Objekten zu tun haben, implementieren wir nur den /-Operator, also die Funktion __truediv__(self, other) und nicht den //-Operator. Wir wollen aber wieder die Fallunterscheidung nach den Typen machen, so dass auch Anweisungen wie v1 = v0 / 4 verwendet werden können. Dabei benötigen wir nur die Faktorregel und nicht die Quotientenregel. Um schliesslich auch noch v1 = -4 / v0 zu ermöglichen, muss noch __rtruediv__(self, other) implementiert werden. Bei letzterem darf nicht vergessen werden, dass auch die Kettenregel benutzt werden muss, denn \\(\\frac{dv_1}{dx} = \\frac{dv_1}{dv_0}\\cdot \\frac{dv_0}{dx} = \\frac{4}{v_0^2}\\cdot v_0'\\). Quadrate kann man mit math.pow(value, 2) berechnen. Bei der Implementierung müssen wir uns übrigens nicht um Fehlerbehandlungen, wie das Abfangen einer Division durch Null, kümmern, weil diese bereits im /-Operator, den wir verwenden, implementiert sind.\n\nÜbungsaufgabe 3.13 (Den Operator / implementieren) \nErgänze die Datei floatsad.py mit den Methoden __truediv__(self, other) und __rtruediv__(self, other). Überlege dir auch wieder verschiedene Testfälle und überzeuge dich von der Korrektheit deines Programms.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef __truediv__(self, other):\n    if type(other) in (float, int):\n        newValue = self.value / other\n        newDerivative = self.derivative / other\n    else:\n        newValue = self.value / other.value\n        newDerivative = (self.derivative * other.value - self.value * other.derivative) / math.pow(other.value, 2)\n    return FloatSad(newValue, newDerivative)\n\ndef __rtruediv__(self, other):\n    if type(other) in (float, int):\n        newValue = other / self.value\n        newDerivative = - other / math.pow(self.value, 2) * self.derivative\n    else:\n        newValue = other.value / self.value\n        newDerivative = (other.derivative * self.value - other.value * self.derivative) / math.pow(self.value, 2) * self.derivative\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\n\n\n3.2.5 Der Operator **\nInteressant ist nun die Implementation des Potenzoperators. Hier sind mehrere Fallunterscheidungen nötig.\nBetrachten wir zuerst den Fall, type(other) in (float, int), d.h. wir haben einen Ausdruck der Form v1 = v0 ** 3. In diesem Fall wenden wir die Potenzregel zusammen mit der Kettenregel an.\nIm zweiten Fall haben wir einen Ausdruck wie v3 = v1 ** v2. Wir müssen uns also zuerst überlegen, wie wir einen Ausdruck der Form \\(v_3 (x) = v_1(x) ^{v_2 (x)}\\) überhaupt ableiten. Offenbar muss dazu \\(v_1(x) &gt; 0\\) gelten. Um die Ableitung zu finden wenden wir den Trick an, dass wir die Funktion zuerst logarithmieren, \\[\n\\ln(v_3(x)) = \\ln(v_1(x) ^{v_2 (x)}) = v_2(x) \\cdot \\ln(v_1(x))\n\\] und danach beide Seiten ableiten, wobei wir auf der rechten Seite die Produktregel anwenden: \\[\n\\frac{d}{dx}(\\ln(v_3(x))) = v_2'(x) \\cdot \\ln(v_1(x)) + v_2(x) \\cdot \\frac{1}{v_1(x)} \\cdot v_1'(x)\n\\] Die linke Seite ergibt andererseits \\(\\frac{d}{dx}(\\ln(v_3(x))) = \\frac{1}{v_3(x)}\\cdot v_3'(x)\\), so dass wir nun nach \\(v_3'(x)\\) auflösen können:\n\\[\\begin{align*}\n    v_3'(x) &= v_3(x) \\cdot \\left( v_2'(x) \\cdot \\ln(v_1(x)) + \\frac{v_2(x)}{v_1(x)} \\cdot v_1'(x) \\right) \\\\\n    &= v_1(x) ^{v_2 (x)} \\cdot \\left( \\ln(v_1(x)) \\cdot v_2'(x) + \\frac{v_2(x)}{v_1(x)} \\cdot v_1'(x) \\right)\n\\end{align*}\\]\nAuch hier sind alle nötigen Fehlerbehandlungen bereits in math.pow implementiert.\n\nÜbungsaufgabe 3.14 (Den Operator ** implementieren - Teil 1) \nErgänze die Datei floatsad.py mit der Methode __pow__(self, other). Dabei übernimmt self die Rolle von \\(v_1\\) in der obigen Herleitung und other entspricht \\(v_2\\). Die Funktion \\(\\ln(\\ldots)\\) heisst in Python math.log(). Teste dein Programm an verschiedenen Funktionen.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef __pow__(self, other):\n    if type(other) in (float, int):\n        newValue = math.pow(self.value, other)\n        newDerivative = other * math.pow(self.value, other - 1) * self.derivative\n    else:\n        newValue = math.pow(self.value, other.value)\n        newDerivative = math.pow(self.value, other.value) * \\\n            (other.derivative * math.log(self.value) + other.value * self.derivative / self.value)\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\nNun implementieren wir auch noch die Methode __rpow__(self, other). Im Fall, dass type(other) in (float, int) ist, handelt es sich hierbei um eine Exponentialfunktion. math.pow stellt dann sicher, dass die Basis, also other, eine positive Zahl ist. Falls other ebenfalls ein FloatSad ist, dann kann die Ableitung gleich wie oben berechnet werden, ausser, dass jetzt self und other ihre Rollen tauschen.\n\nÜbungsaufgabe 3.15 (Den Operator ** implementieren - Teil 2) \nErgänze die Datei floatsad.py mit der Methode __rpow__(self, other). Teste dein Programm an verschiedenen Funktionen.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef __rpow__(self, other):\n    if type(other) in (float, int):\n        newValue = math.pow(other, self.value)\n        newDerivative = math.pow(other, self.value) * math.log(other) * self.derivative\n    else:\n        newValue = math.pow(other.value, self.value)\n        newDerivative = math.pow(other.value, self.value) * \\\n            (self.derivative * math.log(other.value) + self.value * other.derivative / other.value)\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\n\n\n3.2.6 Vergleichsoperatoren\nEs könnte sein, dass wir FloatSad-Objekte auch miteinander vergleichen wollen, also eine der Abfragen aus Tabelle 3.1 machen wollen.\n\n\nTabelle 3.1: Vergleichsoperatoren\n\n\nOperator\nMethode\n\n\n\n\n&lt;\n__lt__(self, other)\n\n\n&lt;=\n__le__(self, other)\n\n\n==\n__eq__(self, other)\n\n\n!=\n__ne__(self, other)\n\n\n&gt;\n__gt__(self, other)\n\n\n&gt;=\n__ge__(self, other)\n\n\n\n\nDazu vergleichen wir jeweils nur die Funktionswerte. Die Implementation sieht dann folgendermassen aus:\n\n\nCode\n# Vergleichsoperatoren \n\ndef __lt__(self, other):\n    if type(other) in (float, int):\n        return self.value &lt; other\n    else:\n        return self.value &lt; other.value\n\ndef __le__(self, other):\n    if type(other) in (float, int):\n        return self.value &lt;= other\n    else:\n        return self.value &lt;= other.value\n\ndef __eq__(self, other):\n    if type(other) in (float, int):\n        return self.value == other\n    else:\n        return self.value == other.value\n\ndef __ne__(self, other):\n    if type(other) in (float, int):\n        return self.value != other\n    else:\n        return self.value != other.value\n\ndef __gt__(self, other):\n    if type(other) in (float, int):\n        return self.value &gt; other\n    else:\n        return self.value &gt; other.value\n\ndef __ge__(self, other):\n    if type(other) in (float, int):\n        return self.value &gt;= other\n    else:\n        return self.value &gt;= other.value"
  },
  {
    "objectID": "ADOneDimManually.html#die-klasse-floatsad-im-einsatz",
    "href": "ADOneDimManually.html#die-klasse-floatsad-im-einsatz",
    "title": "3  Standard Algorithmische Differentiation für eindimensionale Funktionen",
    "section": "3.3 Die Klasse FloatSad im Einsatz",
    "text": "3.3 Die Klasse FloatSad im Einsatz\nFalls im letzten Abschnitt etwas nicht geklappt haben sollte, kann die fertige Klasse FloatSad von hier kopiert werden.\nUm unsere Klasse zu verwenden müssen wir sie jeweils am Anfang mit\n\nfrom floatsad import FloatSad\n\neinbinden.\n\nBeispiel 3.1 (Ein Programm mit FloatSad) \nBetrachte die Funktion aus Beispiel 1.1. Wir übernehmen das Programm und passen lediglich die erste Zeile der Funktion gemäss der Konvention aus Kapitel 3.2.1 an.\n\n\nCode\nfrom floatsad import FloatSad\n\ndef f(x):\n    v0 = FloatSad(x)\n    v1 = 2 + v0\n    v2 = v0 - 3\n    y = v1 * v2\n    return y\n\nx0 = 2\nprint(f(x0))\n\n\n&lt; -4.0 ; 3.0 &gt;\n\n\nDa nun alle Ableitungsregeln in den verwendeten Operatoren integriert sind, können wir sogar auf die Zwischenschritte mit den v verzichten:\n\n\nCode\nfrom floatsad import FloatSad\n\ndef f(x):\n    x = FloatSad(x)\n    y = (2+x) * (x-3)\n    return y\n\nx0 = 2\nprint(f(x0))\n\n\n&lt; -4.0 ; 3.0 &gt;\n\n\n\n\nWir sehen, dass wir also alle unsere Konventionen, die dazu dienten, komplizierte Funktionsausdrücke in ihre Bestandteile zu zerlegen und diese mit den elementaren Ableitungsregeln zu differenzieren, wieder aufgeben können! Der einzige Zusatzaufwand, den wir bei der Programmierung haben, ist das Umwandeln des Arguments x in ein FloatSad-Objekt.\n\nÜbungsaufgabe 3.16 (FloatSad anwenden) \nVereinfache die Lösung von Übungsaufgabe 3.6 mit Hilfe der Klasse FloatSad. Überzeuge dich davon, dass die Ableitungen auch für Programme mit Schleifen korrekt berechnet werden.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\nfrom floatsad import FloatSad\n\ndef l(x):\n    y = x**2 + 1\n    return y\n\ndef f(x):\n    x = FloatSad(x)\n    for i in range(3):\n        x = l(x)\n    return x\n\nprint(f(1))\n\n\n&lt; 26.0 ; 80.0 &gt;"
  },
  {
    "objectID": "ADOneDimManually.html#das-modul-mathsad",
    "href": "ADOneDimManually.html#das-modul-mathsad",
    "title": "3  Standard Algorithmische Differentiation für eindimensionale Funktionen",
    "section": "3.4 Das Modul mathsad",
    "text": "3.4 Das Modul mathsad\nMit der Klasse FloatSad können wir Funktionswerte und Ableitungen von algebraischen Funktionen bilden. Wir können aber unsere FloatSad-Objekte noch nicht mit den Funktionen aus dem Python-Modul math verwenden, z.B. mit exp oder sin. In diesem Abschnitt wollen wir ein eigenes Modul mathsad schreiben, in dem wir die Funktionen aus Tabelle 3.2 so implementieren, dass wir sie auf FloatSad-Objekte anwenden können.\n\n\nTabelle 3.2: Funktionen des Moduls math (Auswahl)\n\n\nsqrt\nexp\nlog\n\n\nsin\ncos\ntan\n\n\nasin\nacos\natan\n\n\nsinh\ncosh\ntanh\n\n\nasinh\nacosh\natanh\n\n\nfabs\n\n\n\n\n\n\nGemäss der Python-Dokumentation liefert die Funktion math.exp(x) präzisere Werte als math.e ** x oder math.pow(math.e, x). Die Funktion math.log(x) berechnet den Logarithmus zur Basis \\(e\\), man kann ihr aber als zweites Argument auch eine andere Basis übergeben, z.B. math.log(x,b), was dann mit math.log(x)/math.log(b) berechnet wird. Die Funktion math.fabs(x) schliesslich berechnet den Absolutbetrag \\(|x|\\). Ihre Ableitung ist\n\n(math.fabs(v)).derivative = v.derivative if v>=0 else -v.derivative\n\nDie Funktion \\(y=|x|\\) ist an der Stelle \\(x=0\\) eigentlich nicht differenzierbar. Da wir aber nicht Ableitungsfunktionen, sondern nur Werte von Ableitungen an einer bestimmten Stelle berechnen, reicht es, den rechts- oder linksseitigen Grenzwert zurückzugeben, siehe Gander (1992). Wir müssen es dem Benutzer überlassen, das Ergebnis im jeweiligen Kontext korrekt zu interpretieren.\n\n3.4.1 Die Funktion sqrt\nBeginnen wir mit der Implementierung der Wurzelfunktion.\n\n\nCode\nimport math\nfrom floatsad import FloatSad\n\ndef sqrt(x):\n    newValue = math.sqrt(x.value)\n    newDerivative = 1/(2*math.sqrt(x.value)) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\nif __name__ == '__main__':\n\n    def f(x):\n        x = FloatSad(x)\n        y = 1 / sqrt(x**2 + 1)\n        return y\n\n    x0 = -1\n    print(f(x0))\n\n\n< 0.7071067811865475 ; 0.3535533905932737 >\n\n\nWir gehen davon aus, dass x ein FloatSad-Objekt ist. Für den Wert von sqrt(x) verwenden wir einfach die Funktion math.sqrt. Diese enthält auch die nötige Fehlerbehandlung. Zusätzlich berechnen wir aber noch den Wert der Ableitung mit Hilfe der bekannten Ableitungsregel und wie zuvor wenden wir immer die Kettenregel an. Das Programm enthält auch ein Testprogramm, welches die Ableitung der Funktion \\(f(x) = \\frac{1}{\\sqrt{x^2+1}}\\) an der Stelle \\(x_0 = -1\\) berechnet. Zur Kontrolle kann die GeoGebra-Vorlage zu Beginn von Kapitel 3.1 verwendet werden.\n\n\n3.4.2 Die Funktionen exp und log\n\nÜbungsaufgabe 3.15 (Exponentialfunktion) \nKopiere den obigen Code und speichere ihn in einer Datei mit dem Namen mathsad.py. Speichere die Datei im gleichen Ordner wie die anderen Dateien. Ergänze die Datei danach mit der Funktion exp. Wähle eine neue Testfunktion im main, um dich von der Richtigkeit deiner Lösung zu überzeugen.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef exp(x):\n    newValue = math.exp(x.value)\n    newDerivative = math.exp(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\nFür die Logarithmusfunktion müssen wir uns wieder etwas mehr Gedanken machen. Mit def log(x, b = math.e) kann man der Basis \\(b\\) wie oben beschrieben den Standardwert \\(b=e\\) geben. Solange b vom Typ int oder float ist, kann man einfach die bekannte Ableitungsregel anwenden. Wenn aber b ein FloatSad-Objekt ist, wie z.B. in v3 = math.log(v1, v2), dann müssen wir den Basiswechselsatz \\[\nv_3(x) = \\log_{v_2(x)}(v_1(x)) = \\frac{\\ln(v_1(x))}{\\ln(v_2(x))}\n\\] verwenden und mit der Quotientenregel ableiten.\n\nÜbungsaufgabe 3.16 (Logarithmusfunktion) \nÜberlege dir, wie die Ableitung von \\(v_3(x)\\) aussieht. Ergänze danach die Datei mathsad.py mit der Implementation der Logarithmusfunktion. Überzeuge dich mit einer Testfunktion von der Richtigkeit deines Programms.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nDie Ableitung lautet \\[\n\\frac{d}{dx}v_3(x) = \\frac{\\frac{v_1'(x)}{v_1(x)}\\cdot \\ln(v_2(x))-\\ln(v_1(x))\\cdot\\frac{v_2'(x)}{v_2(x)}}{\\ln^2(v_2(x))}\n\\]\n\n\nCode\ndef log(x, b = math.e):\n    if type(b) in (float, int):\n        newValue = math.log(x.value, b)\n        newDerivative = 1 / (x.value * math.log(b)) * x.derivative\n    else:\n        newValue = math.log(x.value, b.value)\n        newDerivative = (x.derivative/x.value * math.log(b.value) - math.log(x.value) * b.derivative / b.value) \\\n            / math.pow(math.log(b.value), 2)\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\n\n\n3.4.3 Die trigonometrischen Funktionen und ihre Umkehrfunktionen\nBei den trigonometrischen Funktionen und den Arcus Funktionen können wir einfach die bekannten Ableitungsregeln verwenden.\n\nÜbungsaufgabe 3.17 (Trigonometrische Funktionen) \nErgänze die Datei mathsad.py mit den Funktionen sin, cos und tan, sowie den Funktionen asin, acos und atan.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nBeachte, dass man für tan einfach \\(\\tan(x)=\\frac{\\sin(x)}{\\cos(x)}\\) verwenden kann, wenn sin und cos bereits implementiert sind.\n\n\nCode\ndef sin(x):\n    newValue = math.sin(x.value)\n    newDerivative = math.cos(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef cos(x):\n    newValue = math.cos(x.value)\n    newDerivative = -math.sin(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef tan(x):\n    return sin(x) / cos(x)\n\ndef asin(x):\n    newValue = math.asin(x.value)\n    newDerivative = 1/math.sqrt( 1 - math.pow(x.value, 2)) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef acos(x):\n    newValue = math.acos(x.value)\n    newDerivative = -1/math.sqrt( 1 - math.pow(x.value, 2)) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef atan(x):\n    newValue = math.atan(x.value)\n    newDerivative = 1/(math.pow(x.value, 2) + 1) * x.derivative\n    return FloatSad(newValue, newDerivative) \n\n\n\n\n\n\n\n3.4.4 Die hyperbolischen Funktionen und ihre Umkehrfunktionen\nAuch bei den hyperbolischen Funktionen und den Area Funktionen verwenden wir die bekannten Ableitungsregeln.\n\nÜbungsaufgabe 3.18 (Hyperbolische Funktionen) \nErgänze die Datei mathsad.py mit den Funktionen sinh, cosh und tanh, sowie den Funktionen asinh, acosh und atanh.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nWie bei den trigonometrischen Funktionen gilt auch hier \\(\\tanh(x)=\\frac{\\sinh(x)}{\\cosh(x)}\\).\n\n\nCode\ndef sinh(x):\n    newValue = math.sinh(x.value)\n    newDerivative = math.cosh(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef cosh(x):\n    newValue = math.cosh(x.value)\n    newDerivative = math.sinh(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef tanh(x):\n    return sinh(x) / cosh(x)\n\ndef asinh(x):\n    newValue = math.asinh(x.value)\n    newDerivative = 1/math.sqrt(math.pow(x.value, 2) + 1) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef acosh(x):\n    newValue = math.acosh(x.value)\n    newDerivative = 1/math.sqrt(math.pow(x.value, 2) - 1) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef atanh(x):\n    newValue = math.atanh(x.value)\n    newDerivative = -1/(math.pow(x.value, 2) - 1) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\n\n\n3.4.5 Die Betragsfunktion\nSchliesslich ergänzen wir die Datei mathsad.py noch mit der Funktion fabs wie oben beschrieben:\n\n\nCode\ndef fabs(x):\n    newValue = math.fabs(x.value)\n    newDerivative = x.derivative if x>=0 else -x.derivative\n    return FloatSad(newValue, newDerivative)\n\n\n\nBeispiel 3.2 (Motivation für die Funktion fabs) Dieses Beispiel stammt aus Griewank und Walther (2008), S. 24. Die Funktion \\(f(x) = \\sqrt{x^6} = |x|^3\\) ist auch an der Stelle \\(x_0=0\\) differenzierbar, aber die Wurzelfunktion wie auch die Betragsfunktion sind es nicht. Die folgenden Programme scheitern an dieser Schwierigkeit.\n\n\nCode\nfrom floatsad import FloatSad\n\ndef f(x):\n    x = FloatSad(x)\n    v = x**6\n    y = v**0.5\n    return y\n\nx0 = 0\nprint(f(x0)) # math domain error\n\n\nDer rationale Exponent (Wurzel) führt zu einem Fehler.\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\n\ndef g(x):\n    x = FloatSad(x)\n    v = x**6\n    y = mathsad.sqrt(v)\n    return Y\n\nx0 = 0\nprint(g(x0)) # float division by zero\n\n\nDie Auswertung der Ableitung der Wurzelfunktion an der Stelle 0 erzeugt einen Fehler. Mit unserer Funktion fabs erhalten wir jedoch den korrekten Wert:\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\n\ndef h(x):\n    x = FloatSad(x)\n    v = mathsad.fabs(x)\n    y = v**3\n    return y\n\nx0 = 0\nprint(h(x0)) # funktioniert\n\n\n< 0.0 ; 0.0 >\n\n\n\n\n\n\n\nDas fertige Modul kann auch von hier kopiert werden."
  },
  {
    "objectID": "ADOneDimManually.html#das-modul-mathsad-im-einsatz",
    "href": "ADOneDimManually.html#das-modul-mathsad-im-einsatz",
    "title": "3  Standard Algorithmische Differentiation für eindimensionale Funktionen",
    "section": "3.5 Das Modul mathsad im Einsatz",
    "text": "3.5 Das Modul mathsad im Einsatz\nNun können wir unser Modul mit\n\nimport mathsad\n\neinbinden und verwenden. Die Funktion aus Übungsaufgabe 3.4 beispielsweise können wir nun direkt hinschreiben:\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\n\ndef f(x):\n    x = FloatSad(x)\n    y = mathsad.cos(x**2 + 2) * mathsad.exp(-1/2 * x**2) + 1/x\n    return y\n\nx0 = -2\nprint(f(x0))\n\n\n&lt; -0.3700550823007931 ; -0.14136926695938976 &gt;\n\n\n\nÜbungsaufgabe 3.21 (Verwendung von mathsad) \nVerwende das Modul mathsad, um die Lösung von Übungsaufgabe 3.5 zu vereinfachen. Bestimme die Ableitung von \\(f\\) an der Stelle \\(x_0=\\sqrt{2}\\).\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport math\nimport mathsad\n\ndef f(x):\n    x = FloatSad(x)\n    u = x**2 + 1\n    y = mathsad.log(u) / mathsad.sqrt(u + x)\n    return y\n\nx0 = math.sqrt(2)\nf0 = f(x0)\nprint(f0.derivative)\n\n\n0.22198842685304976\n\n\n\n\n\n\nÜbungsaufgabe 3.22 (Billard-Problem mit mathsad) \nVerwende das Modul mathsad, um die Lösung des Billard-Problems aus Übungsaufgabe 3.7 zu vereinfachen. Programmiere dazu nochmals die Funktion f(x), aber verwende aussagekräfigere Variablen. Weil f nun FloatSad-Objekte zurückgibt, muss auch die Funktion newton(f, x0) angepasst werden. Die Funktion main kann aus der obigen Lösung kopiert werden.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nIn der Funktion newton(f, x0) muss lediglich die Berechnung des neuen Näherungswertes angepasst werden durch x1 = x0 - y0.value / y0.derivative.\n\n\nCode\nfrom floatsad import FloatSad\nimport math\nimport mathsad\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    # Parameter a, px, py werden im global space gefunden\n    x = FloatSad(x)\n    Xx, Xy = mathsad.cos(x), mathsad.sin(x) # Koordinaten von X\n    tx, ty = -Xy, Xx                        # Komponenten des Tangentialvektors\n    XPx, XPy = px - Xx, py - Xy             # Komponenten des Vektors XP\n    lXP = mathsad.sqrt(XPx**2 + XPy**2)     # Länge des Vektors XP\n    ePx, ePy = XPx / lXP, XPy / lXP         # Komponenten des Einheitsvektors in Richtung XP\n    XQx, XQy = a - Xx, -Xy                  # Komponenten des Vektors XQ\n    lXQ = mathsad.sqrt(XQx**2 + XQy**2)     # Länge des Vektors XQ\n    eQx, eQy = XQx / lXQ, XQy / lXQ         # Komponenten des Einheitsvektors in Richtung XQ\n    y = (ePx + eQx) * tx + (ePy + eQy) * ty # Skalarprodukt\n    return y\n\ndef newton(f, x0):\n    tol = 1e-8\n    y0 = f(x0)\n    x1 = x0 - y0.value / y0.derivative\n    while math.fabs(x1 - x0) &gt; tol:\n        x0 = x1\n        y0 = f(x0)\n        x1 = x0 - y0.value / y0.derivative\n    return x1\n\n# Parameter definieren\na = -0.5          # Position von Q = (a|0)\npx, py = 0.2, 0.6 # Position von P = (px|py)\n# Lösung des Billardproblems berechnen\nsol = set({}) # leere Menge, in der die gefundenen Lösungen gespeichert werden\nX = [2*math.pi * k / 10 for k in range(10)]  # Liste der Startwerte für Newton\nfor x0 in X:\n    x = newton(f, x0)\n    sol.add(x)\n# Lösungen grafisch darstellen\nfig = plt.figure()\nax = plt.gca()\nax.set_xlim((-1.2, 1.2))\nax.set_ylim((-1.2, 1.2))\nax.set_aspect('equal')\ncircle = plt.Circle((0,0), 1, color='b', fill=False)\nqBall = plt.Circle((a,0), 0.02, color='k')\npBall = plt.Circle([px, py], 0.02, color='k')\nax.add_patch(circle)\nax.add_patch(qBall)\nax.add_patch(pBall)\nfor x in sol:\n    xcoords = [a, math.cos(x), px]\n    ycoords = [0, math.sin(x), py]\n    plt.plot(xcoords, ycoords, linewidth=1, linestyle='--')\nplt.show()\n\n\n\n\n\nAbbildung 3.3: Lösung des Billardproblems mit anderen Startwerten.\n\n\n\n\n\n\n\n\nÜbungsaufgabe 3.23 (Kürzeste Distanz mit mathsad) \nVerwende das Modul mathsad, um die Lösung von Übungsaufgabe 3.9 zu vereinfachen. Weil d nun FloatSad-Objekte zurückgibt, muss auch die Funktion gradient_descent(f, x0, lam) angepasst werden.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nIn der Funktion gradient_descent(f, x0, lam) muss lediglich die Berechnung des neuen Näherungswertes angepasst werden. Statt des Graphen wird hier nur das globale Minimum als Punkt ausgegeben.\n\n\nCode\nfrom floatsad import FloatSad\nimport math\nimport mathsad\n\ndef d(t):\n    t = FloatSad(t)\n    Px = 2 * mathsad.cos(t) - 1    # x-Koordinate von P\n    Py = 1.5 * mathsad.sin(t)      # y-Koordinate von P\n    Pz = 0                         # z-Koordinate von P\n    Qx = -3 * mathsad.sin(2*t)     # x-Koordinate von Q\n    Qy = 2 * mathsad.cos(2*t) + 1  # y-Koordinate von Q\n    Qz = 2 * mathsad.sin(2*t) + 1  # z-Koordinate von Q\n    y = mathsad.sqrt((Px-Qx)**2 + (Py-Qy)**2 + (Pz-Qz)**2)\n    return y\n\ndef gradient_descent(f, x0, lam):\n    tol = 1e-9\n    # Erster Schritt berechnen\n    y0 = f(x0)\n    x1 = x0 - lam * y0.derivative\n    while math.fabs(x1-x0) &gt; tol:\n        x0 = x1\n        y0 = f(x0)\n        x1 = x0 - lam * y0.derivative\n    return x1\n\n\nt0 = 3\ntmin = gradient_descent(d, t0, 0.01)\ndmin = d(tmin)\nprint(\"Minimum bei (\", tmin, dmin.value, \")\")\n\n\nMinimum bei ( 4.712388977478413 1.5 )\n\n\n\n\n\n\n\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, und Jeffrey Mark Siskind. 2018. „Automatic Differentiation in Machine Learning: a Survey“. Journal of Machine Learning Research 18 (153): 1–43. http://jmlr.org/papers/v18/17-468.html.\n\n\nGander, Walter. 1992. Computermathematik. Birkhäuser.\n\n\nGriewank, Andreas, und Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2. Aufl. Other Titles in Applied Mathematics 105. Philadelphia, PA: SIAM. http://bookstore.siam.org/ot105/.\n\n\nHenrard, Marc. 2017. Algorithmic Differentiation in Finance Explained. Financial Engineering Explained. Cham: Palgrave Macmillan. https://doi.org/10.1007/978-3-319-53979-9.\n\n\nSlater, Max. 2022. „Differentiable programming from scratch“. Juli 2022. https://thenumb.at/Autodiff/."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arens, Tilo, Frank Hettlich, Christian Karpfinger, Ulrich Kockelkorn,\nKlaus Lichtenegger, and Hellmuth Stachel. 2022. Mathematik.\nBerlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul,\nand Jeffrey Mark Siskind. 2018. “Automatic Differentiation in\nMachine Learning: A Survey.” Journal of Machine Learning\nResearch 18 (153): 1–43. http://jmlr.org/papers/v18/17-468.html.\n\n\nGander, Walter. 1992. Computermathematik. Birkhäuser.\n\n\n———. 2015. Learning MATLAB: A Problem Solving\nApproach. 1st ed. UNITEXT. Cham, Switzerland: Springer\nInternational Publishing.\n\n\nGriewank, Andreas, and Andrea Walther. 2008. Evaluating Derivatives:\nPrinciples and Techniques of Algorithmic\nDifferentiation. 2. ed. Other Titles in Applied Mathematics 105.\nPhiladelphia, PA: SIAM. http://bookstore.siam.org/ot105/.\n\n\nHenrard, Marc. 2017. Algorithmic Differentiation in Finance\nExplained. Financial Engineering Explained. Cham: Palgrave\nMacmillan. https://doi.org/10.1007/978-3-319-53979-9.\n\n\nHromkovic, Juraj, Jarka Arnold, Cédric Donner, Urs Hauser, Matthias\nHauswirth, Tobias Kohn, Dennis Komm, David Maletinsky, and Nicole Roth.\n2021. INFORMATIK, Programmieren Und Robotik: Grundlagen\nDer Informatik für Schweizer\nMaturitätsschulen.\n\n\nRadcliffe, Sidney. 2021. “Reverse-Mode Automatic Differentiation\nfrom Scratch, in Python.” May 22, 2021. https://sidsite.com/posts/autodiff/.\n\n\nSlater, Max. 2022. “Differentiable Programming from\nScratch.” July 2022. https://thenumb.at/Autodiff/.\n\n\nWeitz, Edmund. 2021. Konkrete Mathematik (Nicht Nur)\nfür Informatiker. 2nd ed. Wiesbaden, Germany: Springer\nSpektrum."
  },
  {
    "objectID": "HigherDimFunctions.html#funktionen-mathbbrrightarrowmathbbrm",
    "href": "HigherDimFunctions.html#funktionen-mathbbrrightarrowmathbbrm",
    "title": "4  Funktionen mit mehreren In- und Outputs",
    "section": "4.1 Funktionen \\(\\mathbb{R}\\rightarrow\\mathbb{R}^m\\)",
    "text": "4.1 Funktionen \\(\\mathbb{R}\\rightarrow\\mathbb{R}^m\\)\nEine vektorwertige Funktion \\(f : \\mathbb{R}\\rightarrow\\mathbb{R}^m\\) mit\n\\[\nf(t) = \\left( \\begin{align*} y_1 &(t) \\\\ &\\vdots \\\\ y_m &(t) \\end{align*}  \\right)\n\\]\nkann man sich als eine Kurve in einem \\(m\\)-dimensionalen Raum vorstellen. Im Beispiel Beispiel 1.3 wird beispielsweise die Bahn des Punktes \\(Q\\) durch die Funktion\n\\[\nf(t) = \\left( \\begin{align*} -3 &\\sin(2t) \\\\ 2 &\\cos(2t) + 1 \\\\ 2 &\\sin(2t) + 1 \\end{align*}  \\right)\n\\]\nbeschrieben. Die Ableitung einer solchen Funktion wird komponentenweise berechnet und gibt zu einem bestimmten Zeitpunkt \\(t_0\\) den Tangentialvektor im Kurvenpunkt \\(f(t_0)\\) an:\n\\[\ndf(t) = \\left( \\begin{align*} \\dot y_1 &(t) \\\\ &\\vdots \\\\ \\dot y_m &(t) \\end{align*}  \\right)\n\\]\nPhysikalisch entspricht dies dem Geschwindigkeitsvektor zum Zeitpunkt \\(t_0\\).\nAls Programm können wir die obige Kurve so darstellen\n\n\nCode\nimport math\n\ndef f(t):\n    y1 = -3*math.sin(2*t)\n    y2 = 2*math.cos(2*t) + 1\n    y3 = 2*math.sin(2*t) + 1\n    y = [y1, y2, y3]\n    return y\n\nt0 = 2\ny0 = f(t0)\nprint(y0)\n\n\n[2.2704074859237844, -0.3072872417272239, -0.5136049906158564]\n\n\nFür die Ableitung können wir unser Modul FloatSad benutzen. Der Rückgabewert der Funktion ist dann ein Array mit drei FloatSad-Objekten.\n\n\nCode\nfrom floatsad import FloatSad \nimport mathsad\n\ndef f(t):\n    t = FloatSad(t)\n    y1 = -3*mathsad.sin(2*t)\n    y2 = 2*mathsad.cos(2*t) + 1\n    y3 = 2*mathsad.sin(2*t) + 1\n    y = [y1, y2, y3]\n    return y\n\nt0 = 2\ny0 = f(t0)\nprint(\"y1(\" + str(t0) + \") = \" + str(y0[0].value))\nprint(\"y1'(\" + str(t0) + \") = \" + str(y0[0].derivative))\n\n\ny1(2) = 2.2704074859237844\ny1'(2) = 3.921861725181672\n\n\nDiese Implementation hat den Nachteil, dass die Handhabung etwas kompliziert wird. Insbesondere kann man nicht einfach y0.value schreiben, um eine Liste der Funktionswerte zu erhalten. Abhilfe schafft dabei das Modul numpy und die Vektorisierung der Funktion:\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\nimport numpy as np\n\n@np.vectorize\ndef f(t):\n    t = FloatSad(t)\n    y1 = -3*mathsad.sin(2*t)\n    y2 = 2*mathsad.cos(2*t) + 1\n    y3 = 2*mathsad.sin(2*t) + 1\n    y = [y1, y2, y3]\n    return y\n\ngetValues = np.vectorize(lambda y : y.value)\ngetDerivatives = np.vectorize(lambda y : y.derivative)\n\nt0 = 2\ny0 = f(t0)\nprint(getValues(y0))\nprint(getDerivatives(y0))\n\n\n[ 2.27040749 -0.30728724 -0.51360499]\n[ 3.92186173  3.02720998 -2.61457448]"
  },
  {
    "objectID": "HigherDimFunctions.html#funktionen-f-mathbbrn-rightarrowmathbbrm",
    "href": "HigherDimFunctions.html#funktionen-f-mathbbrn-rightarrowmathbbrm",
    "title": "4  Funktionen mit mehreren In- und Outputs",
    "section": "4.2 Funktionen \\(f : \\mathbb{R}^n \\rightarrow\\mathbb{R}^m\\)",
    "text": "4.2 Funktionen \\(f : \\mathbb{R}^n \\rightarrow\\mathbb{R}^m\\)"
  },
  {
    "objectID": "HigherDimFunctions.html#funktionen-mit-mehreren-ausgabewerten",
    "href": "HigherDimFunctions.html#funktionen-mit-mehreren-ausgabewerten",
    "title": "4  Funktionen mit mehreren In- und Outputs",
    "section": "4.1 Funktionen mit mehreren Ausgabewerten",
    "text": "4.1 Funktionen mit mehreren Ausgabewerten\nEine vektorwertige Funktion \\(f : \\mathbb{R}\\rightarrow\\mathbb{R}^m\\) mit\n\\[\nf(t) = \\begin{pmatrix} y_1(t) \\\\ \\vdots \\\\ y_m(t) \\end{pmatrix}\n\\]\nkann man sich als eine Kurve in einem \\(m\\)-dimensionalen Raum vorstellen. Im Beispiel 1.3 wird etwa die Bahn des Punktes \\(Q\\) durch die Funktion\n\\[\nf(t) = \\left( \\begin{align*} -3 &\\sin(2t) \\\\ 2 &\\cos(2t) + 1 \\\\ 2 &\\sin(2t) + 1 \\end{align*}  \\right)\n\\]\nbeschrieben. Die Ableitung einer solchen Funktion wird komponentenweise berechnet und gibt zu einem bestimmten Zeitpunkt \\(t_0\\) den Tangentialvektor im Kurvenpunkt \\(f(t_0)\\) an:\n\\[\n\\dot{f}(t_0) = \\begin{pmatrix} \\dot y_1(t_0) \\\\ \\vdots \\\\ \\dot y_m(t_0) \\end{pmatrix}\n\\]\nPhysikalisch entspricht dies dem Geschwindigkeitsvektor zum Zeitpunkt \\(t_0\\). Mehr über die Ableitung von Parameterkurven findet man z.B. in Arens u. a. (2022), S. 947.\nAls Programm können wir die obige Kurve so darstellen\n\n\nCode\nimport math\n\ndef f(t):\n    y1 = -3*math.sin(2*t)\n    y2 =  2*math.cos(2*t) + 1\n    y3 =  2*math.sin(2*t) + 1\n    y = [y1, y2, y3]\n    return y\n\nt0 = 2\ny0 = f(t0)\nprint(y0)\n\n\n[2.2704074859237844, -0.3072872417272239, -0.5136049906158564]\n\n\nFür die Ableitung können wir unser Modul FloatSad benutzen. Der Rückgabewert der Funktion ist dann eine Liste mit drei FloatSad-Objekten.\n\n\nCode\nfrom floatsad import FloatSad \nimport mathsad\n\ndef f(t):\n    t = FloatSad(t)\n    y1 = -3*mathsad.sin(2*t)\n    y2 =  2*mathsad.cos(2*t) + 1\n    y3 =  2*mathsad.sin(2*t) + 1\n    y = [y1, y2, y3]\n    return y\n\nt0 = 2\ny0 = f(t0)\nprint(\"y1(\" + str(t0) + \") = \" + str(y0[0].value))\nprint(\"y1'(\" + str(t0) + \") = \" + str(y0[0].derivative))\n\n\ny1(2) = 2.2704074859237844\ny1'(2) = 3.921861725181672\n\n\nDiese Implementation hat den Nachteil, dass die Handhabung etwas kompliziert wird. Insbesondere kann man nicht einfach y0.value schreiben, um eine Liste der Funktionswerte zu erhalten. Abhilfe schafft dabei das Modul numpy. Wir verwenden daraus die Möglichkeit, Funktionen zu vektorisieren, um zwei Funktionen getValues(y) und getDerivatives(y) zu definieren, welche aus der Liste y von FloatSad-Objekten jeweils die Funktionswerte, respektive die Werte der Ableitungen extrahieren.\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\nimport numpy as np\n\ndef f(t):\n    t = FloatSad(t)\n    y1 = -3*mathsad.sin(2*t)\n    y2 =  2*mathsad.cos(2*t) + 1\n    y3 =  2*mathsad.sin(2*t) + 1\n    y = [y1, y2, y3]\n    return y\n\ngetValues = np.vectorize(lambda y : y.value)\ngetDerivatives = np.vectorize(lambda y : y.derivative)\n\nt0 = 2\ny0 = f(t0)\nprint(getValues(y0))\nprint(getDerivatives(y0))\n\n\n[ 2.27040749 -0.30728724 -0.51360499]\n[ 3.92186173  3.02720998 -2.61457448]"
  },
  {
    "objectID": "HigherDimFunctions.html#funktionen-mit-mehreren-eingabewerten",
    "href": "HigherDimFunctions.html#funktionen-mit-mehreren-eingabewerten",
    "title": "4  Funktionen mit mehreren In- und Outputs",
    "section": "4.2 Funktionen mit mehreren Eingabewerten",
    "text": "4.2 Funktionen mit mehreren Eingabewerten\nDie Ableitung einer Funktion \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) ist der Gradient\n\\[\n\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)\n\\]\nFür weitere Details zum Gradienten sei auf Arens u. a. (2022), S. 870 verwiesen.\n\nBeispiel 4.1 (Eine Funktion mit drei Eingabwerten) \nBetrachten wir als Beispiel die Funktion \\(f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}\\)\n\\[\nf(x_0, x_1, x_2) = x_0^2 + 2\\cdot x_0 \\cdot x_1 - \\frac{x_1}{x_2 ^3}\n\\]\nDas folgende Programm berechnet den Funktionswert \\(f(1, 2, 3)=\\frac{133}{27}\\approx 4.9259...\\)\n\n\nCode\ndef f(x):\n    y = x[0]**2 + 2*x[0]*x[1] - x[1]/x[2]**3\n    return y\n\nx0 = [1, 2, 3]\ny0 = f(x0)\nprint(y0)\n\n\n4.925925925925926\n\n\nDer Gradient dieser Funktion ist \\[\n\\nabla f = \\left( 2x_0+2x_1, 2x_0 - \\frac{1}{x_2^3}, 3\\frac{x_1}{x_2^4} \\right)\n\\]\nbzw. ausgewertet an der Stelle \\((x_0, x_1, x_2) = (1, 2, 3)\\) \\[\n\\begin{align*}\n\\nabla f \\vert _{(1, 2, 3)} &= \\left( 6, \\frac{53}{27}, \\frac2{27} \\right) \\\\\n&\\approx \\left( 6, 1.9629..., 0.0740... \\right)\n\\end{align*}\n\\]\n\n\nMit der Standard Algorithmischen Differentiation kann der Gradient nicht in einem Durchgang berechnet werden. Bei der Umwandlung der Anfangswerte in FloatSad-Objekte müssen wir allen Variablen in \\(x = (x_1, \\ldots, x_n)\\) einen Anfangswert \\(\\dot{x} = (\\dot x_1, \\ldots, \\dot x_n)\\) geben. Wenn wir für die Initialisierung \\(\\dot{x} = e_i = (0, \\ldots, 1, \\ldots, 0)\\) verwenden (mit \\(1\\) an der \\(i\\)-ten Stelle und sonst lauter \\(0\\)), dann bekommen wir den Wert der \\(i\\)-ten partiellen Ableitung \\(\\frac{\\partial f}{\\partial x_i}\\).\nUm die Funktion im obigen Beispiel mit unserer Klasse FloatSad abzuleiten, verwenden wir wieder numpy. Als erstes definieren wir eine vektorisierte Funktion float2FloatSad, mit der wir aus der Liste x eine Liste von FloatSad-Objekten erzeugen. Die Werte der Ableitungen werden zu Beginn explizit in der Variablen xdot initialisiert.\n\nAbleitung nach x0Ableitung nach x1Ableitung nach x2\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport numpy as np\n\ndef f(x):\n    xdot = [1, 0, 0]\n    x = float2FloatSad(x, xdot)\n    y = x[0]**2 + 2*x[0]*x[1] - x[1]/x[2]**3\n    return y\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\n\nx0 = [1, 2, 3]\ny0 = f(x0)\nprint(y0)\n\n\n< 4.925925925925926 ; 6.0 >\n\n\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport numpy as np\n\ndef f(x):\n    xdot = [0, 1, 0]\n    x = float2FloatSad(x, xdot)\n    y = x[0]**2 + 2*x[0]*x[1] - x[1]/x[2]**3\n    return y\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\n\nx0 = [1, 2, 3]\ny0 = f(x0)\nprint(y0)\n\n\n< 4.925925925925926 ; 1.962962962962963 >\n\n\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport numpy as np\n\ndef f(x):\n    xdot = [0, 0, 1]\n    x = float2FloatSad(x, xdot)\n    y = x[0]**2 + 2*x[0]*x[1] - x[1]/x[2]**3\n    return y\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\n\nx0 = [1, 2, 3]\ny0 = f(x0)\nprint(y0)\n\n\n< 4.925925925925926 ; 0.07407407407407407 >\n\n\n\n\n\nInitialisiert man die Ableitungen beispielsweise mit xdot = [1, 1, 1], dann erhält man die Summe der drei Richtungsableitungen:\n\n\n< 4.925925925925926 ; 8.037037037037036 >\n\n\nAllgemein gilt: Initialisiert man xdot mit dem Vektor \\(\\vec r = (r_1, \\ldots, r_n)^\\intercal\\), dann erhält man das Skalarprodukt \\[\n\\nabla f \\cdot \\vec r =\n\\left ( \\left .\\frac{\\partial f}{\\partial x_1} \\right \\vert_{(x_1, \\ldots, x_n)}, \\ldots, \\left .\\frac{\\partial f}{\\partial x_n} \\right \\vert_{(x_1, \\ldots, x_n)}  \\right ) \\cdot \\begin{pmatrix} r_1 \\\\ \\vdots \\\\ r_n \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "HigherDimFunctions.html",
    "href": "HigherDimFunctions.html",
    "title": "4  Funktionen mit mehreren In- und Outputs",
    "section": "",
    "text": "5 Funktionen mit mehreren Ein- und Ausgabewerten\nEine Funktion \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) hat die Form \\[\nf(x_1, \\ldots, x_n) = \\left( \\begin{align*} y_1(x_1, &\\ldots, x_n) \\\\ &\\vdots \\\\ y_m(x_1, &\\ldots, x_n) \\end{align*} \\right)\n\\] Die Ableitung einer solchen Funktion wird durch die Jakobi Matrix \\[\nJf = \\begin{pmatrix}\n\\frac{\\partial y_1}{\\partial x_1} & \\ldots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n\\vdots & & \\vdots \\\\\n\\frac{\\partial y_m}{\\partial x_1} & \\ldots & \\frac{\\partial y_m}{\\partial x_n}\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "HigherDimFunctions.html#funktionen-mit-mehreren-ein--und-ausgabewerten",
    "href": "HigherDimFunctions.html#funktionen-mit-mehreren-ein--und-ausgabewerten",
    "title": "4  Funktionen mit mehreren In- und Outputs",
    "section": "4.3 Funktionen mit mehreren Ein- und Ausgabewerten",
    "text": "4.3 Funktionen mit mehreren Ein- und Ausgabewerten\nEine Funktion \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) hat die Form \\[\nf(x_1, \\ldots, x_n) = \\left( \\begin{align*} y_1(x_1, &\\ldots, x_n) \\\\ &\\vdots \\\\ y_m(x_1, &\\ldots, x_n) \\end{align*} \\right)\n\\] Die Ableitung einer solchen Funktion wird durch die Jacobi Matrix \\[\nJf = \\begin{pmatrix}\n    \\frac{\\partial y_1}{\\partial x_1} & \\ldots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n    \\vdots & & \\vdots \\\\\n    \\frac{\\partial y_m}{\\partial x_1} & \\ldots & \\frac{\\partial y_m}{\\partial x_n}\n\\end{pmatrix}\n\\in\\mathbb{R}^{m\\times n}\n\\] gegeben. Auch hierzu findet der Leser mehr Informationen in Arens u. a. (2022), S. 878.\n\nBeispiel 4.2 (Eine Funktion mit zwei Ein- und drei Ausgabwerten) \nBetrachte die Funktion \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) \\[\nf(x_0, x_1) =\n    \\begin{pmatrix}\n        x_0\\cdot \\sqrt{x_1} + 3x_1 \\\\\n        \\cos(x_0) / x_1 \\\\\n        e^{x_0 ^2\\cdot x_1}\n    \\end{pmatrix}\n\\]\nDie Jacobi Matrix lautet in diesem Fall \\[\nJf =\n\\begin{pmatrix}\n    \\sqrt{x_1} & \\frac{x_0}{2\\sqrt{x_1}} + 3 \\\\\n    -\\frac{\\sin(x_0)}{x_1} & -\\frac{\\cos(x_0)}{x_1^2} \\\\\n    e^{x_0^2\\cdot x_1}\\cdot 2 x_0 x_1 & e^{x_0^2\\cdot x_1}\\cdot x_0^2\n\\end{pmatrix}\n\\]\nAusgewertet an der Stelle \\((x_0, x_1) = (2, 1)\\) ergibt dies \\[\nf(2,1) \\approx \\begin{pmatrix} 5 \\\\ -0.4161... \\\\ 54.5981... \\end{pmatrix}, \\qquad\nJF \\vert _{(2,1)} \\approx\n    \\begin{pmatrix}  \n        1 & 4 \\\\\n        -0.9092... & 0.4161... \\\\\n        218.3926... & 218.3926...\n    \\end{pmatrix}\n\\]\n\n\nUm die Funktion aus dem Beispiel mit SAD abzuleiten kombinieren wir die Techniken aus den beiden vorherigen Abschnitten. Je nach Initialisierung von xdot erhalten wir die erste oder die zweite Spalte von \\(JF\\).\n\n1. Spalte2. Spalte\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\nimport numpy as np\n\ndef f(x):\n    xdot = [1, 0]\n    x = float2FloatSad(x, xdot)\n    y1 = x[0]*mathsad.sqrt(x[1]) + 3*x[1]\n    y2 = mathsad.cos(x[0]) / x[1]\n    y3 = mathsad.exp(x[0]**2 * x[1])\n    return [y1, y2, y3]    \n\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\ngetValues = np.vectorize(lambda y : y.value)\ngetDerivatives = np.vectorize(lambda y : y.derivative)\n\n\nx0 = (2, 1)\ny0 = f(x0)\nprint(\"Funktionswerte:\")\nprint(getValues(y0))\nprint(\"1. Spalte von Jf:\")\nprint(getDerivatives(y0))\n\n\nFunktionswerte:\n[ 5.         -0.41614684 54.59815003]\n1. Spalte von Jf:\n[  1.          -0.90929743 218.39260013]\n\n\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\nimport numpy as np\n\ndef f(x):\n    xdot = [0, 1]\n    x = float2FloatSad(x, xdot)\n    y1 = x[0]*mathsad.sqrt(x[1]) + 3*x[1]\n    y2 = mathsad.cos(x[0]) / x[1]\n    y3 = mathsad.exp(x[0]**2 * x[1])\n    return [y1, y2, y3]    \n\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\ngetValues = np.vectorize(lambda y : y.value)\ngetDerivatives = np.vectorize(lambda y : y.derivative)\n\n\nx0 = (2, 1)\ny0 = f(x0)\nprint(\"Funktionswerte:\")\nprint(getValues(y0))\nprint(\"2. Spalte von Jf:\")\nprint(getDerivatives(y0))\n\n\nFunktionswerte:\n[ 5.         -0.41614684 54.59815003]\n2. Spalte von Jf:\n[  4.           0.41614684 218.39260013]\n\n\n\n\n\nInitialisiert man allgemein xdot mit dem Vektor \\(r = (r_1, \\ldots, r_n)^\\intercal\\), dann erhält man als Resultat das Produkt \\[\nJf \\cdot r =\n\\begin{pmatrix}\n    \\frac{\\partial y_1}{\\partial x_1} & \\ldots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n    \\vdots & & \\vdots \\\\\n    \\frac{\\partial y_m}{\\partial x_1} & \\ldots & \\frac{\\partial y_m}{\\partial x_n}\n\\end{pmatrix}\n\\cdot \\begin{pmatrix} r_1 \\\\ \\vdots \\\\ r_n \\end{pmatrix}\n\\]\nBraucht man die gesammte Jacobi Matrix, dann muss man also die Funktion so oft aufrufen, wie die Matrix Spalten hat, d.h. len(x) Mal. Die SAD Methode ist also effizient, wenn eine Funktion mehr Aus- als Eingabewerte hat. Der ineffizienteste Fall tritt auf, wenn die Funktion aus vielen Eingabewerte nur einen Ausgabewert berechnet. Mit anderen Worten: Das Bestimmen des Gradienten einer Funktion \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) benötigt den grössten Aufwand gemessen an der Anzahl der zu berechnenden Werte. Abhilfe schafft in so einem Fall die Adjungierte Automatische Differentiation (AAD).\nZum Schluss sei noch angemerkt, dass die Definition der drei Funktionen float2FloatSad, getValues und getDerivatives in die Datei floatsad.py geschrieben werden könnten (beachte, dass sie nicht eingerückt werden wie die restlichen Befehle der Klasse). Dann vereinfacht sich das obige Programm:\n\n\nCode\nfrom floatsad import *\nimport mathsad\n\ndef f(x):\n    xdot = [1, 0]\n    x = float2FloatSad(x, xdot)\n    y1 = x[0]*mathsad.sqrt(x[1]) + 3*x[1]\n    y2 = mathsad.cos(x[0]) / x[1]\n    y3 = mathsad.exp(x[0]**2 * x[1])\n    return [y1, y2, y3]    \n\nx0 = (2, 1)\ny0 = f(x0)\nprint(getValues(y0))\nprint(getDerivatives(y0))\n\n\n\n\n\n\nArens, Tilo, Frank Hettlich, Christian Karpfinger, Ulrich Kockelkorn, Klaus Lichtenegger, und Hellmuth Stachel. 2022. Mathematik. Berlin, Heidelberg: Springer Berlin Heidelberg."
  },
  {
    "objectID": "aad.html",
    "href": "aad.html",
    "title": "5  Adjungierte Algorithmische Differentiation",
    "section": "",
    "text": "In Kapitel 4 haben wir gesehen, dass wir mit der Standard Algorithmischen Differentiation (SAD) alle \\(m\\) Ableitungen einer Funktion \\(f : \\mathbb{R} \\rightarrow \\mathbb{R}^m\\) mit einem einzigen Funktionsaufruf berechnen können. Die Berechnung des Gradienten einer Funktion \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) benötigt jedoch \\(n\\) Funktionsaufrufe, nämlich einen für jede partielle Ableitung \\(\\partial f / \\partial x_i\\). In diesem Kapitel wollen wir eine Methode entwickeln, die alle \\(n\\) partiellen Ableitungen in einem Funktionsaufruf berechnet.\nÄhnlich wie die SAD beruht auch diese Methode darauf, dass wir eine komplizierte Funktion schrittweise mit Hilfe von elementaren Operationen berechnen und in jedem Schritt die Ableitungen in separaten Variablen akkumulieren. Wir führen also wieder unsere Konvention aus Kapitel Kapitel 1.2 ein. Auch dieses Mal werden wir in jedem Schritt die Kettenregel verwenden. Diesmal fangen wir jedoch am Ende der Funktion an und werden uns dann rückwärts durch alle Ableitungen arbeiten. Aus diesem Grund wird das Verfahren auch Rückwärts-AD 1 oder Adjungierte AD (AAD) genannt.1 Im Englischen spricht man von reverse mode differentiation weil backward differentiation für bestimmte Methoden zur Integration von Differentialgleichungen verwendet wird.\n\nBeispiel 5.1 (Gradient mit AAD) \nDieses Beispiel ist eine leicht abgeänderte Version von Radcliffe (2021) . Betrachten wir die Funktion \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) \\[\ny = f(x_0, x_1) = (x_0 + x_1) \\cdot x_0 - x_1\n\\]\nAls Programm können wir die Funktion unter Berücksichtigung der Konvention so schreiben:\n\n\nCode\ndef f(x0, x1):\n    v0 = x0\n    v1 = x1\n    v2 = v0 + v1\n    v3 = v2 * v0\n    v4 = v3 - v1\n    y = v4\n    return y\n\nx0, x1 = 2, 3\ny0 = f(x0, x1)\nprint(\"f(\" + str(x0) + \",\" + str(x1) + \") = \" + str(y0))\n\n\nf(2,3) = 7\n\n\n\n\n\n\n\n\n   \n\nnx0\n\nx 0   \n\nnPlus\n\n +   \n\nnx0->nPlus\n\n    \n\nnTimes\n\n *   \n\nnx0->nTimes\n\n    \n\nnx1\n\nx 1   \n\nnx1->nPlus\n\n    \n\nnMinus\n\n -   \n\nnx1->nMinus\n\n    \n\nnPlus->nTimes\n\n    \n\nnTimes->nMinus\n\n    \n\nny\n\ny   \n\nnMinus->ny\n\n   \n\n\nAbbildung 5.1: Computational Graph für y = (x0 + x1) * x0 - x1.\n\n\n\n\nDie partiellen Ableitungen von \\(f\\) lauten \\[\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_0} &= \\frac{\\partial f}{\\partial x_0} = (1+0)\\cdot x_0 + (x_0 + x_1)\\cdot 1 - 0=2x_0 + x_1 \\\\\n    \\frac{\\partial y}{\\partial x_1} &= \\frac{\\partial f}{\\partial x_1} = (0 + 1)\\cdot x_0 - 1 = x_0 - 1\n\\end{align*}\n\\] wobei für die Ableitung nach \\(x_0\\) die Produktregel verwendet wurde. Die partielle Ableitung \\(\\partial y / \\partial x_0\\) können wir auch berechnen, indem wir bei \\(y = v_4\\) anfangen und jeweils die Definition der Hilfsvariablen einsetzen: \\[\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_0} &= \\frac{\\partial v_4}{\\partial x_0} \\\\\n    &= \\frac{\\partial (v_3 - v_1)}{\\partial x_0} \\\\\n    &= \\frac{\\partial v_3}{\\partial x_0} - \\frac{\\partial v_1}{\\partial x_0} \\\\\n    &= \\frac{\\partial (v_2 \\cdot v_0)}{\\partial x_0} - \\frac{\\partial x_1}{\\partial x_0} \\\\\n    &= \\frac{\\partial v_2}{\\partial x_0} \\cdot v_0 + v_2 \\cdot \\frac{\\partial v_0}{\\partial x_0} - 0 \\\\\n    &= \\frac{\\partial (v_0 + v_1)}{\\partial x_0} \\cdot v_0 + v_2 \\cdot \\frac{\\partial x_0}{\\partial x_0} \\\\\n    &= \\left( \\frac{\\partial v_0}{\\partial x_0} + \\frac{\\partial v_1}{\\partial x_0} \\right) \\cdot x_0 + (v_0 + v_1) \\cdot 1 \\\\\n    &= \\left( \\frac{\\partial x_0}{\\partial x_0} + \\frac{\\partial x_1}{\\partial x_0} \\right) \\cdot x_0 + (x_0 + x_1) \\\\\n    &= (1 + 0) \\cdot x_0 + (x_0 + x_1) \\\\\n    &= 2x_0 + x_1   \n\\end{align*}\n\\]\nAnalog findet man \\(\\partial y / \\partial x_1\\) (diesmal lassen wir einige der offensichtlicheren Zwischenschritte weg): \\[\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_1} &= \\frac{\\partial v_4}{\\partial x_1} \\\\\n    &= \\frac{\\partial v_3}{\\partial x_1} - \\frac{\\partial v_1}{\\partial x_1} \\\\\n    &= \\frac{\\partial (v_2 \\cdot v_0)}{\\partial x_1} - 1 \\\\\n    &= \\frac{\\partial v_2}{\\partial x_1} \\cdot v_0 + v_2 \\cdot \\frac{\\partial v_0}{\\partial x_1} - 1 \\\\\n    &= \\frac{\\partial (v_0 + v_1)}{\\partial x_1} \\cdot x_0 + v_2 \\cdot 0 - 1 \\\\\n    &= \\left( \\frac{\\partial v_0}{\\partial x_1} + \\frac{\\partial v_1}{\\partial x_1} \\right) \\cdot x_0 - 1 \\\\\n    &= ( 0 + 1) \\cdot x_0 - 1 \\\\\n    &= x_0 - 1   \n\\end{align*}\n\\]\nUm die beiden Rechnungen zusammenzufassen, führen nun für jede Hilfsvariable \\(v_i\\) eine neue Variable \\(\\bar v_i\\) ein, welche definiert ist als \\[\n\\bar v_i = \\frac{\\partial y}{\\partial v_i}\n\\] Ähnlich wie die \\(\\dot v_i\\) aus der SAD speichern diese Variablen die Werte der Ableitungen. Die neue Notation soll anzeigen, dass es sich um die AAD Methode handelt. Unser Ziel ist es also, \\(\\bar v_0 = \\partial y / \\partial v_0 = \\partial y / \\partial x_0\\) und \\(\\bar v_1 = \\partial y / \\partial v_1 = \\partial y / \\partial x_1\\) zu bestimmen. Beginnen wir in der letzten Zeile des Programms, dann gilt offenbar immer \\(\\bar v_4 = \\partial y / \\partial v_4 = 1\\). In der Zeile darüber können wir \\(\\bar v_3\\) und \\(\\bar v_1\\) berechnen, indem wir die Kettenregel verwenden.\n\\[\\begin{align*}\n    \\bar v_3 &= \\frac{\\partial y}{\\partial v_3} = \\frac{\\partial y}{\\partial v_4} \\cdot \\frac{\\partial v_4}{\\partial v_3} = \\bar v_4 \\cdot (1-0)=\\bar v_4  \\\\\n\n    \\bar v_1 &= \\frac{\\partial y}{\\partial v_1} = \\frac{\\partial y}{\\partial v_4} \\cdot \\frac{\\partial v_4}{\\partial v_1} = \\bar v_4 \\cdot (0-1)= -\\bar v_4\n\\end{align*}\\]\nAlso sind \\(\\bar v_3 = 1\\) und \\(\\bar v_1 = -1\\). Der Zwischenwert in \\(\\bar v_1\\) wird später ergänzt werden. Aus der Zeile \\(v_3 = v_2 \\cdot v_0\\) lassen sich als nächstes Ausdrücke für \\(\\bar v_2\\) und \\(\\bar v_0\\) finden.\n\\[\\begin{align*}\n    \\bar v_2 &= \\frac{\\partial y}{\\partial v_2} = \\frac{\\partial y}{\\partial v_3} \\cdot \\frac{\\partial v_3}{\\partial v_2} = \\bar v_3 \\cdot v_0 \\\\\n\n    \\bar v_0 &= \\frac{\\partial y}{\\partial v_0} = \\frac{\\partial y}{\\partial v_3} \\cdot \\frac{\\partial v_3}{\\partial v_0} = \\bar v_3 \\cdot v_2\n\\end{align*}\\]\nMit den vorher berechneten Werten erhalten wir also \\(\\bar v_2 = v_0\\) und \\(\\bar v_0 = v_2\\). Beide Werte sind durch die Funktion bereits berechnet worden. Im obigen Beispiel gilt etwa v0 = x0 = 2 und v2 = x0 + x1 = 5. Auch diese Zwischenwerte werden im nächsten Schritt ergänzt. Aus der Zeile \\(v_2 = v_0 + v_1\\) ergibt sich nämlich\n\\[\\begin{align*}\n    \\bar v_0 &= \\bar v_0 + \\frac{\\partial y}{\\partial v_0} = \\bar v_0 + \\frac{\\partial y}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial v_0} \\\\\n    &= \\bar v_0 + \\bar v_2 \\cdot (1+0) = \\bar v_0 + \\bar v_2 \\\\ & \\\\\n\n    \\bar v_1 &= \\bar v_1 + \\frac{\\partial y}{\\partial v_1} = \\bar v_1 + \\frac{\\partial y}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial v_1} \\\\\n    &= \\bar v_1 + \\bar v_2 \\cdot (0+1) = \\bar v_1 + \\bar v_2\n\\end{align*}\\]\nMit den bereits bekannten Werten erhalten wir \\(\\bar v_0 = v_2 + v_0\\) (bzw. mit den konkreten Werten des Beispiels v0bar = 5 + 2) und \\(\\bar v_1 = -1 + v_0\\) (bzw. v1bar = -1 + 2). Nun enthalten die Variablen \\(\\bar v_0\\) und \\(\\bar v_1\\) die Werte der gewünschten Ableitungen, nämlich \\(\\bar v_0 = (v_0 + v_1) + v_0 = 2x_0 + x_1\\) und \\(\\bar v_1 = -1 + x_0\\). Wir können aber die letzten Schritte analog zu den vorherigen ausführen:\n\\[\\begin{align*}\n    \\bar x_1 &= \\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial x_1} = \\bar v_1 \\cdot 1  \\\\\n\n    \\bar x_0 &= \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial y}{\\partial v_0} \\cdot \\frac{\\partial v_0}{\\partial x_0} = \\bar v_0 \\cdot 1  \\\\\n\\end{align*}\\]\nDie Schwierigkeit besteht darin, dass wir nicht wie bei der SAD in jedem Schritt die Variable \\(v_i\\) und gleichzeitig die Variable \\(\\dot v_i\\) berechnen können. Um die \\(\\bar v_i\\) zu bestimmen muss man zuerst die Funktion komplett ausführen, und sich dabei den Aufbau des Computational Graph merken. Erst dann kann man rückwärts die Ableitungswerte berechnen, angefangen bei der letzten Hilfsvariablen \\(\\bar v_4 = 1\\). Das folgende Schema fasst die obigen Rechnungen zusammen.\n\\[\\begin{equation*}\n\\left \\downarrow\n    \\begin{aligned}[c]\n        v_0 &= x_0 \\\\\n        v_1 &= x_1 \\\\\n        v_2 &= v_0 + v_1 \\\\\n        v_3 &= v_2 \\cdot v_0 \\\\\n        v_4 &= v_3 - v_1 \\\\\n        y &= v_4\n    \\end{aligned}  \n\\right .\n\n\\qquad\n\n\\begin{aligned}[c]\n    & \\\\\n    & \\\\\n    & \\\\\n    & \\\\\n    & \\\\\n    &\\longrightarrow\n\\end{aligned}  \n\n\\qquad\n\n\\left \\uparrow\n    \\begin{aligned}[c]\n        \\bar x_0 &= \\bar v_0 = 2\\cdot x_0 + x_1 \\\\\n        \\bar x_1 &= \\bar v_1 = -1 + x_0 \\\\\n        \\bar v_0 &= \\bar v_0 + \\bar v_2, \\quad \\bar v_1 = \\bar v_1 + \\bar v_2 \\\\\n        \\bar v_2 &= \\bar v_3 \\cdot v_0, \\quad \\bar v_0 = \\bar v_3 \\cdot v_2 \\\\\n        \\bar v_3 &= \\bar v_4, \\quad \\bar v_1 = -\\bar v_4 \\\\\n        \\bar v_4 &= \\bar y = 1\n    \\end{aligned}  \n\\right .\n\\end{equation*}\\]\n\n\nCode\ndef f(x0, x1):\n    v0 = x0\n    v1 = x1\n    v2 = v0 + v1\n    v3 = v2 * v0\n    v4 = v3 - v1\n    y = v4\n    v4bar = 1\n    v3bar = v4bar\n    v1bar = -v4bar\n    v2bar = v3bar * v0\n    v0bar = v3bar * v2\n    v0bar = v0bar + v2bar\n    v1bar = v1bar + v2bar\n    grad = [v0bar, v1bar]\n    return [y, grad]\n\nx0, x1 = 2, 3\n[y0, dy] = f(x0, x1)\nprint(\"Funktionswert: \" + str(y0))\nprint(\"Gradient: \" + str(dy))\n\n\nFunktionswert: 7\nGradient: [7, 1]\n\n\nDie Werte der Ableitungen \\(\\bar v_i\\) lassen sich auch im Computational Graph verfolgen. Der Wert bei der Kante von \\(v_i\\) nach \\(v_j\\) entspricht der partiellen Ableitung \\(\\partial v_i / \\partial v_j\\). Entlang eines Weges werden die Werte multipliziert. Führen mehrere Wege zu einer Variablen \\(x_i\\), so werden die Werte der einzelnen Wege addiert.\n\n\n\n\n\n\n   \n\nnx0\n\nx 0   \n\nnPlus\n\n +   \n\nnx0->nPlus\n\n  1   \n\nnTimes\n\n *   \n\nnx0->nTimes\n\n  v 2   \n\nnx1\n\nx 1   \n\nnx1->nPlus\n\n  1   \n\nnMinus\n\n -   \n\nnx1->nMinus\n\n  -1   \n\nnPlus->nTimes\n\n  v 0   \n\nnTimes->nMinus\n\n  1   \n\nny\n\ny   \n\nnMinus->ny\n\n  1  \n\n\nAbbildung 5.2: Werte der vbar.\n\n\n\n\n\n\n\nÜbungsaufgabe 5.1 (Eigene Beispiele finden) \nSchreibe eigene Funktion \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) für \\(n\\in\\lbrace 2, 3 \\rbrace\\) hin und erstelle den Computational Graph und ein Programm. Leite das Programm nach der oben beschriebenen AAD Methode ab und überzeuge dich an verschiedenen Stellen davon, dass der Gradient korrekt ist.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nBeispiele können in der Literatur gefunden werden, z.B. bei Radcliffe (2021), Slater (2022), Baydin u. a. (2018) (S. 13), Griewank und Walther (2008) (S. 9, S. 42) oder Henrard (2017) (S. 24).\n\n\n\n\n\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, und Jeffrey Mark Siskind. 2018. „Automatic Differentiation in Machine Learning: a Survey“. Journal of Machine Learning Research 18 (153): 1–43. http://jmlr.org/papers/v18/17-468.html.\n\n\nGriewank, Andreas, und Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2. Aufl. Other Titles in Applied Mathematics 105. Philadelphia, PA: SIAM. http://bookstore.siam.org/ot105/.\n\n\nHenrard, Marc. 2017. Algorithmic Differentiation in Finance Explained. Financial Engineering Explained. Cham: Palgrave Macmillan. https://doi.org/10.1007/978-3-319-53979-9.\n\n\nRadcliffe, Sidney. 2021. „Reverse-mode automatic differentiation from scratch, in Python“. 22. Mai 2021. https://sidsite.com/posts/autodiff/.\n\n\nSlater, Max. 2022. „Differentiable programming from scratch“. Juli 2022. https://thenumb.at/Autodiff/."
  },
  {
    "objectID": "aad.html#manuelle-implementation-der-aad",
    "href": "aad.html#manuelle-implementation-der-aad",
    "title": "5  Adjungierte Algorithmische Differentiation",
    "section": "5.1 Manuelle Implementation der AAD",
    "text": "5.1 Manuelle Implementation der AAD\nWir erläutern die Methode zuerst an einem einfachen Beispiel, welches eine leicht abgeänderte Version des Beispiels von Radcliffe (2021) ist.\n\nBeispiel 5.1 (Gradient mit AAD) \nBetrachten wir die Funktion \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) \\[\ny = f(x_0, x_1) = (x_0 + x_1) \\cdot x_0 - x_1\n\\]\nAls Programm können wir die Funktion unter Berücksichtigung der Konvention so schreiben (siehe auch Abbildung 5.1):\n\n\nCode\ndef f(x0, x1):\n    v0 = x0\n    v1 = x1\n    v2 = v0 + v1\n    v3 = v2 * v0\n    v4 = v3 - v1\n    y = v4\n    return y\n\nx0, x1 = 2, 3\ny0 = f(x0, x1)\nprint(\"f(\" + str(x0) + \",\" + str(x1) + \") = \" + str(y0))\n\n\nf(2,3) = 7\n\n\n\n\n\n\n\n\n   \n\nnx0\n\nx 0   \n\nnPlus\n\n +   \n\nnx0-&gt;nPlus\n\n    \n\nnTimes\n\n *   \n\nnx0-&gt;nTimes\n\n    \n\nnx1\n\nx 1   \n\nnx1-&gt;nPlus\n\n    \n\nnMinus\n\n -   \n\nnx1-&gt;nMinus\n\n    \n\nnPlus-&gt;nTimes\n\n    \n\nnTimes-&gt;nMinus\n\n    \n\nny\n\ny   \n\nnMinus-&gt;ny\n\n   \n\n\nAbbildung 5.1: Computational Graph für y = (x0 + x1) * x0 - x1.\n\n\n\n\nDie partiellen Ableitungen von \\(f\\) lauten \\[\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_0} &= \\frac{\\partial f}{\\partial x_0} = (1+0)\\cdot x_0 + (x_0 + x_1)\\cdot 1 - 0=2x_0 + x_1 \\\\\n    \\frac{\\partial y}{\\partial x_1} &= \\frac{\\partial f}{\\partial x_1} = (0 + 1)\\cdot x_0 - 1 = x_0 - 1\n\\end{align*}\n\\] wobei für die Ableitung nach \\(x_0\\) die Produktregel verwendet wurde. Die partielle Ableitung \\(\\partial y / \\partial x_0\\) können wir auch berechnen, indem wir bei \\(y = v_4\\) anfangen und jeweils die Definition der Hilfsvariablen einsetzen: \\[\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_0} &= \\frac{\\partial v_4}{\\partial x_0} \\\\\n    &= \\frac{\\partial (v_3 - v_1)}{\\partial x_0} \\\\\n    &= \\frac{\\partial v_3}{\\partial x_0} - \\frac{\\partial v_1}{\\partial x_0} \\\\\n    &= \\frac{\\partial (v_2 \\cdot v_0)}{\\partial x_0} - \\frac{\\partial x_1}{\\partial x_0} \\\\\n    &= \\frac{\\partial v_2}{\\partial x_0} \\cdot v_0 + v_2 \\cdot \\frac{\\partial v_0}{\\partial x_0} - 0 \\\\\n    &= \\frac{\\partial (v_0 + v_1)}{\\partial x_0} \\cdot v_0 + v_2 \\cdot \\frac{\\partial x_0}{\\partial x_0} \\\\\n    &= \\left( \\frac{\\partial v_0}{\\partial x_0} + \\frac{\\partial v_1}{\\partial x_0} \\right) \\cdot x_0 + (v_0 + v_1) \\cdot 1 \\\\\n    &= \\left( \\frac{\\partial x_0}{\\partial x_0} + \\frac{\\partial x_1}{\\partial x_0} \\right) \\cdot x_0 + (x_0 + x_1) \\\\\n    &= (1 + 0) \\cdot x_0 + (x_0 + x_1) \\\\\n    &= 2x_0 + x_1   \n\\end{align*}\n\\]\nAnalog findet man \\(\\partial y / \\partial x_1\\) (diesmal lassen wir einige der offensichtlicheren Zwischenschritte weg): \\[\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_1} &= \\frac{\\partial v_4}{\\partial x_1} \\\\\n    &= \\frac{\\partial v_3}{\\partial x_1} - \\frac{\\partial v_1}{\\partial x_1} \\\\\n    &= \\frac{\\partial (v_2 \\cdot v_0)}{\\partial x_1} - 1 \\\\\n    &= \\frac{\\partial v_2}{\\partial x_1} \\cdot v_0 + v_2 \\cdot \\frac{\\partial v_0}{\\partial x_1} - 1 \\\\\n    &= \\frac{\\partial (v_0 + v_1)}{\\partial x_1} \\cdot x_0 + v_2 \\cdot 0 - 1 \\\\\n    &= \\left( \\frac{\\partial v_0}{\\partial x_1} + \\frac{\\partial v_1}{\\partial x_1} \\right) \\cdot x_0 - 1 \\\\\n    &= ( 0 + 1) \\cdot x_0 - 1 \\\\\n    &= x_0 - 1   \n\\end{align*}\n\\]\nUm die beiden Rechnungen zusammenzufassen, führen nun für jede Hilfsvariable \\(v_i\\) eine neue Variable \\(\\bar v_i\\) ein, welche definiert ist als \\[\n\\bar v_i = \\frac{\\partial y}{\\partial v_i}\n\\] Ähnlich wie die \\(\\dot v_i\\) aus der SAD speichern diese Variablen die Werte der Ableitungen. Die neue Notation soll anzeigen, dass es sich um die AAD Methode handelt. Unser Ziel ist es also, \\(\\bar v_0 = \\partial y / \\partial v_0 = \\partial y / \\partial x_0\\) und \\(\\bar v_1 = \\partial y / \\partial v_1 = \\partial y / \\partial x_1\\) zu bestimmen. Beginnen wir in der letzten Zeile des Programms, dann gilt offenbar immer \\(\\bar v_4 = \\partial y / \\partial v_4 = 1\\). In der Zeile darüber können wir \\(\\bar v_3\\) und \\(\\bar v_1\\) berechnen, indem wir die Kettenregel verwenden.\n\\[\\begin{align*}\n    \\bar v_3 &= \\frac{\\partial y}{\\partial v_3} = \\frac{\\partial y}{\\partial v_4} \\cdot \\frac{\\partial v_4}{\\partial v_3} = \\bar v_4 \\cdot (1-0)=\\bar v_4  \\\\\n\n    \\bar v_1 &= \\frac{\\partial y}{\\partial v_1} = \\frac{\\partial y}{\\partial v_4} \\cdot \\frac{\\partial v_4}{\\partial v_1} = \\bar v_4 \\cdot (0-1)= -\\bar v_4\n\\end{align*}\\]\nAlso sind \\(\\bar v_3 = 1\\) und \\(\\bar v_1 = -1\\). Der Zwischenwert in \\(\\bar v_1\\) wird später ergänzt werden. Aus der Zeile \\(v_3 = v_2 \\cdot v_0\\) lassen sich als nächstes Ausdrücke für \\(\\bar v_2\\) und \\(\\bar v_0\\) finden.\n\\[\\begin{align*}\n    \\bar v_2 &= \\frac{\\partial y}{\\partial v_2} = \\frac{\\partial y}{\\partial v_3} \\cdot \\frac{\\partial v_3}{\\partial v_2} = \\bar v_3 \\cdot v_0 \\\\\n\n    \\bar v_0 &= \\frac{\\partial y}{\\partial v_0} = \\frac{\\partial y}{\\partial v_3} \\cdot \\frac{\\partial v_3}{\\partial v_0} = \\bar v_3 \\cdot v_2\n\\end{align*}\\]\nMit den vorher berechneten Werten erhalten wir also \\(\\bar v_2 = v_0\\) und \\(\\bar v_0 = v_2\\). Beide Werte sind durch die Funktion bereits berechnet worden. Im obigen Beispiel gilt etwa v0 = x0 = 2 und v2 = x0 + x1 = 5. Auch diese Zwischenwerte werden im nächsten Schritt ergänzt. Aus der Zeile \\(v_2 = v_0 + v_1\\) ergibt sich nämlich\n\\[\\begin{align*}\n    \\bar v_0 &= \\bar v_0 + \\frac{\\partial y}{\\partial v_0} = \\bar v_0 + \\frac{\\partial y}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial v_0} \\\\\n    &= \\bar v_0 + \\bar v_2 \\cdot (1+0) = \\bar v_0 + \\bar v_2 \\\\ & \\\\\n\n    \\bar v_1 &= \\bar v_1 + \\frac{\\partial y}{\\partial v_1} = \\bar v_1 + \\frac{\\partial y}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial v_1} \\\\\n    &= \\bar v_1 + \\bar v_2 \\cdot (0+1) = \\bar v_1 + \\bar v_2\n\\end{align*}\\]\nMit den bereits bekannten Werten erhalten wir \\(\\bar v_0 = v_2 + v_0\\) (bzw. mit den konkreten Werten des Beispiels v0bar = 5 + 2) und \\(\\bar v_1 = -1 + v_0\\) (bzw. v1bar = -1 + 2). Nun enthalten die Variablen \\(\\bar v_0\\) und \\(\\bar v_1\\) die Werte der gewünschten Ableitungen, nämlich \\(\\bar v_0 = (v_0 + v_1) + v_0 = 2x_0 + x_1\\) und \\(\\bar v_1 = -1 + x_0\\). Wir können aber die letzten Schritte analog zu den vorherigen ausführen:\n\\[\\begin{align*}\n    \\bar x_1 &= \\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial x_1} = \\bar v_1 \\cdot 1  \\\\\n\n    \\bar x_0 &= \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial y}{\\partial v_0} \\cdot \\frac{\\partial v_0}{\\partial x_0} = \\bar v_0 \\cdot 1  \\\\\n\\end{align*}\\]\nDie Schwierigkeit besteht darin, dass wir nicht wie bei der SAD in jedem Schritt die Variable \\(v_i\\) und gleichzeitig die Variable \\(\\dot v_i\\) berechnen können. Um die \\(\\bar v_i\\) zu bestimmen muss man zuerst die Funktion komplett ausführen, und sich dabei den Aufbau des Computational Graph merken. Erst dann kann man rückwärts die Ableitungswerte berechnen, angefangen bei der letzten Hilfsvariablen \\(\\bar v_4 = 1\\). Das folgende Schema fasst die obigen Rechnungen zusammen.\n\\[\\begin{equation*}\n\\left \\downarrow\n    \\begin{aligned}[c]\n        v_0 &= x_0 \\\\\n        v_1 &= x_1 \\\\\n        v_2 &= v_0 + v_1 \\\\\n        v_3 &= v_2 \\cdot v_0 \\\\\n        v_4 &= v_3 - v_1 \\\\\n        y &= v_4\n    \\end{aligned}  \n\\right .\n\n\\qquad\n\n\\begin{aligned}[c]\n    & \\\\\n    & \\\\\n    & \\\\\n    & \\\\\n    & \\\\\n    &\\longrightarrow\n\\end{aligned}  \n\n\\qquad\n\n\\left \\uparrow\n    \\begin{aligned}[c]\n        \\bar x_0 &= \\bar v_0 = 2\\cdot x_0 + x_1 \\\\\n        \\bar x_1 &= \\bar v_1 = -1 + x_0 \\\\\n        \\bar v_0 &= \\bar v_0 + \\bar v_2, \\quad \\bar v_1 = \\bar v_1 + \\bar v_2 \\\\\n        \\bar v_2 &= \\bar v_3 \\cdot v_0, \\quad \\bar v_0 = \\bar v_3 \\cdot v_2 \\\\\n        \\bar v_3 &= \\bar v_4, \\quad \\bar v_1 = -\\bar v_4 \\\\\n        \\bar v_4 &= \\bar y = 1\n    \\end{aligned}  \n\\right .\n\\end{equation*}\\]\n\n\nCode\ndef f(x0, x1):\n    v0 = x0\n    v1 = x1\n    v2 = v0 + v1\n    v3 = v2 * v0\n    v4 = v3 - v1\n    y = v4\n    v4bar = 1\n    v3bar = v4bar\n    v1bar = -v4bar\n    v2bar = v3bar * v0\n    v0bar = v3bar * v2\n    v0bar = v0bar + v2bar\n    v1bar = v1bar + v2bar\n    grad = [v0bar, v1bar]\n    return [y, grad]\n\nx0, x1 = 2, 3\n[y0, dy] = f(x0, x1)\nprint(\"Funktionswert: \" + str(y0))\nprint(\"Gradient: \" + str(dy))\n\n\nFunktionswert: 7\nGradient: [7, 1]\n\n\nDie Werte der Ableitungen \\(\\bar v_i\\) lassen sich auch im Computational Graph verfolgen, siehe Abbildung 5.2. Der Wert bei der Kante von \\(v_i\\) nach \\(v_j\\) entspricht der partiellen Ableitung \\(\\partial v_i / \\partial v_j\\). Entlang eines Weges werden die Werte multipliziert. Führen mehrere Wege zu einer Variablen \\(x_i\\), so werden die Werte der einzelnen Wege addiert.\n\n\n\n\n\n\n   \n\nnx0\n\nx 0   \n\nnPlus\n\n +   \n\nnx0-&gt;nPlus\n\n  1   \n\nnTimes\n\n *   \n\nnx0-&gt;nTimes\n\n  v 2   \n\nnx1\n\nx 1   \n\nnx1-&gt;nPlus\n\n  1   \n\nnMinus\n\n -   \n\nnx1-&gt;nMinus\n\n  -1   \n\nnPlus-&gt;nTimes\n\n  v 0   \n\nnTimes-&gt;nMinus\n\n  1   \n\nny\n\ny   \n\nnMinus-&gt;ny\n\n  1  \n\n\nAbbildung 5.2: Werte der Hilfsvariablen vbar.\n\n\n\n\n\n\n\nÜbungsaufgabe 5.1 (Eigene Beispiele finden) \nSchreibe eine eigene Funktion \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) für \\(n\\in\\lbrace 2, 3 \\rbrace\\) hin und erstelle den Computational Graph und ein Programm. Leite das Programm nach der oben beschriebenen AAD Methode ab und überzeuge dich an verschiedenen Stellen davon, dass der Gradient korrekt ist.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nBeispiele findet man in der Literatur, z.B. bei Radcliffe (2021), Slater (2022), Baydin u. a. (2018) (S. 13), Griewank und Walther (2008) (S. 9, S. 42) oder Henrard (2017) (S. 24)."
  },
  {
    "objectID": "aad.html#implementation-der-aad-mit-operator-overlaoding",
    "href": "aad.html#implementation-der-aad-mit-operator-overlaoding",
    "title": "5  Adjungierte Algorithmische Differentiation",
    "section": "5.2 Implementation der AAD mit Operator Overlaoding",
    "text": "5.2 Implementation der AAD mit Operator Overlaoding\nNun wollen wir ähnlich wie im Kapitel 3.2 eine Klasse FloatAad entwerfen, welche die Berechnung aller Hilfsvariablen vbar automatisch ausführt. Wie auch zuvor hat jedes FloatAad-Objekt ein Attribut value vom Typ Int. Allerdings reicht es nicht mehr aus, ein Int-Attribut derivative zu definieren, um den Wert der Ableitung zu speichern weil auch die Struktur des Computational Graph gespeichert werden muss. Als Attribut derivatives wählen wir ein tuple, dessen erster Eintrag ein FloatAad-Objekt ist, nämlich die Variable, nach der die partielle Ableitung berechnet wird, und der zweite Eintrag ist der Wert dieser partiellen Ableitung. Da das erste Element des Tupels selber auch ein Attribut derivatives hat, entsteht so eine rekursive Darstellung des Computational Graph.\nDie folgende Implementation ist stark von Radcliffe (2021) inspiriert. Die Variablennamen wurden angepasst, so dass sie konsistent mit den Bezeichnungen aus Kapitel 3 sind. Ausserdem werden wir unsere Klasse noch mit einiger zusätzlicher Funktionalität ausstatten, etwa mit Typunterscheidungen, so dass wir Int-Zahlen zu FloatAad-Objekten addieren können.\n\n5.2.1 Die Klasse FloatAad\nWir beginnen unsere Klasse mit einer neuen Datei, welche wir floataad.py nennen. Als erstes definieren wir einen Konstruktor, der uns das Umwandeln von Int- oder Float-Objekten in FloatAad-Objekte erlaubt. Ausserdem definieren wir auch gleich eine Funktion, mit der wir eine Liste von solchen Int oder Float in eine Liste von FloatAad umwandeln können, siehe dazu Kapitel 4.2.\n\n\nCode\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\n\n\nif __name__ == '__main__':\n\n    x = FloatAad(2)\n    print(x.value)\n    print(x.derivatives)\n    print(\"\")\n\n    x = [1,2]\n    v = float2FloatAad(x)\n    print(type(v))\n    print(type(v[0]))\n    print(type(v[0].value))\n    print(type(v[0].derivatives))\n\n\n2\n()\n\n<class 'numpy.ndarray'>\n<class '__main__.FloatAad'>\n<class 'int'>\n<class 'tuple'>\n\n\n\n\n5.2.2 Vorzeichen\nWir gehen bei der Implementation der unären und binären Operatoren etwas anders vor als im Kapitel 3.2 . Wir definieren zunächst Funktionen für die Operationen und benutzen diese, um die Operatoren zu überladen. Für das negative Vorzeichen sieht das so aus:\n\n\nCode\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\n    def __pos__(self):\n        return self\n\n    def __neg__(self):\n        return neg(self)\n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\n\ndef neg(a):\n    newValue = -1 * a.value\n    newDerivative = (\n        (a, -1),\n    )\n    return FloatAad(newValue, newDerivative)\n\n\nif __name__ == '__main__':\n\n    x = FloatAad(2)\n    v = -x\n\n    print(v.value)\n    print(v.derivatives)\n\n\n-2\n((<__main__.FloatAad object at 0x0FA45E68>, -1),)\n\n\nDer Wert von v.derivatives ist ein Tupel, dessen erster Eintrag eine Referenz auf x ist und der Wert des zweiten Eintrags ist -1 weil \\(\\partial v / \\partial x = -1\\) ist.\n\n\n5.2.3 Die Operatoren + und -\nWenn wir zwei FloatAad-Objekte a und b addieren, dann müssen wir zwei Tupel als Ableitung zurückgeben, nämlich für \\[\n\\frac{\\partial}{\\partial a}(a+b)=1 \\qquad\\textrm{und für}\\qquad \\frac{\\partial}{\\partial b}(a+b)=1\n\\]\nDie entsprechende Funktion sieht so aus:\n\n\nCode\ndef add(a, b):\n    newValue = a.value + b.value\n    newDerivative = (\n        (a, 1),  # a+b nach a abgeleitet gibt 1\n        (b, 1)   # a+b nach b abgeleitet gibt 1\n    )\n    return FloatAad(newValue, newDerivative)\n\n\nFür das Überladen des +-Operators geben wir dann einfach return add(self, other) zurück. Wir wollen bei dieser Gelegenheit aber gleich noch die Typabfrage implementieren, so dass wir nicht nur zwei FloatAad-Objekte addieren können, sondern auch x + 1 schreiben können. In diesem Fall wandeln wir die Zahl einfach in ein FloatAad-Objekt um und wenden die Funktion add an. Der __radd__-Operator, mit dem wir einen Ausdruck wie 1 + x schreiben können, wird analog definiert.\n\n\nCode\ndef __add__(self, other):\n    if type(other) in [int, float]:\n        return add(self, FloatAad(other))\n    else:\n        return add(self, other)\n\ndef __radd__(self, other):\n    if type(other) in [int, float]:\n        return add(FloatAad(other), self)\n    else:\n        return add(other, self)\n\n\n\nÜbungsaufgabe 5.2 (Den Operator - implementieren) \nImplementiere die Funktionen __sub__ und __rsub__. Du kannst dafür die Funktion neg(a) verwenden.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef __sub__(self, other):\n    if type(other) in [int, float]:\n        return add(self, FloatAad(-other))\n    else:\n        return add(self, neg(other))\n        \ndef __rsub__(self, other):\n    if type(other) in [int, float]:\n        return add(FloatAad(other), neg(self))\n    else:\n        return add(neg(other), self)\n\n\n\n\n\n\n\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, und Jeffrey Mark Siskind. 2018. „Automatic Differentiation in Machine Learning: a Survey“. Journal of Machine Learning Research 18 (153): 1–43. http://jmlr.org/papers/v18/17-468.html.\n\n\nGriewank, Andreas, und Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2. Aufl. Other Titles in Applied Mathematics 105. Philadelphia, PA: SIAM. http://bookstore.siam.org/ot105/.\n\n\nHenrard, Marc. 2017. Algorithmic Differentiation in Finance Explained. Financial Engineering Explained. Cham: Palgrave Macmillan. https://doi.org/10.1007/978-3-319-53979-9.\n\n\nRadcliffe, Sidney. 2021. „Reverse-mode automatic differentiation from scratch, in Python“. 22. Mai 2021. https://sidsite.com/posts/autodiff/.\n\n\nSlater, Max. 2022. „Differentiable programming from scratch“. Juli 2022. https://thenumb.at/Autodiff/."
  },
  {
    "objectID": "HigherDimFunctions.html#sec-FunktionenMehrereInputs",
    "href": "HigherDimFunctions.html#sec-FunktionenMehrereInputs",
    "title": "4  Funktionen mit mehreren In- und Outputs",
    "section": "4.2 Funktionen mit mehreren Eingabewerten",
    "text": "4.2 Funktionen mit mehreren Eingabewerten\nDie Ableitung einer Funktion \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) ist der Gradient\n\\[\n\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)\n\\]\nFür weitere Details zum Gradienten sei auf Arens u. a. (2022), S. 870 verwiesen.\n\nBeispiel 4.1 (Eine Funktion mit drei Eingabwerten) \nBetrachten wir als Beispiel die Funktion \\(f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}\\)\n\\[\nf(x_0, x_1, x_2) = x_0^2 + 2\\cdot x_0 \\cdot x_1 - \\frac{x_1}{x_2 ^3}\n\\]\nDas folgende Programm berechnet den Funktionswert \\(f(1, 2, 3)=\\frac{133}{27}\\approx 4.9259...\\)\n\n\nCode\ndef f(x):\n    y = x[0]**2 + 2*x[0]*x[1] - x[1]/x[2]**3\n    return y\n\nx0 = [1, 2, 3]\ny0 = f(x0)\nprint(y0)\n\n\n4.925925925925926\n\n\nDer Gradient dieser Funktion ist \\[\n\\nabla f = \\left( 2x_0+2x_1, 2x_0 - \\frac{1}{x_2^3}, 3\\frac{x_1}{x_2^4} \\right)\n\\]\nbzw. ausgewertet an der Stelle \\((x_0, x_1, x_2) = (1, 2, 3)\\) \\[\n\\begin{align*}\n\\nabla f \\vert _{(1, 2, 3)} &= \\left( 6, \\frac{53}{27}, \\frac2{27} \\right) \\\\\n&\\approx \\left( 6, 1.9629..., 0.0740... \\right)\n\\end{align*}\n\\]\n\n\nMit der Standard Algorithmischen Differentiation kann der Gradient nicht in einem Durchgang berechnet werden. Bei der Umwandlung der Anfangswerte in FloatSad-Objekte müssen wir allen Variablen in \\(x = (x_1, \\ldots, x_n)\\) einen Anfangswert \\(\\dot{x} = (\\dot x_1, \\ldots, \\dot x_n)\\) geben. Wenn wir für die Initialisierung \\(\\dot{x} = e_i = (0, \\ldots, 1, \\ldots, 0)\\) verwenden (mit \\(1\\) an der \\(i\\)-ten Stelle und sonst lauter \\(0\\)), dann bekommen wir den Wert der \\(i\\)-ten partiellen Ableitung \\(\\frac{\\partial f}{\\partial x_i}\\).\nUm die Funktion im obigen Beispiel mit unserer Klasse FloatSad abzuleiten, verwenden wir wieder numpy. Als erstes definieren wir eine vektorisierte Funktion float2FloatSad, mit der wir aus der Liste x eine Liste von FloatSad-Objekten erzeugen. Die Werte der Ableitungen werden zu Beginn explizit in der Variablen xdot initialisiert.\n\nAbleitung nach x0Ableitung nach x1Ableitung nach x2\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport numpy as np\n\ndef f(x):\n    xdot = [1, 0, 0]\n    x = float2FloatSad(x, xdot)\n    y = x[0]**2 + 2*x[0]*x[1] - x[1]/x[2]**3\n    return y\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\n\nx0 = [1, 2, 3]\ny0 = f(x0)\nprint(y0)\n\n\n&lt; 4.925925925925926 ; 6.0 &gt;\n\n\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport numpy as np\n\ndef f(x):\n    xdot = [0, 1, 0]\n    x = float2FloatSad(x, xdot)\n    y = x[0]**2 + 2*x[0]*x[1] - x[1]/x[2]**3\n    return y\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\n\nx0 = [1, 2, 3]\ny0 = f(x0)\nprint(y0)\n\n\n&lt; 4.925925925925926 ; 1.962962962962963 &gt;\n\n\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport numpy as np\n\ndef f(x):\n    xdot = [0, 0, 1]\n    x = float2FloatSad(x, xdot)\n    y = x[0]**2 + 2*x[0]*x[1] - x[1]/x[2]**3\n    return y\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\n\nx0 = [1, 2, 3]\ny0 = f(x0)\nprint(y0)\n\n\n&lt; 4.925925925925926 ; 0.07407407407407407 &gt;\n\n\n\n\n\nInitialisiert man die Ableitungen beispielsweise mit xdot = [1, 1, 1], dann erhält man die Summe der drei Richtungsableitungen:\n\n\n&lt; 4.925925925925926 ; 8.037037037037036 &gt;\n\n\nAllgemein gilt: Initialisiert man xdot mit dem Vektor \\(\\vec r = (r_1, \\ldots, r_n)^\\intercal\\), dann erhält man das Skalarprodukt \\[\n\\nabla f \\cdot \\vec r =\n\\left ( \\left .\\frac{\\partial f}{\\partial x_1} \\right \\vert_{(x_1, \\ldots, x_n)}, \\ldots, \\left .\\frac{\\partial f}{\\partial x_n} \\right \\vert_{(x_1, \\ldots, x_n)}  \\right ) \\cdot \\begin{pmatrix} r_1 \\\\ \\vdots \\\\ r_n \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "aad.html#implementation-der-aad-mit-operator-overloading",
    "href": "aad.html#implementation-der-aad-mit-operator-overloading",
    "title": "5  Adjungierte Algorithmische Differentiation",
    "section": "5.2 Implementation der AAD mit Operator Overloading",
    "text": "5.2 Implementation der AAD mit Operator Overloading\nNun wollen wir ähnlich wie im Kapitel 3.2 eine Klasse FloatAad entwerfen, welche die Berechnung aller Hilfsvariablen vbar automatisch ausführt. Wie auch zuvor hat jedes FloatAad-Objekt ein Attribut value vom Typ Int oder Float. Allerdings reicht es nicht mehr aus, ein Float-Attribut derivative zu definieren, um den Wert der Ableitung zu speichern weil auch die Struktur des Computational Graph gespeichert werden muss. Als Attribut derivatives wählen wir ein tuple, dessen erster Eintrag ein FloatAad-Objekt ist, nämlich die Variable, nach der die partielle Ableitung berechnet wird. Der zweite Eintrag ist der Wert dieser partiellen Ableitung. Da das erste Element des Tupels selber auch ein Attribut derivatives hat, entsteht so eine rekursive Darstellung des Computational Graph.\nDie folgende Implementation lehnt sich stark an Radcliffe (2021) an. Die Variablennamen wurden angepasst, so dass sie konsistent mit den Bezeichnungen aus Kapitel 3 sind. Ausserdem werden wir unsere Klasse noch mit einiger zusätzlicher Funktionalität ausstatten, etwa mit Typunterscheidungen, so dass wir z.B. auch wieder Int-Zahlen zu FloatAad-Objekten addieren können.\n\n5.2.1 Die Klasse FloatAad\nWir beginnen unsere Klasse mit einer neuen Datei, welche wir floataad.py nennen. Als erstes definieren wir einen Konstruktor, der uns das Umwandeln von Int- oder Float-Objekten in FloatAad-Objekte erlaubt. Ausserdem definieren wir auch gleich eine Funktion float2FloatAad, mit der wir eine Liste von solchen Int oder Float in eine Liste von FloatAad umwandeln können und eine Funktion getValues, welche aus einer Liste von FloatAad-Objekten die Funktionswerte ausliest, siehe dazu Kapitel 4.2.\n\n\nCode\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\ngetValues = np.vectorize(lambda x : x.value)\n\nif __name__ == '__main__':\n\n    x = FloatAad(2)\n    print(x.value)\n    print(x.derivatives)\n    print(\"\")\n\n    x = [1,2]\n    v = float2FloatAad(x)\n    print(type(v))\n    print(type(v[0]))\n    print(type(v[0].value))\n    print(type(v[0].derivatives))\n\n\n2\n()\n\n&lt;class 'numpy.ndarray'&gt;\n&lt;class '__main__.FloatAad'&gt;\n&lt;class 'int'&gt;\n&lt;class 'tuple'&gt;\n\n\n\n\n5.2.2 Vorzeichen\nWir gehen bei der Implementation der unären und binären Operatoren etwas anders vor als im Kapitel 3.2 . Wir definieren zunächst Funktionen für die Operationen und benutzen diese, um die Operatoren zu überladen. Für das negative Vorzeichen sieht das so aus:\n\n\nCode\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\n    def __pos__(self):\n        return self\n\n    def __neg__(self):\n        return neg(self)\n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\ngetValues = np.vectorize(lambda x : x.value)\n\ndef neg(a):\n    newValue = -1 * a.value\n    newDerivative = (\n        (a, -1),\n    )\n    return FloatAad(newValue, newDerivative)\n\n\nif __name__ == '__main__':\n\n    x = FloatAad(2)\n    v = -x\n\n    print(v.value)\n    print(v.derivatives)\n\n\n-2\n((&lt;__main__.FloatAad object at 0x00000162B3333940&gt;, -1),)\n\n\nDer Wert von v.derivatives ist ein Tupel, dessen erster Eintrag eine Referenz auf das FloatAad-Objekt x ist und der Wert des zweiten Eintrags ist -1 weil \\(\\partial v / \\partial x = -1\\) ist.\n\n\n5.2.3 Die Operatoren + und -\nWenn wir zwei FloatAad-Objekte a und b addieren, dann müssen wir zwei Tupel als Ableitung zurückgeben, nämlich für \\[\n\\frac{\\partial}{\\partial a}(a+b)=1 \\qquad\\textrm{und für}\\qquad \\frac{\\partial}{\\partial b}(a+b)=1\n\\]\nDie entsprechende Funktion sieht so aus:\n\n\nCode\ndef add(a, b):\n    newValue = a.value + b.value\n    newDerivative = (\n        (a, 1),  # a+b nach a abgeleitet gibt 1\n        (b, 1)   # a+b nach b abgeleitet gibt 1\n    )\n    return FloatAad(newValue, newDerivative)\n\n\nFür das Überladen des +-Operators geben wir dann einfach return add(self, other) zurück. Wir wollen bei dieser Gelegenheit aber gleich noch die Typabfrage implementieren, so dass wir nicht nur zwei FloatAad-Objekte addieren können, sondern auch Ausdrücke wie x + 1 schreiben können. In diesem Fall brauchen wir für die newDerivative nur ein Tupel, welches wir direkt in der Funktion __add__ bestimmen. Der __radd__-Operator, mit dem wir einen Ausdruck wie 1 + x schreiben können, wird analog definiert.\n\n\nCode\ndef __add__(self, other):\n    if type(other) in [int, float]:\n        newValue = self.value + other\n        newDerivative = (\n            (self, 1),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return add(self, other)\n    \ndef __radd__(self, other):\n    if type(other) in [int, float]:\n        newValue = other + self.value\n        newDerivative = (\n            (self, 1),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return add(other, self)\n\n\n\nÜbungsaufgabe 5.2 (Den Operator - implementieren) \nImplementiere die Funktionen __sub__ und __rsub__. Du kannst dafür die Funktionen neg(a) und add(a,b) verwenden.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef __sub__(self, other):\n    if type(other) in [int, float]:\n        newValue = self.value - other\n        newDerivative = (\n            (self, 1),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return add(self, neg(other))\n        \ndef __rsub__(self, other):\n    if type(other) in [int, float]:\n        newValue = other - self.value\n        newDerivative = (\n            (self, -1),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return add(other, neg(self)) \n\n\n\n\n\n\n\n5.2.4 Gradienten berechnen\nHier ist die bisher implementierte Klasse zusammen mit einem kleinen Testprogramm, welches die Funktion \\(f(x_0, x_1) = 2x_0 - x_1 + 5\\) berechnet.\n\n\nCode\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\n    def __pos__(self):\n        return self\n\n    def __neg__(self):\n        return neg(self)\n\n    def __add__(self, other):\n        if type(other) in [int, float]:\n            newValue = self.value + other\n            newDerivative = (\n                (self, 1),\n            )\n            return FloatAad(newValue, newDerivative)\n        else:\n            return add(self, other)\n        \n    def __radd__(self, other):\n        if type(other) in [int, float]:\n            newValue = other + self.value\n            newDerivative = (\n                (self, 1),\n            )\n            return FloatAad(newValue, newDerivative)\n        else:\n            return add(other, self)\n    \n    def __sub__(self, other):\n        if type(other) in [int, float]:\n            newValue = self.value - other\n            newDerivative = (\n                (self, 1),\n            )\n            return FloatAad(newValue, newDerivative)\n        else:\n            return add(self, neg(other))\n        \n    def __rsub__(self, other):\n        if type(other) in [int, float]:\n            newValue = other - self.value\n            newDerivative = (\n                (self, -1),\n            )\n            return FloatAad(newValue, newDerivative)\n        else:\n            return add(other, neg(self)) \n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\ngetValues = np.vectorize(lambda x : x.value)\n\ndef neg(a):\n    newValue = -1 * a.value\n    newDerivative = (\n        (a, -1),\n    )\n    return FloatAad(newValue, newDerivative)\n\ndef add(a, b):\n    newValue = a.value + b.value\n    newDerivative = (\n        (a, 1),  # a+b nach a abgeleitet gibt 1\n        (b, 1)   # a+b nach b abgeleitet gibt 1\n    )\n    return FloatAad(newValue, newDerivative)\n\nif __name__ == '__main__':\n\n    x0 = FloatAad(2)\n    x1 = FloatAad(3)\n    y = x0 + x0 - x1 + 5\n\n    print(y.value)\n    print(y.derivatives)\n    print(y.derivatives[0][0].derivatives)\n\n\n6\n((&lt;__main__.FloatAad object at 0x00000162B3352F50&gt;, 1),)\n((&lt;__main__.FloatAad object at 0x00000162B3353F10&gt;, 1), (&lt;__main__.FloatAad object at 0x00000162B3352FB0&gt;, 1))\n\n\nWir sehen, dass der Funktionswert \\(f(2,3) = 6\\) korrekt ist. Als Ableitung sehen wir jedoch nur ein Tupel bestehend aus einer Referenz auf ein FloatAad-Objekt und einem Zwischenschritt bei der Berechnung der Ableitung. Auch die Ableitung des referenzierten FloatAad-Objekts enthält nur ein weiteres solches Tupel. Mit anderen Worten, wir sehen noch nirgends den Wert der partiellen Ableitungen \\(\\partial f / \\partial x_0 = 2\\) und \\(\\partial f / \\partial x_1 = -1\\). Wir schreiben dafür nun eine Funktion getDerivatives(y), welche aus dem FloatAad-Objekt y rekursiv die partiellen Ableitungen berechnet. Dies geschieht nach der Regel, dass die Zwischenwerte der Ableitungen entlang eines Weges im Computational Graph multipliziert werden und Werte von verschiedenen Wegen, die zur gleichen Variablen \\(x_i\\) führen, addiert werden. Für diese rekursive Berechnung definieren wir eine lokale Funktion computeDerivative. Der Rückgabewert soll dann ein Dictionary sein (defaultdict aus dem Modul collections, welches zu Beginn importiert werden muss), dessen Schlüsselwerte die Variablen x0, x1 etc. sind und die zugehörigen Werte sind die partiellen Ableitungen \\(\\partial f / \\partial x_i\\).\n\n\nCode\ndef getDerivatives(y):\n    dy = defaultdict(lambda: 0)\n\n    def computeDerivatives(y, pathValue):\n        for node, localDerivative in y.derivatives:\n            # Multipliziere entlang eines Weges im Graph\n            valueOfPathToNode = pathValue * localDerivative\n            # Addiere entlang unterschiedlicher Wege\n            dy[node] = dy[node] + valueOfPathToNode\n            # Rekursion zum Durchlaufen des ganzen Graphen\n            computeDerivatives(node, valueOfPathToNode)\n\n    # Initialisierung mit 1 (Ableitung von y nach y)\n    computeDerivatives(y, pathValue = 1)\n    return dy\n\n\nDie partiellen Ableitungen können nun mit dy = getDerivatives(y) berechnet und mit dy[x0], bzw. dy[x1] ausgegeben werden.\nWenn der Input der Funktion eine Liste x0 ist, dann möchten wir noch eine Funktion getGradient(x0, y) haben, welche den Gradienten von y in Form einer Liste zurückgibt.\n\n\nCode\ndef getGradient(x0, y):\n    dy = getDerivatives(y)\n    grad = []\n    for i in range(len(x0)):\n        grad.append(dy[x0[i]])\n    return grad\n\n\nMit diesen Befehlen können wir nun unsere Programme testen.\n\n\nCode\nif __name__ == '__main__':\n\n    x = [2, 3]\n    x = float2FloatAad(x)\n    \n    y = x[0] + x[0] - x[1] + 5\n    dy = getGradient(x, y)\n\n    print(y.value)\n    print(dy)\n\n\n\n\n6\n[2, -1]\n\n\n\n\n5.2.5 Die Operatoren * und /\nWenn wir zwei FloatAad-Objekte a und b multiplizieren, dann lauten die partiellen Ableitungen \\[\n\\frac{\\partial}{\\partial a}(a \\cdot b)=b \\qquad\\textrm{und}\\qquad \\frac{\\partial}{\\partial b}(a \\cdot b)=a\n\\]\nWir erzeugen also wieder zwei Tupel für die Ableitung.\n\n\nCode\ndef mul(a, b):\n    newValue = a.value * b.value\n    newDerivative = (\n        (a, b.value),  # a*b nach a abgeleitet gibt b\n        (b, a.value)   # a*b nach b abgeleitet gibt a\n    )\n    return FloatAad(newValue, newDerivative)\n\n\nDamit überladen wir nun den *-Operator:\n\n\nCode\ndef __mul__(self, other):\n    if type(other) in [int, float]:\n        newValue = self.value * other\n        newDerivative = (\n            (self, other), \n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return mul(self, other)\n        \ndef __rmul__(self, other):\n    if type(other) in [int, float]:\n        newValue = other * self.value\n        newDerivative = (\n            (self, other), \n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return mul(other, self)\n\n\n\nÜbungsaufgabe 5.3 (Den Operator / implementieren) \nÄhnlich wie in Übungsaufgabe 5.2 können wir die Division mit Hilfe der Funktion mul(a,b) realisieren. Schreibe dafür eine Funktion inv(a), welche in \\(\\frac{1}{a}\\) als FloatAad-Objekt berechnet. Implementiere damit die Funktionen __truediv__ und __rtruediv__.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nDie Funktion inv(a):\n\n\nCode\ndef inv(a):\n    newValue = 1. / a.value\n    newDerivative = (\n        (a, -1. / a.value**2), \n    )\n    return FloatAad(newValue, newDerivative)\n\n\nUnd damit der /-Operator\n\n\nCode\ndef __truediv__(self, other):\n    if type(other) in [int, float]:\n        newValue = self.value / other\n        newDerivative = (\n            (self, 1 / other),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return mul(self, inv(other))\n        \ndef __rtruediv__(self, other):\n    if type(other) in [int, float]:\n        newValue = other / self.value\n        newDerivative = (\n            (self, - other / math.pow(self.value,2)),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return mul(other, inv(self))\n\n\n\n\n\n\n\n5.2.6 Der Operator **\nEs fehlt nun nur noch der Potenzoperator. Die partiellen Ableitungen von \\(a^b\\) lauten \\[\n\\frac{\\partial}{\\partial a}(a^b)=b\\cdot a^{b-1} \\qquad\\textrm{und}\\qquad \\frac{\\partial}{\\partial b}(a^b)=a^b \\cdot \\ln(a)\n\\]\nWieder definieren wir uns zuerst eine Funktion\n\n\nCode\ndef pow(a, b):\n    newValue = math.pow(a.value,b.value)\n    newDerivative = (\n        (a, b.value * math.pow(a.value, b.value-1)), # a^b nach a abgeleitet gibt b*a^(b-1)\n        (b, math.pow(a.value, b.value) * math.log(a.value))  # a^b nach b abgeleitet gibt a^b * ln(a)\n    )\n    return FloatAad(newValue, newDerivative)\n\n\nund benutzen diese zur Überladung des **-Operators:\n\n\nCode\ndef __pow__(self, other):\n    if type(other) in [int, float]:\n        newValue = math.pow(self.value, other)\n        newDerivative = (\n            (self, other * math.pow(self.value, other - 1)),\n        ) \n        return FloatAad(newValue, newDerivative)           \n    else:\n        return pow(self, other)\n    \ndef __rpow__(self, other):\n    if type(other) in [int, float]:\n        newValue = math.pow(other, self.value)\n        newDerivative = (\n            (self, math.pow(other, self.value) * math.log(other)),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return pow(other, self)\n\n\nDie fertige Klasse FloatAad von hier kopiert werden."
  },
  {
    "objectID": "aad.html#die-klasse-floataad-im-einsatz",
    "href": "aad.html#die-klasse-floataad-im-einsatz",
    "title": "5  Adjungierte Algorithmische Differentiation",
    "section": "5.3 Die Klasse FloatAad im Einsatz",
    "text": "5.3 Die Klasse FloatAad im Einsatz\n\nWir sind nun in der Lage, Gradienten von Funktionen zu berechnen, solange darin noch keine Ausdrücke wie \\(\\sin(x)\\) oder \\(\\ln(x)\\) etc. vorkommen.\n\nBeispiel 5.2 (Gradient mit FloatAad) \nBetrachte die Funktion \\[\nf(x_0, x_1, x_2) = x_0 \\cdot x_1^2 + \\frac{2 ^{x_1}}{x_2} - \\frac{2}{x_2^2}\n\\] Der Gradient lautet \\[\n\\nabla f = \\begin{pmatrix}\n    x_1^2  , \\;\n    2x_0x_1 + \\frac{2^{x_1}\\cdot\\ln(2)}{x_2} , \\;\n    -\\frac{2^{x_1}}{x_2^2}+\\frac{4}{x_2^3}\n    \\end{pmatrix}\n\\]\nWerten wir die Funktion an der Stelle \\((x_0, x_1, x_2) = (3, 2, -1)\\) aus, dann erhalten wir \\(f(3, 2, -1) = 6\\) und\n\\[\\begin{align*}\n\\nabla f|_{(3, 2 -1)} &= \\begin{pmatrix}\n    4  , \\;\n    12-4\\cdot \\ln(2) ,\\;\n     -8\n    \\end{pmatrix} \\\\\n    &\\approx \\begin{pmatrix}\n    4  , \\;\n    9.23 , \\;\n     -8\n    \\end{pmatrix}\n\\end{align*}\\]\nTesten wir dies mit unserer Klasse:\n\n\nCode\nfrom floataad import float2FloatAad, getGradient\n\ndef f(x):\n    v1 = x[0] * x[1]**2\n    v2 = 2**x[1] / x[2]\n    v3 = 2 / x[2]**2\n    y = v1 + v2 - v3\n    return y\n\nx0 = [3,2,-1]\nx0 = float2FloatAad(x0)\n\ny = f(x0)\ny0 = y.value\ndy = getGradient(x0, y)\n\nprint(\"Funktionswert: \" + str(y0))\nprint(\"Gradient: \" + str(dy))\n\n\nFunktionswert: 6.0\nGradient: [4.0, 9.227411277760218, -8.0]\n\n\n\n\nWir können mit der Klasse FloatAad auch die Jacobi Matrix einer Funktion \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) bestimmen. Ähnlich wie in Kapitel 4.3 benötigen wir dazu jedoch mehrere Durchgänge. Diesmal muss jede Zeile der Matrix \\(Jf\\) einzeln berechnet werden. Dafür müssen wir die Methode getGradient \\(m\\) Mal aufrufen, nämlich einmal für jedes y[i]. Um dies zu automatisieren definieren wir uns wieder zwei Funktionen getValues und getJacobian (welche wir natürlich auch in die Klasse FloatAad schreiben könnten). Während FloatSad günstig war, solange \\(n&lt;m\\) war, so ist FloatAad günstig, wenn \\(n&gt;m\\) ist.\n\nBeispiel 5.3 (Jacobi Matrix mit FloatAad) \nWir betrachten die Funktion \\(f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^2\\) mit \\[\nf(x_0, x_1, x_2) =\n    \\begin{pmatrix}\n        y_0 \\\\ y_1\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n        x_0 + x_1^2 + \\frac{1}{x_2} \\\\\n        x_0 \\cdot x_1 \\cdot x_2\n    \\end{pmatrix}\n\\]\nDie Jacobi Matrix lautet \\[\\begin{align*}\nJf &=\n    \\begin{pmatrix}\n        \\frac{\\partial y_0}{\\partial x_0} & \\frac{\\partial y_0}{\\partial x_1} & \\frac{\\partial y_0}{\\partial x_2} \\\\\n        \\frac{\\partial y_1}{\\partial x_0} & \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2}\n    \\end{pmatrix} \\\\\n    &=\n    \\begin{pmatrix}\n        1 & 2x_1 & -\\frac{1}{x_2 ^2} \\\\\n        x_1 x_2 & x_0 x_2 & x_0 x_1\n    \\end{pmatrix}\n\\end{align*}\\]\nAusgewertet an der Stelle \\((x_0, x_1, x_2) = (-2, -4, 0.5)\\) erhalten wir \\(f(-2, -4, 0.5) = (16, 4)^\\intercal\\) und \\[\nJf|_{(-2,-4,0.5)} =\n\\begin{pmatrix}\n    1 & -8 & -4 \\\\\n    -2 & -1 & 8\n\\end{pmatrix}\n\\]\nAls Programm mit den oben beschriebenen Funktionen erhalten wir\n\n\nCode\nfrom floataad import float2FloatAad, getGradient\nimport numpy as np\n\ndef f(x):\n    y0 = x[0] + x[1]**2 + 1/x[2]\n    y1 = x[0] * x[1] * x[2]\n    return [y0, y1]\n\nx0 = [-2,-4,0.5]\nx0 = float2FloatAad(x0)\n\ngetValues = np.vectorize(lambda y : y.value)\ngetJacobian = lambda x,y : np.array([getGradient(x, y[i]) for i in range(len(y))])\n\ny = f(x0)\n\nval = getValues(y)\nJacobian = getJacobian(x0, y)\n\nprint(\"Funktionswert: \" +str(val))\nprint(\"Jf =\")\nprint(Jacobian)\n\n\nFunktionswert: [16.  4.]\nJf =\n[[ 1. -8. -4.]\n [-2. -1.  8.]]\n\n\n\n\n\n5.3.1 Gradient Descent zum Auffinden lokaler Minima\nEine Funktion \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) kann man sich als eine i.A. gekrümmte Fläche im \\(\\mathbb{R}^3\\) vorstellen. Der Gradient \\(\\nabla f|_{(x_0, x_1)}\\) ist ein Vektor in \\(\\mathbb{R}^2\\), der gerade in die Richtung des steilsten Anstiegs der Fläche beim Punkt \\((x_0, x_1)\\) zeigt. Entsprechend zeigt \\(-\\nabla f\\) in die Richtung des steilsten Abstiegs, siehe Arens u. a. (2022), S. 871. Damit lässt sich die Gradient Descent Methode zur Bestimmung eines lokalen Minimums, die wir im Kapitel 1.4.2 kennen gelernt haben, auch zum Auffinden eines lokalen Minimums einer Fläche verwenden. Sie lässt sich sogar auf Funktionen \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) anwenden, siehe Arens u. a. (2022), S. 1324.\nIm Fall einer 2-dimensionalen Fläche starten wir an einem Punkt \\((x_{0,0}, x_{1,0})\\), welchen wir durch seinen Ortsvektor \\(\\vec{x}_0\\) beschreiben. Dann berechnen wir rekursiv eine Folge von (Ortsvektoren zu) Punkten \\(\\vec{x}_n\\), welche im Idealfall zu einem lokalen Minimum der Funktion konvergieren, gemäss der Vorschrift \\[\n\\vec{x}_{n+1} = \\vec{x}_n - \\lambda\\cdot(\\nabla f(\\vec{x}_n))^\\intercal\n\\]\n\\(\\lambda\\in\\mathbb{R}^+\\) beeinflusst wie im Kapitel 1.4.2 die Schrittweite. Ist \\(\\lambda\\) zu klein, dann konvergiert die Iteration nur sehr langsam, wird \\(\\lambda\\) hingegen zu gross gewählt, dann kann es passieren, dass sich die Iteration von einem lokalen Minimum wieder weg bewegt.\n\nBeispiel 5.4 (Gradient Descent auf einer Fläche) \nBetrachte die Funktion \\(f(x_0, x_1) = x_0^4 + x_1^4 + x_0 x_1^3 - x_0^2 x_1 - x_1^2\\).\nHier wird diese Funktion einmal als 2-dimensionale Fläche und einmal als mit Hilfe von Konturlinien dargestellt. Im linken Feld kann man den Startpunkt und \\(\\lambda\\) wählen und die ersten 30 Iterationen darstellen.\n\n\n\n\nMan erkennt, dass die Funktion drei lokale Minima hat.\nNun wollen wir ein lokales Minimum mit Hilfe der Gradient Descent Methode finden. Damit wir \\(\\lambda \\cdot (\\nabla f)\\) als lam * df berechnen können, wandeln wir die Liste, die wir mit getGradient erhalten, in ein numpy-array um. Die oben beschriebene Iteration wird so lange ausgeführt, bis \\(|\\vec{x}_{n+1}-\\vec{x}_n|\\le 10^{-6}\\) ist.\n\n\nCode\nfrom floataad import float2FloatAad, getGradient\nimport numpy as np\n\ndef f(x):\n    y = x[0]**4 + x[1]**4 + x[0] * x[1]**3\n    y = y - x[0]**2 * x[1] - x[1]**2\n    return y\n\ngetValues = np.vectorize(lambda y : y.value)\n\n# Startwert und Lambda für Gradient Descent\nx0 = [0.5, 0]\nlam = 0.1\ntol = 1e-6 # Toleranz für Abbruchbedingung\n\nx0 = float2FloatAad(x0)\ny0 = f(x0)\ndy = np.array(getGradient(x0, y0))\n\n# Erster Schritt\nx1 = x0 - lam * dy\n\n# Iteration bis die Distanz zwischen zwei\n# aufeinanderfolgenden Punkten kleiner ist als tol.\nwhile np.linalg.norm(getValues(x0) - getValues(x1)) &gt; tol:\n    x0 = x1\n    y0 = f(x0)\n    dy = np.array(getGradient(x0, y0))\n    x1 = x0 - lam * dy\n\nprint(\"Lokales Minimum gefunden in der Nähe von\")\nprint(getValues(x1))\n\n\nLokales Minimum gefunden in der Nähe von\n[0.40427048 0.6160611 ]\n\n\n\n\nDie folgenden drei Abschnitte beschreiben verschiedene Anwendungen des Gradient Descent Verfahrens, die wir mit Hilfe von FloatAad programmieren können.\n\n\n5.3.2 Lineare Regression\nBeim klassischen Ausgleichsproblem sind \\(n\\) Datenpunkte \\((x_i , y_i)\\) (Messwerte) gegeben, zwischen denen ein linearer Zusammenhang vermutet wird, d.h. \\(y_i \\approx g(x_i) = a\\cdot x_i + b\\). Auf Grund von Messfehlern und anderen Einflüssen können wir jedoch nicht erwarten, dass der Zusammenhang exakt einer linearen Funktion entspricht, d.h. der Fehler \\(y_i - g(x_i)\\) ist im Allgemeinen nicht Null. Das Ziel ist nun, die Parameter \\(a\\) und \\(b\\) so zu bestimmen, dass die Summe der Quadrate dieser Fehler möglichst klein wird. Mit anderen Worten: wir suchen das Minimum der Funktion \\(\\Phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) \\[\n\\Phi(a, b) = \\sum_{i = 1}^n (y_i - (a\\cdot x_i + b))^2\n\\] Dieser Ansatz stammt von Gauss und wird auch die Methode der kleinsten Fehlerquadrate genannt. Die Funktion \\(\\Phi\\) wird insbesondere im Kontext des maschinellen Lernens auch Loss Funktion genannt. Da es sich bei \\(\\Phi(a,b)\\) um eine quadratische Funktion in \\(a\\) und \\(b\\) handelt, besitzt sie ein eindeutiges Minimum, welches auch rein analytisch gefunden werden kann, siehe z.B. Arens u. a. (2022), S. 1526. Wir wollen das Minimum aber mit dem Gradient Descent Verfahren bestimmen. Als Startwert verwenden wir einfach \\((a, b) = (0, 0)\\).\n\nBeispiel 5.5 (Lineare Regression mit Gradient Descent und AAD) \nIn diesem Beispiel gehen wir davon aus, dass der korrekte, aber unbekannte, lineare Zusammenhang durch \\(y = f(x) = 2x+3\\) gegeben ist. Wir erzeugen zunächst eine Anzahl von anz = 50 Datenpunkten, die zufällig um diese Gerade streuen. Danach versuchen wir, die Parameter \\(a\\) und \\(b\\) mittels linearer Regression aus diesen Datenpunkten zu rekonstruieren. Die Funktion loss(a, b) berechnet die Funktion \\(\\Phi(a,b)\\), deren Ableitung automatisch mittels FloatAad berechnet wird. Der so berechnete Gradient von \\(\\Phi\\) wird für das Gradient Descent Verfahren verwendet. Da die Funktion nur zwei unabhängige Variablen hat, verwenden wir direkt den Konstruktor FloatAad und die Methode getDerivatives anstelle der vektorisierten Methoden float2FloatAad und getGradient. Schliesslich werden die Datenpunkte zusammen mit der korrekten Funktion \\(f\\) und der Ausgleichsgeraden \\(g\\) grafisch dargestellt.\n\n\nCode\nfrom floataad import FloatAad, getDerivatives\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef loss(a, b):\n    # X und Y werden im global space gefunden\n    sum = 0\n    for i in range(len(X)):\n        d = float(Y[i]) - (a * float(X[i]) + b)\n        sum += d**2\n    return np.sum(sum)\n    \n\n# Parameter zur Erzeugung der Datenpunkte\nanz = 50 # Anzahl Datenpunkte\nxmin, xmax = 0, 10\ns = 2    # Streuung\n\n# Erzeugende Funktion\nf = lambda x: 2 * x + 3\n\n# Daten erzeugen\nX = np.linspace(xmin, xmax, anz)\nY = f(X)\n# Rauschen hinzufügen\nY = Y + s * np.random.randn(anz)\n\n\n# Gradient Descent\nlam = 0.0005\n\na0, b0 = FloatAad(0), FloatAad(0) # Startwerte\n\nPhi = loss(a0, b0)\ndPhi = getDerivatives(Phi)\n\na1 = a0 - lam * dPhi[a0]\nb1 = b0 - lam * dPhi[b0]\n\nwhile (a1.value-a0.value)**2 + (b1.value-b0.value)**2 &gt; 1e-9:\n    a0, b0 = a1, b1\n    Phi = loss(a0, b0)\n    dPhi = getDerivatives(Phi)\n    a1 = a0 - lam * dPhi[a0]\n    b1 = b0 - lam * dPhi[b0]\n\n# Regressionsgerade\na, b = a1.value, b1.value\ng = np.vectorize(lambda x : a * x + b)\nprint(\"y = g(x) = \" + str(a) + \"x + \" + str(b))\n\n# Daten darstellen\nplt.plot(X,Y, 'b.', X, f(X), 'r--', X, g(X), 'g-.')\nplt.legend([\"Datenpunkte\", \"Erzeugende Funktion\", \"Regressionsgerade\"])\nplt.show()\n\n\ny = g(x) = 1.9184181719335762x + 3.4394278870812\n\n\n\n\n\nAbbildung 5.3: Ausgleichsgerade.\n\n\n\n\n\n\n\n\n5.3.3 Bilder schärfen\nDie Idee zu dieser Anwendung stammt von Slater (2022). Beim Fotografieren kann es passieren, dass die Linse der Kamera nicht richtig fokussiert ist und das Bild dadurch unscharf wirkt. Es ist jedoch möglich, ein Bild bis zu einem gewissen Grad nachträglich zu schärfen.\nAls Testbild verwenden wir das folgende Bild eines Teddybären, welches unter der Creative Commons 4.0 Lizenz auf https://www.pngall.com/toy-png/download/55843 [Letzter Zugriff 02.04.2023] zur Verfügung gestellt wird. Allerdings müssen wir die Auflösung von ursprünglich 180 x 180 Pixel auf 30 x 30 Pixel reduzieren, was mit jeder Bildbearbeitungssoftware gemacht werden kann. Der Grund dafür ist, dass die Anzahl Pixel darüber entscheidet, wie viele FloatAad-Objekte wir erzeugen müssen und während des Gradient Descent Verfahrens wird der Computational Graph bei vielen Variablen sehr gross, so dass beim Berechnen der Ableitungen die maximale Rekursionstiefe überschritten würde. Wir verwenden also das in rechts dargestellte Bild Bear30.jpg.\n\n\n\n\n\n\n\n(a) Original 180 x 180\n\n\n\n\n\n\n\n(b) Testbild 30 x 30\n\n\n\n\nAbbildung 5.4: Testbild eines Teddybärs.\n\n\nEs stellt sich heraus, dass selbst diese kleine Auflösung noch zu viele FloatAad-Objekte benötigt weil jedes Pixel drei Farbkanäle hat. Daher wandeln wir das Bild nach dem Laden zuerst in ein Graustufenbild um.\n\n\nCode\nfrom matplotlib.image import imread\nfrom floataad import float2FloatAad, getGradient\n\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\n\noriginal = imread('bear30.jpg') / 255\n# Bild in Graustufenbild umwandeln\nimage = 1/3 * (original[:,:,0] + original[:,:,1] + original[:,:,2])\n[length, width] = np.shape(image)\n\n\nUm die Unschärfe einer schlecht fokussierten Linse zu simulieren, wenden wir einen Gaussian Blur an. Wie das genau funktioniert wird z.B. in diesem Video von Grant Sanderson von 3blue1brown [Letzter Zugriff 02.04.2023] erklärt.\n\nDie folgende Funktion wendet einen solchen Gaussian Blur auf ein Bild an. Die Vorlage des Codes stammt aus dem Forumsbeitrag How to gauss-filter (blur) a floating point numpy array von stackoverflow [Letzter Zugriff 02.04.2023] und wurde so abgeändert, dass die kernel-Grösse selber bestimmt werden kann.\n\n\nCode\ndef blur(a): \n    # kernel erzeugen\n    kernel_size = 4\n    k1 = [np.array([math.comb(kernel_size, k) for k in range(kernel_size)])]\n    kernel = np.dot(np.transpose(k1), k1)\n    kernel = kernel / np.sum(kernel)\n    \n    # Faltung (Convolution) ausführen\n    arraylist = []\n    for y in range(kernel_size):\n        temparray = np.copy(a)\n        temparray = np.roll(temparray, y - 1, axis=0)\n        for x in range(kernel_size):\n            temparray_X = np.copy(temparray)\n            temparray_X = np.roll(temparray_X, x - 1, axis=1)*kernel[y,x]\n            arraylist.append(temparray_X)\n\n    arraylist = np.array(arraylist)\n    arraylist_sum = np.sum(arraylist, axis=0)\n    return arraylist_sum\n\n\nNun wenden wir diesen Filter auf unser Testbild an.\n\n\nCode\n# Blur erzeugen\nblurredimage = blur(image)\nblurrarray = np.reshape(blurredimage, length*width)\n\n# Plot\nax = plt.subplot(1,2,1)\nax.set_title(\"Original\")\nax.set_axis_off()\nplt.imshow(image, cmap = \"gray\")\n\nax = plt.subplot(1,2,2)\nax.set_title(\"Blurred\")\nax.set_axis_off()\nplt.imshow(blurredimage, cmap = \"gray\")\n\nplt.show()\n\n\n\n\n\nAbbildung 5.5: Testbild und Blur.\n\n\n\n\nUnser Ziel ist wie gesagt, mit Hilfe des Gradient Descent Verfahrens aus dem unscharfen Bild rechts wieder so nahe wie möglich an das Original heranzukommen. Dazu beginnen wir mit einem “Startwert”, das ein Bild ist, welches aus 30 x 30 Pixeln besteht, die alle den Wert 0.5 haben, d.h. mit einem grauen Bild.\n\n\nCode\n# Startwert\nguessimage = np.full(shape = [length, width], fill_value=0.5)\n\n\nDiese \\(30\\times30=900\\) Pixelwerte sind nun der Input unserer Loss Funktion und werden im Laufe der Gradient Descent Iteration so verändert, dass sie diese Funktion minimieren. Diese Loss Funktion definieren wir folgendermassen: Auf das guessimage wird zuerst ein Gaussian Blur angewendet. Danach betrachten wir die Differenzen blur(guessimage) - blurredimage in jedem Pixel. Nach dem Gauss’schen Ansatz der kleinsten Fehlerquadrate definieren wir die Loss Funktion als Summe der Quadrate aller Fehler in den einzelnen Pixeln. Dahinter steckt die Idee, dass wenn die Differenzen zwischen den unscharfen Bildern blur(guessimage) - blur(original) klein ist, dann sollten auch die Differenzen guessimage - original klein sein, d.h. testimage sollte in etwa dem original entsprechen.\nFür die konkrete Umsetzung wandeln wir das Bild mit den \\(30\\times30\\) Pixeln in einen Vektor der Länge 900 um. Das haben wir für für das blurredimage bereits in der Zeile blurrarray = np.reshape(blurredimage, length*width) gemacht. Für das guessimage müssen wir nach der Umwandlung die 900 Einträge zunächst in FloatAad-Objekte umwandeln. All das geschieht in der folgenden Funktion.\n\n\nCode\ndef loss(x):\n    # Input x ist ein Bild, auf welches der Gauss Filter angewendet wird\n    # Danach wird das Bild als 1-dim. Array gespeichert\n    [length, width] = np.shape(x)\n    temp = blur(x)\n    temparray = np.reshape(temp, length * width)\n\n    # Umwandeln in FloatAad\n    temparray = float2FloatAad(temparray)\n\n    y = sum((temparray - blurrarray) ** 2)\n    g = getGradient(temparray, y)\n    return [y.value, g]\n\n\nNun können wir das Gradient Descent Verfahren anwenden.\n\n\nCode\n# Gradient Descent Parameter\nlam = 0.01\ntol = 0.5\n\n[lossval, grad] = loss(guessimage)\n\nwhile lossval &gt; tol:\n    [lossval, grad] = loss(guessimage) \n    diff = np.reshape(grad, [length, width])\n    guessimage = guessimage - lam * diff\n\n# Plot\nax = plt.subplot(2,2,1)\nax.set_title(\"Guess\")\nax.set_axis_off()\nplt.imshow(guessimage, cmap = \"gray\")\n\nax = plt.subplot(2,2,2)\nax.set_title(\"Blurred Guess\")\nax.set_axis_off()\nblurredguess = blur(guessimage)\nplt.imshow(blurredguess, cmap = \"gray\")\n\nax = plt.subplot(2,2,3)\nax.set_title(\"Original - Guess\")\nax.set_axis_off()\ndiffOrig = 0.5 * (image - guessimage + 1)\nplt.imshow(diffOrig, cmap = \"gray\")\n\nax = plt.subplot(2,2,4)\nax.set_title(\"Blurred - Blurred Guess\")\nax.set_axis_off()\ndiffBlurred = 0.5 * (blurredimage - blurredguess + 1)\nplt.imshow(diffBlurred, cmap = \"gray\")\n\nplt.show()\n\n\n\n\n\nAbbildung 5.6: Resultat des Schärfens und Differenz zum Original\n\n\n\n\nDas vollständige Programm kann auch hier heruntergeladen werden."
  },
  {
    "objectID": "HigherDimFunctions.html#sec-FuncRnToRm",
    "href": "HigherDimFunctions.html#sec-FuncRnToRm",
    "title": "4  Funktionen mit mehreren In- und Outputs",
    "section": "4.3 Funktionen mit mehreren Ein- und Ausgabewerten",
    "text": "4.3 Funktionen mit mehreren Ein- und Ausgabewerten\nEine Funktion \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) hat die Form \\[\nf(x_1, \\ldots, x_n) = \\left( \\begin{align*} y_1(x_1, &\\ldots, x_n) \\\\ &\\vdots \\\\ y_m(x_1, &\\ldots, x_n) \\end{align*} \\right)\n\\] Die Ableitung einer solchen Funktion wird durch die Jacobi Matrix \\[\nJf = \\begin{pmatrix}\n    \\frac{\\partial y_1}{\\partial x_1} & \\ldots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n    \\vdots & & \\vdots \\\\\n    \\frac{\\partial y_m}{\\partial x_1} & \\ldots & \\frac{\\partial y_m}{\\partial x_n}\n\\end{pmatrix}\n\\in\\mathbb{R}^{m\\times n}\n\\] gegeben. Auch hierzu findet der Leser mehr Informationen in Arens u. a. (2022), S. 878.\n\nBeispiel 4.2 (Eine Funktion mit zwei Ein- und drei Ausgabwerten) \nBetrachte die Funktion \\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) \\[\nf(x_0, x_1) =\n    \\begin{pmatrix}\n        x_0\\cdot \\sqrt{x_1} + 3x_1 \\\\\n        \\cos(x_0) / x_1 \\\\\n        e^{x_0 ^2\\cdot x_1}\n    \\end{pmatrix}\n\\]\nDie Jacobi Matrix lautet in diesem Fall \\[\nJf =\n\\begin{pmatrix}\n    \\sqrt{x_1} & \\frac{x_0}{2\\sqrt{x_1}} + 3 \\\\\n    -\\frac{\\sin(x_0)}{x_1} & -\\frac{\\cos(x_0)}{x_1^2} \\\\\n    e^{x_0^2\\cdot x_1}\\cdot 2 x_0 x_1 & e^{x_0^2\\cdot x_1}\\cdot x_0^2\n\\end{pmatrix}\n\\]\nAusgewertet an der Stelle \\((x_0, x_1) = (2, 1)\\) ergibt dies \\[\nf(2,1) \\approx \\begin{pmatrix} 5 \\\\ -0.4161... \\\\ 54.5981... \\end{pmatrix}, \\qquad\nJF \\vert _{(2,1)} \\approx\n    \\begin{pmatrix}  \n        1 & 4 \\\\\n        -0.9092... & 0.4161... \\\\\n        218.3926... & 218.3926...\n    \\end{pmatrix}\n\\]\n\n\nUm die Funktion aus dem Beispiel mit SAD abzuleiten kombinieren wir die Techniken aus den beiden vorherigen Abschnitten. Je nach Initialisierung von xdot erhalten wir die erste oder die zweite Spalte von \\(JF\\).\n\n1. Spalte2. Spalte\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\nimport numpy as np\n\ndef f(x):\n    xdot = [1, 0]\n    x = float2FloatSad(x, xdot)\n    y1 = x[0]*mathsad.sqrt(x[1]) + 3*x[1]\n    y2 = mathsad.cos(x[0]) / x[1]\n    y3 = mathsad.exp(x[0]**2 * x[1])\n    return [y1, y2, y3]    \n\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\ngetValues = np.vectorize(lambda y : y.value)\ngetDerivatives = np.vectorize(lambda y : y.derivative)\n\n\nx0 = (2, 1)\ny0 = f(x0)\nprint(\"Funktionswerte:\")\nprint(getValues(y0))\nprint(\"1. Spalte von Jf:\")\nprint(getDerivatives(y0))\n\n\nFunktionswerte:\n[ 5.         -0.41614684 54.59815003]\n1. Spalte von Jf:\n[  1.          -0.90929743 218.39260013]\n\n\n\n\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\nimport numpy as np\n\ndef f(x):\n    xdot = [0, 1]\n    x = float2FloatSad(x, xdot)\n    y1 = x[0]*mathsad.sqrt(x[1]) + 3*x[1]\n    y2 = mathsad.cos(x[0]) / x[1]\n    y3 = mathsad.exp(x[0]**2 * x[1])\n    return [y1, y2, y3]    \n\n\nfloat2FloatSad = np.vectorize(lambda x, v : FloatSad(x,v))\ngetValues = np.vectorize(lambda y : y.value)\ngetDerivatives = np.vectorize(lambda y : y.derivative)\n\n\nx0 = (2, 1)\ny0 = f(x0)\nprint(\"Funktionswerte:\")\nprint(getValues(y0))\nprint(\"2. Spalte von Jf:\")\nprint(getDerivatives(y0))\n\n\nFunktionswerte:\n[ 5.         -0.41614684 54.59815003]\n2. Spalte von Jf:\n[  4.           0.41614684 218.39260013]\n\n\n\n\n\nInitialisiert man allgemein xdot mit dem Vektor \\(\\vec{r} = (r_1, \\ldots, r_n)^\\intercal\\), dann erhält man als Resultat das Produkt \\[\nJf \\cdot \\vec{r} =\n\\begin{pmatrix}\n    \\frac{\\partial y_1}{\\partial x_1} & \\ldots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n    \\vdots & & \\vdots \\\\\n    \\frac{\\partial y_m}{\\partial x_1} & \\ldots & \\frac{\\partial y_m}{\\partial x_n}\n\\end{pmatrix}\n\\cdot \\begin{pmatrix} r_1 \\\\ \\vdots \\\\ r_n \\end{pmatrix}\n\\]\nBraucht man die gesammte Jacobi Matrix, dann muss man also die Funktion so oft aufrufen, wie die Matrix Spalten hat, d.h. len(x) Mal. Die SAD Methode ist also effizient, wenn eine Funktion mehr Aus- als Eingabewerte hat. Der ineffizienteste Fall tritt auf, wenn die Funktion aus vielen Eingabewerte nur einen Ausgabewert berechnet. Mit anderen Worten: Das Bestimmen des Gradienten einer Funktion \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) benötigt den grössten Aufwand gemessen an der Anzahl der zu berechnenden Werte. Abhilfe schafft in so einem Fall die Adjungierte Automatische Differentiation (AAD).\nZum Schluss sei noch angemerkt, dass die Definition der drei Funktionen float2FloatSad, getValues und getDerivatives in die Datei floatsad.py geschrieben werden könnten (beachte, dass sie nicht eingerückt werden wie die restlichen Befehle der Klasse). Dann vereinfacht sich das obige Programm:\n\n\nCode\nfrom floatsad import *\nimport mathsad\n\ndef f(x):\n    xdot = [1, 0]\n    x = float2FloatSad(x, xdot)\n    y1 = x[0]*mathsad.sqrt(x[1]) + 3*x[1]\n    y2 = mathsad.cos(x[0]) / x[1]\n    y3 = mathsad.exp(x[0]**2 * x[1])\n    return [y1, y2, y3]    \n\nx0 = (2, 1)\ny0 = f(x0)\nprint(getValues(y0))\nprint(getDerivatives(y0))\n\n\n\n\n\n\nArens, Tilo, Frank Hettlich, Christian Karpfinger, Ulrich Kockelkorn, Klaus Lichtenegger, und Hellmuth Stachel. 2022. Mathematik. Berlin, Heidelberg: Springer Berlin Heidelberg."
  },
  {
    "objectID": "aad.html#gradient-descent-auf-einer-fläche",
    "href": "aad.html#gradient-descent-auf-einer-fläche",
    "title": "5  Adjungierte Algorithmische Differentiation",
    "section": "5.4 Gradient Descent auf einer Fläche",
    "text": "5.4 Gradient Descent auf einer Fläche\n\nBetrachte die Funktion \\(f(x_0, x_1) = x_0^4 + x_1^4 + x_0 x_1^3 - x_0^2 x_1 - x_1^2\\).\n\nHier wird diese Funktion einmal als 2-dimensionale Fläche und einmal als Konturlinien Plot dargestellt. Im linken Feld kann man den Startpunkt und \\(\\lambda\\) wählen und die ersten 30 Iterationen darstellen.\n\n\n\n\n\n\n\n5.4.1 Lineare Regression\n\n\n5.4.2 Bilder schärfen\n\n\n5.4.3 Ein einaches neuronales Netz\n\n\n\n\nArens, Tilo, Frank Hettlich, Christian Karpfinger, Ulrich Kockelkorn, Klaus Lichtenegger, und Hellmuth Stachel. 2022. Mathematik. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, und Jeffrey Mark Siskind. 2018. „Automatic Differentiation in Machine Learning: a Survey“. Journal of Machine Learning Research 18 (153): 1–43. http://jmlr.org/papers/v18/17-468.html.\n\n\nGriewank, Andreas, und Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2. Aufl. Other Titles in Applied Mathematics 105. Philadelphia, PA: SIAM. http://bookstore.siam.org/ot105/.\n\n\nHenrard, Marc. 2017. Algorithmic Differentiation in Finance Explained. Financial Engineering Explained. Cham: Palgrave Macmillan. https://doi.org/10.1007/978-3-319-53979-9.\n\n\nRadcliffe, Sidney. 2021. „Reverse-mode automatic differentiation from scratch, in Python“. 22. Mai 2021. https://sidsite.com/posts/autodiff/.\n\n\nSlater, Max. 2022. „Differentiable programming from scratch“. Juli 2022. https://thenumb.at/Autodiff/."
  },
  {
    "objectID": "aad.html#sec-modulMathAad",
    "href": "aad.html#sec-modulMathAad",
    "title": "5  Adjungierte Algorithmische Differentiation",
    "section": "5.4 Das Modul mathaad",
    "text": "5.4 Das Modul mathaad\nIn diesem Kapitel schreiben wir ein Modul mathaad, welches eine Auswahl an mathematischen Funktionen beinhaltet, die auf FloatAad-Objekte angewendet werden können. Wir gehen dabei analog zum Kapitel 3.4 vor, beschränken uns aber auf die Funktionen sqrt, exp, log und die drei trigonometrischen Funktionen. Ausserdem verwenden wir Funktionen aus numpy weil wir als Argumente auch Arrays von FloatAad-Objekte übergeben wollen. Die Funktion soll in diesem Fall elementweise angewendet werden, wofür der Decorator @np.vectorize sorg. Der folgende Code sollte in einer Datei mathaad.py gespeichert und im gleichen Ordner wie die anderen Dateien abgelegt werden.\n\n\nCode\nimport numpy as np\nfrom floataad import FloatAad\n\n@np.vectorize\ndef sqrt(x):\n    newValue = np.sqrt(x.value)\n    newDerivative = (\n        (x, 1. / (2 * np.sqrt(x.value))),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef exp(x):\n    newValue = np.exp(x.value)\n    newDerivative = (\n        (x, newValue),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef log(x):\n    newValue = np.log(x.value)\n    newDerivative = (\n        (x, 1. / x.value),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef sin(x):\n    newValue = np.sin(x.value)\n    newDerivative = (\n        (x, np.cos(x.value)),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef cos(x):\n    newValue = np.cos(x.value)\n    newDerivative = (\n        (x, -np.sin(x.value)),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef tan(x):\n    return sin(x) / cos(x)\n\n\nDie Datei kann hier heruntergeladen werden."
  },
  {
    "objectID": "aad.html#das-modul-mathaad-im-einsatz",
    "href": "aad.html#das-modul-mathaad-im-einsatz",
    "title": "5  Adjungierte Algorithmische Differentiation",
    "section": "5.5 Das Modul mathaad im Einsatz",
    "text": "5.5 Das Modul mathaad im Einsatz\nZum Schluss wollen wir uns ein einfaches neuronales Netz zur Lösung eines berühmten Klassifikationsproblems programmieren.\n\n5.5.1 Ein einaches neuronales Netz zur Klassifikation von Lilien\nWir verwenden für dieses Beispiel einen der bekanntesten Datensätze, nämlich Fisher’s Datensatz zu Lilien. Er wurde bereits 1936 vom britischen Biologen und Statistier Ronald Fisher verwendet und ist heute nach ihm benannt. Auch in Barot u. a. (2022) (S. 160) wird auf diesen Datensatz Bezug genommen. Die Datei iris.data kann von Fisher (1936) heruntergeladen werden. Über diesen Datensatz liest man dort\n\n[It is] A small classic dataset from Fisher, 1936. One of the earliest datasets used for evaluation of classification methodologies. […] This is perhaps the best known database to be found in the pattern recognition literature. Fisher’s paper is a classic in the field and is referenced frequently to this day.\n\nDie Datei enthält 150 Datensätze (Zeilen) mit je 5 Spalten. Die 1. Spalte gibt die Länge des Kelchblattes (Sepalum) an, die 2. Spalte die Breite des Kelchblattes, die 3. Spalte enthält die Länge des Kornblattes (Petalum) und die 4. Spalte enthält die Breite des Kornblattes. Die 5. Spalte schliesslich gibt an, von welcher Lilienart die Daten stammen. Im Datensatz gibt es drei Arten von Lilien (Iris setosa, Iris versicolor und Iris virginica) und von jeder Art sind 50 Messungen enthalten.\nUnser Ziel wird es sein, auf Grund der vier gemessenen Grössen (Länge und Breite des Kelch- bzw. Kornblattes) die Art vorher zu sagen. Als erstes wollen wir die Daten grafisch als Scatterplot darstellen (Frochte (2021), S. 71). Zunächst ändern wir aber die Label in der 5. Spalte noch zu 0 (Iris setosa), 1 (Iris versicolor), bzw. 2 (Iris virginica)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\n\nfrom floataad import float2FloatAad, getValues, getGradient\nimport mathaad\n\n# Daten einlesen und Labels ändern\nfString = open('iris.data','r')\nfFloat  = open('iris.csv','w')\n\nfor line in fString:\n    line = line.replace('Iris-setosa', '0')\n    line = line.replace('Iris-versicolor', '1')\n    line = line.replace('Iris-virginica', '2')\n    fFloat.write(line)\n\nfString.close()\nfFloat.close()\n\nfFloat = open('iris.csv','r')\ndataset = np.loadtxt(fFloat, delimiter = ',')\nfFloat.close()\n\n# Daten plotten\nfig = plt.figure(1)\n\nax = fig.add_subplot(2,2,1)\nax.scatter(dataset[0:50,0], dataset[0:50,1], \n            c = 'red', s = 20, alpha = 0.6)\nax.scatter(dataset[50:100,0], dataset[50:100,1], \n            c = 'green', marker = '^', s = 20, alpha = 0.6)\nax.scatter(dataset[100:150,0], dataset[100:150,1], \n            c = 'blue', marker = '*', s = 20, alpha = 0.6)\nax.set_xlabel('Kelchblattlaenge (cm)')\nax.set_ylabel('Kelchblattbreite (cm)')\n\nax = fig.add_subplot(2,2,2)\nax.scatter(dataset[0:50,2], dataset[0:50,3], \n            c = 'red', s = 20, alpha = 0.6)\nax.scatter(dataset[50:100,2], dataset[50:100,3], \n            c = 'green', marker = '^', s = 20, alpha = 0.6)\nax.scatter(dataset[100:150,2], dataset[100:150,3], \n            c = 'blue', marker = '*', s = 20, alpha = 0.6)\nax.set_xlabel('Kronblattlaenge (cm)')\nax.set_ylabel('Kronblattbreite (cm)')\n\nax = fig.add_subplot(2,2,3)\nax.scatter(dataset[0:50,0], dataset[0:50,2], \n            c = 'red', s = 20, alpha = 0.6)\nax.scatter(dataset[50:100,0], dataset[50:100,2], \n            c = 'green', marker = '^', s = 20, alpha = 0.6)\nax.scatter(dataset[100:150,0], dataset[100:150,2], \n            c = 'blue', marker = '*', s = 20, alpha = 0.6)\nax.set_xlabel('Kelchblattlaenge (cm)')\nax.set_ylabel('Kronblattlaenge (cm)')\n\nax = fig.add_subplot(2,2,4)\nax.scatter(dataset[0:50,1], dataset[0:50,3], \n            c = 'red', s = 20, alpha = 0.6)\nax.scatter(dataset[50:100,1], dataset[50:100,3], \n            c = 'green', marker = '^', s = 20, alpha = 0.6)\nax.scatter(dataset[100:150,1], dataset[100:150,3], \n            c = 'blue', marker = '*', s = 20, alpha = 0.6)\nax.set_xlabel('Kelchblattbreite (cm)')\nax.set_ylabel('Kronblattbreite (cm)')\n\nplt.show()\n\n\n\n\n\nAbbildung 5.7: Scatter Plots der Fisher Iris Daten\n\n\n\n\nNun wählen wir uns aus den 150 Einträgen des Datensatzes zufällig 30 heraus, welche wir als Testdaten verwenden. Die übrigen 120 Einträge dienen uns als Trainingsdaten, mit denen wir unser neuronales Netz trainieren werden.\n\n\nCode\n# Daten in Trainings- und Testdaten aufteilen\nX = dataset[:, 0:4] # Messwerte\nY = dataset[:, 4]   # Label\nallData = np.arange(0, X.shape[0])\ntestIndices = np.random.choice(X.shape[0], size = 30, replace = False)\ntrainIndices = np.delete(allData, testIndices)\ndataRecords = len(testIndices)\nXTrain = X[trainIndices, :]\nYTrain = np.array(Y[trainIndices], dtype = np.int32)\nXTest = X[testIndices, :]\nYTest = Y[testIndices]\n\n\nUnser Netz soll aus 4 Inputneuronen und 3 Outputneuronen bestehen. Die vier Inputs stehen für die vier Messwerte X[0], ..., X[3]. Wenn dort die entsprechenden Messwerte eingegeben wurden, dann werden bei den Outputneuronen Y[0], ..., Y[2] vier Werte zwischen \\(0\\) und \\(1\\) generiert, welche die Wahrscheinlichkeiten darstellen, dass es sich um die entsprechende Lilienart handelt. Das Neuron mit dem grössten Wert stellt unsere Vorhersage dar. Das neuronale Netz hat also die folgende Architektur:\n\n\n\n\n\n\n   \n\nx0\n\n   \n\ny0\n\n   \n\nx0–y0\n\n   \n\ny1\n\n   \n\nx0–y1\n\n   \n\ny2\n\n   \n\nx0–y2\n\n   \n\nx1\n\n   \n\nx1–y0\n\n   \n\nx1–y1\n\n   \n\nx1–y2\n\n   \n\nx2\n\n   \n\nx2–y0\n\n   \n\nx2–y1\n\n   \n\nx2–y2\n\n   \n\nx3\n\n   \n\nx3–y0\n\n   \n\nx3–y1\n\n   \n\nx3–y2\n\n  \n\n\nAbbildung 5.8: Neuronales Netz für den Lilienklassifikator.\n\n\n\n\nEntlang jeder Kante multiplizeren wir den Wert \\(x_i\\) mit einem Gewicht \\(w_{ij}\\) und addieren die vier Werte. Zu dieser Summe addieren wir noch einen Bias \\(b_i\\) so entstehen drei Zwischenwerte \\[\nz_i = \\sum_{j=0}^3 w_{ij}x_j + b_i \\quad\\textrm{für}\\quad i\\in\\lbrace0, 1, 2\\rbrace\n\\] In Matrixschreibweise können wir das ausdrücken als \\[\n\\vec{z} = W\\cdot \\vec{x} + \\vec{b}\n\\] mit \\(W\\in \\mathbb{R}^{3\\times 4}\\), \\(\\vec{x}\\in\\mathbb{R}^4\\) und \\(\\vec{b}, \\vec{z} \\in \\mathbb{R}^3\\). Die Zwischenwerten \\(z_i\\) müssen nun noch so skaliert werden, dass sie eine Wahrscheinlichkeitsverteilung darstellen. Das erreichen wir, die Softmax Funktion anwenden: \\[\n\\hat y_i = \\sigma_i(\\vec z) = \\frac{e^{z_i}}{\\sum_{k=0}^2 e^{z_k}} \\quad\\textrm{für}\\quad i\\in\\lbrace0, 1, 2\\rbrace\n\\] Mehr zur Softmax Funktion findet man z.B. in Frochte (2021) (S. 240). Im folgenden Programm unterscheiden wir noch, ob wir die Funktion auf einen Vektor aus Zahlen oder einen Vektor aus FloatAad-Objekten anwenden.\n\n\nCode\n# Softmax Funktion für Vektor z\ndef softmax(z):\n    if z.dtype == \"object\":\n        return [mathaad.exp(x) / sum(mathaad.exp(z)) for x in z]\n    else:\n        return [np.exp(x) / sum(np.exp(z)) for x in z]\n\n\nUnsere Vorhersage ist dann der Wert \\[\ny = \\operatorname{argmax} \\hat y_i \\in \\lbrace0, 1, 2\\rbrace\n\\]\nDie Frage ist also, wie wir die 12 Gewichte \\(w_{ij}\\) und die Bias \\(b_i\\) wählen. Letztere setzen wir einfach auf \\(b_i = 1\\) für alle \\(i\\). Bei der Bestimmung der Gewichte kommen unsere Trainingsdaten ins Spiel. Wir definieren uns eine Loss Funktion \\(J : \\mathbb{R}^{12} \\rightarrow \\mathbb{R}\\), welche als Input die Gewichte \\(W\\) erhält und als Output die so genannte Cross-Entropy liefert, welche ein Mass für die Abweichung von der korrekten Klassifikation ist. Sie wird bestimmt, indem man zuerst \\[\nD(\\vec y, \\vec {\\hat y}) = - \\sum_i y_i\\cdot \\ln(\\hat y_i)\n\\] berechnet, wobei die \\(\\hat y_i\\) wie oben definiert sind und \\(\\vec y = (y_i)_{i=0, 1, 2}\\) die One-Hot Codierung der korrekten Labels ist, d.h. \\[\\begin{align*}\n    \\vec y &= (\\matrix{1, 0, 0})^\\intercal \\quad\\textrm{falls der korrekte Label 0 (Iris setosa) ist,} \\\\\n    \\vec y &= (\\matrix{0, 1, 0})^\\intercal \\quad\\textrm{falls der korrekte Label 1 (Iris versicolor) ist,} \\\\\n    \\vec y &= (\\matrix{0, 0, 1})^\\intercal \\quad\\textrm{falls der korrekte Label 2 (Iris virginica) ist.}\n\\end{align*}\\]\nDie Cross-Entropy über alle \\(N=120\\) Beispiele erhält man dann als Mittelwert dieser Grössen \\[\nJ(W) = \\frac{1}{N}\\sum_{n=0}^{N-1} D(\\vec y^{(n)}, \\vec{\\hat y}^{(n)})\n\\] Weitere Details zur Cross-Entropy findet man ebenfalls in Frochte (2021) (S. 241). In der Python Funktion wandeln wir die Gewichte zuerst in FloatAad-Objete um und geben am Schluss den Wert \\(J(W)\\) und die den Gradienten \\(\\nabla J (W)\\) zurück.\n\n\nCode\ndef loss(Weights, bias, XTrain, YTrain):\n    \n    N = len(YTrain) # Anzahl Trainingsdaten\n    \n    # Gewichte als FloatAad-Matrix\n    Wtemp = float2FloatAad(Weights)\n    WtempMatrix = np.reshape(Wtemp, [4, 3])\n    \n    \n    # Labels als One-Hot Encoding\n    YOneHot = np.zeros([N, 3])\n    YOneHot[range(N), YTrain] = 1\n\n    Z = XTrain @ WtempMatrix + bias\n    # Softmax auf jede Zeile anwenden\n    Yhat = np.apply_along_axis(softmax, 1, Z)\n    \n    # Cross Entropy\n    D = [- YOneHot[i] @ mathaad.log(Yhat[i]) for i in range(N) ]\n    J = sum(D) / N\n    LossValue = getValues(J)\n    LossGrad = np.array(getGradient(Wtemp, J))\n    return [LossValue, LossGrad]\n\n\nMan beachte, dass das Matrixprodukt in der Zeile Z = XTrain @ WtempMatrix + bias eigentlich \\(X\\cdot W^\\intercal + \\vec b^\\intercal\\) ist, mit \\(X\\in\\mathbb{R}^{120\\times 4}\\) und \\(W^\\intercal \\in \\mathbb{R}^{4\\times 3}\\). Die Addition von bias wird dann auf jede Zeile \\(X\\cdot W^\\intercal \\in \\mathbb{R}^{120\\times 3}\\) angewendet.\nNun verwenden wir wieder das Gradient Descent Verfahren, um ein lokales Minimum der Loss Funktion zu finden und damit die Gewichte zu optimieren. Wir initialisieren die Gewichte mit zufälligen Werten.\n\n\nCode\n# Gewichte initialisieren\nW = np.random.random(4 * 3)\nb = np.ones(3)  # bias\n\n# Fit mit Gradient Descent\nlam = 0.5 # Lernrate\ntol = 1e-2\nstart = time()\n[Lval, Lgrad] = loss(W, b, XTrain, YTrain)\nwhile np.linalg.norm(Lgrad) &gt; tol:\n    W1 = W - lam * Lgrad\n    [Lval, Lgrad] = loss(W1, b, XTrain, YTrain)\n    W = W1\nend = time()\nzeit = end - start\nprint(\"Der Lernprozess dauerte %1.2f Sekunden.\" %zeit)\n\n\nDer Lernprozess dauerte 12.33 Sekunden.\n\n\nZum Schluss wenden wir die Matrix mit den optimierten Gewichten auf die 30 Testdaten an, welche wir im Trainingsprozess noch nicht verwendet hatten, und zählen, wie viele davon durch unser Netz korrekt klassifiziert werden.\n\n\nCode\n# Test des Modells\nW = np.reshape(W, [4, 3])\nZ = XTest @ W + b\nYp = Yp = np.apply_along_axis(softmax, 1, Z)\nY = np.apply_along_axis(np.argmax, 1, Yp)\n\n# Vergleich mit Resultaten\nnCorrect = sum(Y == YTest)\nprint(\"%d von %d wurden korrekt klassifiziert.\" %(nCorrect, dataRecords))\n\n\n28 von 30 wurden korrekt klassifiziert.\n\n\nDas vollständige Program kann hier heruntergeladen werden.\n\n\n\n\nArens, Tilo, Frank Hettlich, Christian Karpfinger, Ulrich Kockelkorn, Klaus Lichtenegger, und Hellmuth Stachel. 2022. Mathematik. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nBarot, Michael, Britta Dorn, Ghislain Fourny, Jens Gallenbacher, Juraj Hromkovic, und Regula Lacher. 2022. INFORMATIK, Data Science und Sicherheit: Grundlagen der Informatik für Schweizer Maturitätsschulen. Klett.\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, und Jeffrey Mark Siskind. 2018. „Automatic Differentiation in Machine Learning: a Survey“. Journal of Machine Learning Research 18 (153): 1–43. http://jmlr.org/papers/v18/17-468.html.\n\n\nFisher, R. A. 1936. „Iris“. UC Irvine Machine Learning Repository. 1936. https://archive-beta.ics.uci.edu/dataset/53/iris.\n\n\nFrochte, Jörg. 2021. Maschinelles Lernen - Grundlagen und Algorithmen in Python. 3. Aufl. Hanser.\n\n\nGriewank, Andreas, und Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2. Aufl. Other Titles in Applied Mathematics 105. Philadelphia, PA: SIAM. http://bookstore.siam.org/ot105/.\n\n\nHenrard, Marc. 2017. Algorithmic Differentiation in Finance Explained. Financial Engineering Explained. Cham: Palgrave Macmillan. https://doi.org/10.1007/978-3-319-53979-9.\n\n\nRadcliffe, Sidney. 2021. „Reverse-mode automatic differentiation from scratch, in Python“. 22. Mai 2021. https://sidsite.com/posts/autodiff/.\n\n\nSlater, Max. 2022. „Differentiable programming from scratch“. Juli 2022. https://thenumb.at/Autodiff/."
  },
  {
    "objectID": "ADOneDimManually.html#sec-modulMathSad",
    "href": "ADOneDimManually.html#sec-modulMathSad",
    "title": "3  Standard Algorithmische Differentiation für eindimensionale Funktionen",
    "section": "3.4 Das Modul mathsad",
    "text": "3.4 Das Modul mathsad\nMit der Klasse FloatSad können wir Funktionswerte und Ableitungen von algebraischen Funktionen bilden. Wir können aber unsere FloatSad-Objekte noch nicht mit den Funktionen aus dem Python-Modul math verwenden, z.B. mit exp oder sin. In diesem Abschnitt wollen wir ein eigenes Modul mathsad schreiben, in dem wir die Funktionen aus Tabelle 3.2 so implementieren, dass wir sie auf FloatSad-Objekte anwenden können.\n\n\nTabelle 3.2: Funktionen des Moduls math (Auswahl)\n\n\nsqrt\nexp\nlog\n\n\nsin\ncos\ntan\n\n\nasin\nacos\natan\n\n\nsinh\ncosh\ntanh\n\n\nasinh\nacosh\natanh\n\n\nfabs\n\n\n\n\n\n\nGemäss der Python-Dokumentation liefert die Funktion math.exp(x) präzisere Werte als math.e ** x oder math.pow(math.e, x). Die Funktion math.log(x) berechnet den Logarithmus zur Basis \\(e\\), man kann ihr aber als zweites Argument auch eine andere Basis übergeben, z.B. math.log(x,b), was dann mit math.log(x)/math.log(b) berechnet wird. Die Funktion math.fabs(x) schliesslich berechnet den Absolutbetrag \\(|x|\\). Ihre Ableitung ist\n\n(math.fabs(v)).derivative = v.derivative if v&gt;=0 else -v.derivative\n\nDie Funktion \\(y=|x|\\) ist an der Stelle \\(x=0\\) eigentlich nicht differenzierbar. Da wir aber nicht Ableitungsfunktionen, sondern nur Werte von Ableitungen an einer bestimmten Stelle berechnen, reicht es, den rechts- oder linksseitigen Grenzwert zurückzugeben, siehe Gander (1992). Wir müssen es dem Benutzer überlassen, das Ergebnis im jeweiligen Kontext korrekt zu interpretieren.\n\n3.4.1 Die Funktion sqrt\nBeginnen wir mit der Implementierung der Wurzelfunktion.\n\n\nCode\nimport math\nfrom floatsad import FloatSad\n\ndef sqrt(x):\n    newValue = math.sqrt(x.value)\n    newDerivative = 1/(2*math.sqrt(x.value)) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\nif __name__ == '__main__':\n\n    def f(x):\n        x = FloatSad(x)\n        y = 1 / sqrt(x**2 + 1)\n        return y\n\n    x0 = -1\n    print(f(x0))\n\n\n&lt; 0.7071067811865475 ; 0.3535533905932737 &gt;\n\n\nWir gehen davon aus, dass x ein FloatSad-Objekt ist. Für den Wert von sqrt(x) verwenden wir einfach die Funktion math.sqrt. Diese enthält auch die nötige Fehlerbehandlung. Zusätzlich berechnen wir aber noch den Wert der Ableitung mit Hilfe der bekannten Ableitungsregel und wie zuvor wenden wir immer die Kettenregel an. Das Programm enthält auch ein Testprogramm, welches die Ableitung der Funktion \\(f(x) = \\frac{1}{\\sqrt{x^2+1}}\\) an der Stelle \\(x_0 = -1\\) berechnet. Zur Kontrolle kann die GeoGebra-Vorlage zu Beginn von Kapitel 3.1 verwendet werden.\n\n\n3.4.2 Die Funktionen exp und log\n\nÜbungsaufgabe 3.17 (Exponentialfunktion) \nKopiere den obigen Code und speichere ihn in einer Datei mit dem Namen mathsad.py. Speichere die Datei im gleichen Ordner wie die anderen Dateien. Ergänze die Datei danach mit der Funktion exp. Wähle eine neue Testfunktion im main, um dich von der Richtigkeit deiner Lösung zu überzeugen.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n\nCode\ndef exp(x):\n    newValue = math.exp(x.value)\n    newDerivative = math.exp(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\nFür die Logarithmusfunktion müssen wir uns wieder etwas mehr Gedanken machen. Mit def log(x, b = math.e) kann man der Basis \\(b\\) wie oben beschrieben den Standardwert \\(b=e\\) geben. Solange b vom Typ int oder float ist, kann man einfach die bekannte Ableitungsregel anwenden. Wenn aber b ein FloatSad-Objekt ist, wie z.B. in v3 = math.log(v1, v2), dann müssen wir den Basiswechselsatz \\[\nv_3(x) = \\log_{v_2(x)}(v_1(x)) = \\frac{\\ln(v_1(x))}{\\ln(v_2(x))}\n\\] verwenden und mit der Quotientenregel ableiten.\n\nÜbungsaufgabe 3.18 (Logarithmusfunktion) \nÜberlege dir, wie die Ableitung von \\(v_3(x)\\) aussieht. Ergänze danach die Datei mathsad.py mit der Implementation der Logarithmusfunktion. Überzeuge dich mit einer Testfunktion von der Richtigkeit deines Programms.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nDie Ableitung lautet \\[\n\\frac{d}{dx}v_3(x) = \\frac{\\frac{v_1'(x)}{v_1(x)}\\cdot \\ln(v_2(x))-\\ln(v_1(x))\\cdot\\frac{v_2'(x)}{v_2(x)}}{\\ln^2(v_2(x))}\n\\]\n\n\nCode\ndef log(x, b = math.e):\n    if type(b) in (float, int):\n        newValue = math.log(x.value, b)\n        newDerivative = 1 / (x.value * math.log(b)) * x.derivative\n    else:\n        newValue = math.log(x.value, b.value)\n        newDerivative = (x.derivative/x.value * math.log(b.value) - math.log(x.value) * b.derivative / b.value) \\\n            / math.pow(math.log(b.value), 2)\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\n\n\n3.4.3 Die trigonometrischen Funktionen und ihre Umkehrfunktionen\nBei den trigonometrischen Funktionen und den Arcus Funktionen können wir einfach die bekannten Ableitungsregeln verwenden.\n\nÜbungsaufgabe 3.19 (Trigonometrische Funktionen) \nErgänze die Datei mathsad.py mit den Funktionen sin, cos und tan, sowie den Funktionen asin, acos und atan.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nBeachte, dass man für tan einfach \\(\\tan(x)=\\frac{\\sin(x)}{\\cos(x)}\\) verwenden kann, wenn sin und cos bereits implementiert sind.\n\n\nCode\ndef sin(x):\n    newValue = math.sin(x.value)\n    newDerivative = math.cos(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef cos(x):\n    newValue = math.cos(x.value)\n    newDerivative = -math.sin(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef tan(x):\n    return sin(x) / cos(x)\n\ndef asin(x):\n    newValue = math.asin(x.value)\n    newDerivative = 1/math.sqrt( 1 - math.pow(x.value, 2)) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef acos(x):\n    newValue = math.acos(x.value)\n    newDerivative = -1/math.sqrt( 1 - math.pow(x.value, 2)) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef atan(x):\n    newValue = math.atan(x.value)\n    newDerivative = 1/(math.pow(x.value, 2) + 1) * x.derivative\n    return FloatSad(newValue, newDerivative) \n\n\n\n\n\n\n\n3.4.4 Die hyperbolischen Funktionen und ihre Umkehrfunktionen\nAuch bei den hyperbolischen Funktionen und den Area Funktionen verwenden wir die bekannten Ableitungsregeln.\n\nÜbungsaufgabe 3.20 (Hyperbolische Funktionen) \nErgänze die Datei mathsad.py mit den Funktionen sinh, cosh und tanh, sowie den Funktionen asinh, acosh und atanh.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nWie bei den trigonometrischen Funktionen gilt auch hier \\(\\tanh(x)=\\frac{\\sinh(x)}{\\cosh(x)}\\).\n\n\nCode\ndef sinh(x):\n    newValue = math.sinh(x.value)\n    newDerivative = math.cosh(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef cosh(x):\n    newValue = math.cosh(x.value)\n    newDerivative = math.sinh(x.value) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef tanh(x):\n    return sinh(x) / cosh(x)\n\ndef asinh(x):\n    newValue = math.asinh(x.value)\n    newDerivative = 1/math.sqrt(math.pow(x.value, 2) + 1) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef acosh(x):\n    newValue = math.acosh(x.value)\n    newDerivative = 1/math.sqrt(math.pow(x.value, 2) - 1) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\ndef atanh(x):\n    newValue = math.atanh(x.value)\n    newDerivative = -1/(math.pow(x.value, 2) - 1) * x.derivative\n    return FloatSad(newValue, newDerivative)\n\n\n\n\n\n\n\n3.4.5 Die Betragsfunktion\nSchliesslich ergänzen wir die Datei mathsad.py noch mit der Funktion fabs wie oben beschrieben:\n\n\nCode\ndef fabs(x):\n    newValue = math.fabs(x.value)\n    newDerivative = x.derivative if x&gt;=0 else -x.derivative\n    return FloatSad(newValue, newDerivative)\n\n\n\nBeispiel 3.2 (Motivation für die Funktion fabs) Dieses Beispiel stammt aus Griewank und Walther (2008), S. 24. Die Funktion \\(f(x) = \\sqrt{x^6} = |x|^3\\) ist auch an der Stelle \\(x_0=0\\) differenzierbar, aber die Wurzelfunktion wie auch die Betragsfunktion sind es nicht. Die folgenden Programme scheitern an dieser Schwierigkeit.\n\n\nCode\nfrom floatsad import FloatSad\n\ndef f(x):\n    x = FloatSad(x)\n    v = x**6\n    y = v**0.5\n    return y\n\nx0 = 0\nprint(f(x0)) # math domain error\n\n\nDer rationale Exponent (Wurzel) führt zu einem Fehler.\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\n\ndef g(x):\n    x = FloatSad(x)\n    v = x**6\n    y = mathsad.sqrt(v)\n    return Y\n\nx0 = 0\nprint(g(x0)) # float division by zero\n\n\nDie Auswertung der Ableitung der Wurzelfunktion an der Stelle 0 erzeugt einen Fehler. Mit unserer Funktion fabs erhalten wir jedoch den korrekten Wert:\n\n\nCode\nfrom floatsad import FloatSad\nimport mathsad\n\ndef h(x):\n    x = FloatSad(x)\n    v = mathsad.fabs(x)\n    y = v**3\n    return y\n\nx0 = 0\nprint(h(x0)) # funktioniert\n\n\n&lt; 0.0 ; 0.0 &gt;\n\n\n\n\nDas fertige Modul kann auch von hier kopiert werden."
  },
  {
    "objectID": "intro.html#ein-erstes-beispiel",
    "href": "intro.html#ein-erstes-beispiel",
    "title": "1  Ableitungen und ihre Anwendungen",
    "section": "1.1 Ein erstes Beispiel",
    "text": "1.1 Ein erstes Beispiel\nIn allen Lehrbüchern über Analysis werden Extremwertaufgaben oder Optimierungsprobleme als zentrale Anwendung von Ableitungen eingeführt. Das folgende Beispiel etwa stammt aus Stocker u. a. (2022) (S. 93):\n\nVon einer Erdölraffinerie \\(R\\), die an einer von West nach Ost geradlinig verlaufenden Küste liegt, soll eine Pipeline zum Verteilzentrum \\(V\\) im Landesinnern gebaut werden. \\(V\\) liegt 16 km östlich und 12 km nördlich von \\(R\\). Von \\(R\\) aus soll die Pipeline zuerst ostwärtzs entlang der Küste geführt werden, ab einer geeigneten Stelle dann geradlinig ins Landesinnere nach \\(V\\). Mit welchen minimalen Baukosten ist zu rechnen, wenn die Verlegungskosten entlang der Küste 15’000 Euro je Kilometer betragen und im Landesinneren 25’000 Euro?\n\nÜblicherweise wird eine solche Aufgabe gelöst, indem zuerst eine Zielfunktion aufgestellt wird, in unserem Fall sind das die Gesamtkosten \\(k = k_\\textrm{Küste} + k_\\textrm{Land}\\), die sich aus den Baukosten für den Abschnitt entlang der Küste und den Kosten für die Strecke durch das Landesinnere zusammensetzen. Als nächstes formuliert man Nebenbedingungen, die die beiden Grössen mit einer geeignet gewählten Variablen in Verbindung setzen. Wir wählen \\(x\\) als die Strecke, die von \\(R\\) aus entlang der Küste bis zu dem Punkt führt, an dem die Pipeline abgezweigt wird. Dann gilt \\(k_\\textrm{Küste} = x\\cdot 15000\\) und mit Pythagoras finden wir \\(k_\\textrm{Land} = \\sqrt{(16-x)^2 + 12^2}\\cdot 25000\\). Setzen wir dies in die Hauptbedingung ein, so erhalten wir die Zielfunktion \\[\nk = k(x) = x\\cdot 15000 + \\sqrt{(16-x)^2 + 12^2}\\cdot 25000\n\\] von der wir das (globale) Minimum suchen. Dazu müssen wir die Funktion ableiten und die Gleichung \\(\\frac{dk}{dx}=0\\) nach \\(x\\) auflösen.\nAber könnten wir die Aufgabe nicht auch mit Hilfe des Computers lösen?\n\nÜbungsaufgabe 1.1 (Kostenfunktion als Programm) \nSchreibe eine Python Funktion kosten(x), welche zu einem \\(x\\in[0,16]\\) die gesamten Baukosten \\(k\\) berechnet, ohne die obige Lösung zu verwenden. Achte auf sinnvolle Variablennamen. Das Programm soll die Baukosten für einen sinnvollen Wert von \\(x\\) berechnen und ausgeben.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nEine mögliche Implementierung könnte so aussehen:\n\n\nCode\nimport math\n\ndef kosten(x):\n    # Preise pro Kilometer\n    pKueste = 15000 \n    pLand   = 25000\n\n    # Distanzen\n    sX = x  # Ost-West (x Richtung)\n    d = 16  - sX\n    sY = 12 # Nord-Sued (y Richtung)\n    sLand = d**2 + sY**2\n    sLand = math.sqrt(sLand)\n\n    #Kosten\n    kKueste = sX * pKueste\n    kLand = sLand * pLand\n    return kKueste + kLand\n\nx0 = 8\nkGesamt = kosten(x0)\nprint(\"Mit x =\", x0, \"betragen die Kosten\", kGesamt, \"Euro.\")\n\n\nMit x = 8 betragen die Kosten 480555.1275463989 Euro.\n\n\n\n\n\nDie Python Funktion kosten(x) liefert die gleichen Werte, wie die Funktion \\(k(x)\\), die wir oben hergeleitet haben, aber sie ist leichter zu verstehen, da wir eine Schritt für Schritt Anleitung haben, wie die Kosten berechnet werden, wohingegen in der mathematischen Funktionsgleichung alle diese Schritte zu einer Zeile zusammengefasst wurden. Trotzdem haben wir nicht viel gewonnen, wenn wir nicht die Ableitung der Funktion berechnen können. Und genau darum soll es in dieser Lerneinheit gehen.\n\n1.1.1 Unser Ziel: Programme ableiten\nWir möchten Ableitungen von Funktionen berechnen, die durch Programme beschrieben werden, die wie oben einen numerischen Parameter x als Input erhalten und einen numerischen Wert y zurückliefern. Unser Ziel wird es sein, die Programme so zu modifizieren, dass der Funktionsaufruf f(x0) nicht nur den Funktionswert \\(f(x_0)\\) zurückgibt, sondern auch den Wert der Ableitung \\(f'(x_0)\\). Wir sind dabei nicht an einer symbolischen Ableitung interessiert, wie das z.B. GeoGebra oder Mathematica machen (s. Kapitel 2.2), sondern nur an einer punktweisen Auswertung. Natürlich wollen wir die Ableitungsfunktion auch nicht von Hand bestimmen. Wir wollen uns aber auch nicht bloss mit einer Annhäerung des Wertes der Ableitung zufrieden geben (s. Kapitel 2.1), sondern den Wert von \\(f'(x_0)\\) bis auf Maschinengenauigkeit exakt berechnen. In Kapitel 3 werden wir eine Methode kennen lernen, die all dies leistet und dabei die Laufzeit eines Programms nicht wesentlich erhöht. Der Name dieser Methode: Algorithmische Differentiation (AD), obwohl die Namensgebung hier nicht eindeutig ist:\n\nOne of the obstacles in this area [of computing derivatives], which involves “symbolic” and “numerical” methods, has been a confusion in terminology […]. There is not even general agreement on the best name for the field, which is frequently referred to as automatic or computational differentiation in the literature. For this book the adjective algorithmic seemed preferable, because much of the material emphasizes algorithmic structure, sometimes glossing over the details and pitfalls of actual implementations. (Aus dem Vorwort zu Griewank und Walther (2008))\n\nZuerst wollen wir aber die wichtigsten Ableitungsregeln nochmal zusammenfassen."
  },
  {
    "objectID": "intro.html#numerische-verfahren-die-mit-ableitungen-arbeiten",
    "href": "intro.html#numerische-verfahren-die-mit-ableitungen-arbeiten",
    "title": "1  Ableitungen und ihre Anwendungen",
    "section": "1.4 Numerische Verfahren, die mit Ableitungen arbeiten",
    "text": "1.4 Numerische Verfahren, die mit Ableitungen arbeiten\nEs gibt zahlreiche numerische Verfahren, welche Werte von Ableitungen benötigen. Wir stellen hier exemplarisch zwei von ihnen vor: Das Newtonverfahren zur näherungsweisen Bestimmung von Nullstellen und das Gradient Descent Verfahren zur näherungsweisen Bestimmung von Minimalstellen einer Funktion. Letzteres wird uns an zahlreichen Stellen wieder begegnen.\n\n1.4.1 Das Newtonverfahren zur Berechnung von Nullstellen\nIn vielen Anwendungen steht man vor der Aufgabe, die Gleichung \\(f(x) = 0\\) nach \\(x\\) aufzulösen, d.h. eine Nullstelle \\(\\bar{x}\\) der Funktion zu finden. Oft ist es aber nicht möglich, die Lösung einer solchen Gleichung in geschlossener Form darzustellen. Um dennoch eine Lösung zumindest näherungsweise berechnen zu können, kann man folgendermassen vorgehen:\n\nWähle einen Startwert \\(x_0\\), der in der Nähe einer Nullstelle \\(\\bar{x}\\) von \\(f\\) liegt.\nIm Kurvenpunkt \\((x_0 | y_0)\\) wird die Tangente an die Kurve \\(f\\) gelegt. Deren Schnittpunkt \\(x_1\\) mit der \\(x\\)-Achse liegt in der Regel näher bei \\(\\bar{x}\\) als \\(x_0\\).\nNun wiederholt man das Verfahren, indem man bei \\(x_1\\) die Tangente an die Kurve legt, usw. Auf diese Weise erhält man eine Folge von Näherungen \\(x_0, x_1, x_2, \\ldots\\), deren Grenzwert die Nullstelle \\(\\bar{x}\\) ist.\n\nDieser Algorithmus ist als Newtonverfahren bekannt.\n\n\n\n\nDie Gleichung der Tangente im Punkt \\((x_n | y_n) = (x_n | f(x_n))\\) ist bekanntlich \\(t(x) = f(x_n) + f'(x_n) \\cdot (x - x_n)\\). Die Nullstelle der Tangente ist der Näherungswert \\(x_{n+1}\\). Aus \\(t(x_{n+1}) = 0\\) ergibt sich nun die Iterationsvorschrift des Newtonverfahrens: \\[\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n\\tag{1.1}\\]\n\nÜbungsaufgabe 1.6 (Das Newtonverfahren programmieren) \nSchreibe ein Programm, das mit Hilfe des Newtonverfahrens (Gleichung 1.1) eine Nullstelle der Funktion \\(f(x) = \\frac{1}{31} x^3 -\\frac{1}{20} x^2 -x + 1\\) berechnet. Verwende den Startwert \\(x_0 = -2\\). Du kannst abbrechen, wenn die Differenz \\(|x_{n+1} - x_n|\\) kleiner als eine bestimmte Toleranz wird, z.B. kleiner als tol = 1e-6. Wie flexibel ist dein Programm einsetzbar? Überlege dir z.B., wie viele Änderungen du vornehmen müsstest, wenn du die Nullstelle einer anderen Funktion berechnen müsstest.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nWelche der folgenden Lösungsvorschläge kommt deinem Programm am nächsten?\n\nVersion 1Version 2Version 3Version 4\n\n\n\nfrom math import fabs\n\nx0 = -2\ntol = 1e-6\n# Erster Schritt berechnen\nx1 = x0 - (1/31 * x0**3 - 1/20 * x0**2 - x0 + 1) / (3/31 * x0**2 - 1/10 * x0 - 1)\nwhile fabs(x1 - x0) &gt; tol:\n    x0 = x1\n    x1 = x0 - (1/31 * x0**3 - 1/20 * x0**2 - x0 + 1) / (3/31 * x0**2 - 1/10 * x0 - 1)\nprint(x1)\n\n5.908619865450271\n\n\nDas Newtonverfahren wird als main-Funktion (d.h. im Hauptprogramm) ausgeführt. Braucht man jedoch die Nullstelle einer anderen Funktion, dann muss ein neues Programm geschrieben werden. Die Ableitung wurde von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef f(x):\n    y = 1/31 * x**3 - 1/20 * x**2 - x + 1\n    return y\n\ndef fdot(x):\n    ydot = 3/31 * x**2 - 1/10 * x - 1\n    return ydot\n\nx0 = -2\ntol = 1e-6\n# Erster Schritt berechnen\nx1 = x0 - f(x0) / fdot(x0)\nwhile fabs(x1 - x0) &gt; tol:\n    x0 = x1\n    x1 = x0 - f(x0) / fdot(x0)\nprint(x1)\n\n5.908619865450271\n\n\nDas Newtonverfahren wird als main-Funktion (d.h. im Hauptprogramm) ausgeführt, aber die Berechnung von \\(f\\) und ihrer Ableitung \\(f'\\) wurde in zwei Funktionen f und fdot ausgelagert. Das macht das Programm übersichtlicher und flexibler. Die Ableitung wurde wieder von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef newton(f, fdot, x0):\n    tol = 1e-6\n    # Erster Schritt berechnen\n    x1 = x0 - f(x0) / fdot(x0)\n    while fabs(x1 - x0) &gt; tol:\n        x0 = x1\n        x1 = x0 - f(x0) / fdot(x0)\n    return x1\n\ndef f(x):\n    y = 1/31 * x**3 - 1/20 * x**2 - x + 1\n    return y\n\ndef fdot(x):\n    ydot = 3/31 * x**2 - 1/10 * x - 1\n    return ydot\n\nx0 = -2\nxbar = newton(f, fdot, x0)\nprint(xbar)\n\n5.908619865450271\n\n\n\n\n\nDas Newtonverfahren wird als eigene Funktion newton(f, fdot, x0) implementiert. Dieser werden die Funktion \\(f\\) und ihre Ableitung \\(f'\\), sowie der Startwert \\(x_0\\) als Argumente übergeben. Sie kann dann im Hauptprogramm aufgerufen werden. Die Ableitung wurde aber immer noch von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef newton(f, x0):\n    tol = 1e-6\n    # Erster Schritt berechnen\n    # Ableitung von f an der Stelle x0 annähern\n    h = 1e-6\n    ydot = ( f(x0 + h) - f(x0) ) / h\n    x1 = x0 - f(x0) / ydot\n    while fabs(x1 - x0) &gt; tol:\n        x0 = x1\n        ydot = ( f(x0 + h) - f(x0) ) / h\n        x1 = x0 - f(x0) / ydot\n    return x1\n\ndef f(x):\n    y = 1/31 * x**3 - 1/20 * x**2 - x + 1\n    return y\n\nx0 = -2\nxbar = newton(f, x0)\nprint(xbar)\n\n5.90861986545027\n\n\nHier wird das Newtonverfahren in einer Funktion implementiert. Die Ableitung wird nicht mehr von Hand berechnet, sondern innerhalb der Funktion mit \\(f'(x_0)\\approx \\frac{f(x_0 + h) - f(x_0)}{h}\\) angenähert. Dabei wird einfach h = 1e-6 gesetzt und gehofft, dass der entstehende Rundungsfehler klein genug ist. Beachte aber, dass sich der berechnete Wert von der Ausgabe in den anderen Versionen leicht unterscheidet.\n\n\n\n\n\n\nAuch die Version 4 der vorgestellten Lösung ist noch nicht befriedigend. Als wir die Ableitung von Hand berechnet hatten, musste nur die Funktion fdot and der Stelle x0 ausgewertet werden, um den (bis auf Maschinengenauigkeit) exakten Wert von \\(f'(x_0)\\) zu erhalten. Bei der letzten Methode muss man sich mit einem Näherungswert der Ableitung zufrieden geben. Auch wenn der Wert in diesem Beispiel gut genug war 1, so haben wir doch keine Garantie, dass wir für alle Funktionen einen vernünftigen Wert erhalten. Auf die Probleme, die mit dieser Annäherung von \\(f'(x_0)\\) auftreten, wird in Kapitel 2.1 näher eingegangen.1 Das Newton-Verfahren hat die angenehme Eigenschaft, dass kleine Rundungsfehler automatisch ausgeglichen werden. Auf andere numerische Verfahren, die die Ableitung verwenden, trifft dies aber nicht zu.\n\nBeispiel 1.2 (Billard auf einem runden Tisch) \nWir betrachten ein Beispiel aus Gander (2015). Platziere die weisse und die blaue Billardkugel auf dem runden Tisch. Das Ziel ist es, die weisse Kugel so anzustossen, dass sie die blaue Kugel trifft, nachdem sie vorher genau einmal an die Bande gespielt wurde.\n\n\n\n\nAus Symmetriegründen dürfen wir annehmen, dass der Rand des Billardtisches der Einheitskreis ist und dass die weisse Kugel auf der \\(x\\)-Achse liegt. Die blaue Kugel habe die Koordinaten \\((x_P|y_P)\\). Weiter sei \\(X\\) der Punkt auf dem Einheitskreis, an dem die weisse Kugel abprallt. Wir beschreiben diesen Punkt mit seinen Polarkoordinaten \\(X=(\\cos(x)|\\sin(x))\\). Unser Ziel ist es, \\(x\\) so zu berechnen, dass die weisse Kugel die blaue trifft, nachdem sie bei \\(X\\) an die Bande gestossen ist. Dabei verhält sie sich so, als ob sie an der Kreistangente in \\(X\\) reflektiert wird. Der Tangentenvektor im Punkt \\(X\\) lautet \\(\\vec{t} = \\begin{pmatrix} -\\sin(x) \\\\ \\cos(x) \\end{pmatrix}\\).\n\n\n\n\n\nBillard auf einem runden Tisch\n\n\nWir betrachten nun die Einheitsvektoren \\(\\vec{e}_Q\\) in Richtung \\(\\overrightarrow{XQ}\\) und \\(\\vec{e}_P\\) in Richtung \\(\\overrightarrow{XP}\\). Wenn die weisse Kugel die blaue treffen soll, dann müssen die Winkel zwischen der Tangente und diesen Vektoren gleich sein. Das ist genau dann der Fall, wenn \\(\\vec{t}\\) senkrecht steht auf \\(\\vec{e}_Q + \\vec{e}_P\\). Wir müssen also \\(x\\) so bestimmen, dass \\(\\vec{t} \\cdot (\\vec{e}_Q + \\vec{e}_P) = 0\\) ist.\nDas folgende Programm berechnet das Skalarprodukt der linken Seite dieser Gleichung.\n\n\nCode\nimport math\nimport matplotlib.pyplot as plt\n\ndef f(x):\n    # Parameter\n    a = -0.8           # Position von Q = (a|0)\n    px, py = 0.5, 0.5  # Position von P = (px|py)\n\n    # Berechnung des Skalarprodukts\n    v0 = x\n    v1 = math.cos(v0)  # x-Koordinate von X\n    v2 = math.sin(v0)  # y-Koordinate von X\n    v3 = px - v1       # x-Komponente des Vektors XP\n    v4 = py - v2       # y-Komponente des Vektors XP\n    v5 = math.sqrt(v3**2 + v4**2)  # Länge des Vektors XP\n    v6 = v3 / v5       # x-Komponente des Einheitsvektors eP\n    v7 = v4 / v5       # y-Komponente des Einheitsvektors eP\n    \n    v8 = a - v1        # x-Komponente des Vektors XQ\n    v9 = -v2           # y-Komponente des Vektors XQ\n    v10 = math.sqrt(v8**2 + v9**2)  # Länge des Vektors XQ\n    v11 = v8 / v10     # x-Komponente des Vektors eQ    \n    v12 = v9 / v10     # y-Komponente des Vektors eQ   \n    y = (v6 + v11) * v2 - (v7 + v12) * v1  # Skalarprodukt\n    return y   \n\n# Graph der Funktion f(x) plotten\nfig = plt.figure()\nax = plt.gca()\nax.set_xlim((0,2*math.pi))\nax.set_ylim((-1.5,1.5))\nX = [2*math.pi * k / 1000 for k in range(1001)]\nY = [f(x) for x in X]\nplt.plot([0, 2*math.pi], [0, 0], 'k--') # x-Achse\nplt.plot(X,Y)\nplt.xticks([0, math.pi/2, math.pi, 3*math.pi/2, 2*math.pi],\n           ['0', 'π/2', 'π', '3π/2', '2π'])\nplt.show()  \n\n\n\n\n\nAbbildung 1.3: Graph des Skalarprodukts als Funktion des Polarwinkels \\(x\\) des Punktes \\(X = (cos(x) | sin(x))\\). Die Nullstellen entsprechen den Winkeln, bei denen die weisse Kugel die blaue Kugel trifft, nachdem sie genau einmal an die Bande gespielt wurde.\n\n\n\n\nWir möchten die Nullstellen der Funktion f(x) mit unserer Funtion newton bestimmmen. Dazu müssen wir jedoch die Ableitung von f berechnen.\n\n\n\n\n1.4.2 Gradient Descent zum Auffinden lokaler Minima\nEine weitere wichtige Aufgabe besteht darin, ein Minimum einer Funktion zu finden. Auch hier wollen wir mit Hilfe der Ableitung eine Folge von Näherungswerten \\(x_0, x_1, x_2, \\ldots\\) finden, deren Grenzwert die \\(x\\)-Koordinate eines (lokalen) Minimums von \\(f\\) ist.\nWenn \\(f'(x_n)&gt;0\\) ist, dann wissen wir, dass die Funktion \\(f\\) an der Stelle \\(x_0\\) streng monoton wachsend ist. D.h., dass die Funktionswerte links von \\(x_n\\) kleiner sind, als an der Stelle \\(x_n\\). Analog gilt, dass wenn \\(f'(x_n)&lt;0\\) ist, die Funktion monoton fallend ist und wir uns nach rechts bewegen sollten, um ein Minimum zu finden. In der Nähe eines Minimums ist ausserdem \\(|f'(x)|\\) sehr klein und wir können entsprechend kleinere Schritte machen, um uns diesem anzunähern. Um also von \\(x_n\\) zu \\(x_{n+1}\\) zu kommen, machen wir einen Schritt, der proportional zu \\(-f'(x_n)\\) ist. Mit dem Proporionalitätsfaktor \\(\\lambda\\in\\mathbb{R}\\) und einem geeignet gewählten Startwert \\(x_0\\) erhalten wir die Iterationsvorschrift \\[\nx_{n+1} = x_n - \\lambda\\cdot f'(x_n)\n\\tag{1.2}\\]\n\n\n\n\n\nÜbungsaufgabe 1.7 (Eigenschaften der Gradient Descent Methode)  Experimentiere mit verschiedenen Funktionen und verschiedenen Schrittweiten \\(\\lambda\\). Was passiert, wenn die Schrittweite zu klein bzw. zu gross gewählt wird? Was passiert, wenn \\(f\\) an der Stelle \\(x_0\\) ein lokales Maximum aufweist? Was passiert in der Nähe eines Sattelpunktes?\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nIst \\(\\lambda\\) zu klein, dann konvergiert das Verfahren nur sehr langsam. Ist \\(\\lambda\\) dagegen zu gross, dann kann es passieren, dass die Iteration zwischen zwei oder mehr Werten hin- und herspringt oder sogar nach \\(\\pm\\infty\\) divergiert.\nFalls \\(x_0\\) gerade mit der Stelle eines lokalen Maximums oder eines Sattelpunktes zusammenfällt, gilt auch \\(f'(x_0)=0\\) und damit auch \\(x_n = x_0\\) für alle \\(n\\in\\mathbb{N}\\). Maxima sind aber labile Gleichgewichtspunkte in dem Sinn, dass sich \\(x_n\\) von ihnen wegbewegt, wenn \\(x_0\\) auch nur ein bisschen links oder rechts davon liegt. Ähnlich verhält es sich bei Sattelpunkten. Die Folge konvergiert gegen die Stelle des Sattelpunktes, wenn \\(f(x_0)\\) grösser als der \\(y\\)-Wert des Sattelpunktes ist und \\(\\lambda\\) nicht zu gross ist.\n\n\n\n\nÜbungsaufgabe 1.8 (Gradient Descent programmieren) \nSchreibe ein Programm, das mit Hilfe des Gradient Descent Verfahrens (Gleichung 1.2) ein lokales Minimums der Funktion \\(f(x) = \\frac{1}{16}x^4 - \\frac{1}{3}x^3 + \\frac{1}{8}x^2 + x + 2\\) berechnet. Verwende den Startwert \\(x_0 = 1.5\\) und die Schrittweite \\(\\lambda = 0.5\\). Du kannst abbrechen, wenn die Differenz \\(|x_{n+1} - x_n|\\) kleiner als eine bestimmte Toleranz wird, z.B. kleiner als tol = 1e-6. Wie flexibel ist dein Programm einsetzbar? Überlege dir z.B., wie viele Änderungen du vornehmen müsstest, wenn du ein lokales Minimum einer anderen Funktion berechnen müsstest.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nWelche der folgenden Lösungsvorschläge kommt deinem Programm am nächsten?\n\nVersion 1Version 2Version 3Version 4\n\n\n\nfrom math import fabs\n\nx0 = 1.5\nlam = 0.5\ntol = 1e-6\n# Erster Schritt berechnen\nx1 = x0 - lam * (1/4 * x0**3 - x0**2 + 1/4 * x0 + 1)\nwhile fabs(x1-x0) &gt; tol:\n    x0 = x1\n    x1 = x0 - lam * (1/4 * x0**3 - x0**2 + 1/4 * x0 + 1)\nprint(x1)\n\n3.3429230748530196\n\n\nDas Gradient Descent Verfahren wird als main-Funktion (d.h. im Hauptprogramm) ausgeführt. Um das Minimum einer anderen Funktion zu bestimmen, muss ein neues Programm geschrieben werden. Die Ableitung wurde von Hand berechnet\n\n\n\nfrom math import fabs\n\ndef fdot(x):\n    ydot = 1/4 * x**3 - x**2 + 1/4 * x + 1\n    return ydot\n\nx0 = 1.5\nlam = 0.5\ntol = 1e-6\n# Erster Schritt berechnen\nx1 = x0 - lam * fdot(x0)\nwhile fabs(x1-x0) &gt; tol:\n    x0 = x1\n    x1 = x0 - lam * fdot(x0)\nprint(x1)\n\n3.3429230748530196\n\n\nDas Gradient Descent Verfahren wird als main-Funktion (d.h. im Hauptprogramm) ausgeführt, aber die Berechnung von \\(f'\\) wurde in die Funktion fdot(x) ausgelagert. Das macht das Programm etwas flexibler. Die Ableitung wurde wieder von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef gradient_descent(fdot, x0, lam):\n    tol = 1e-6\n    # Erster Schritt berechnen\n    x1 = x0 - lam * fdot(x0)\n    while fabs(x1-x0) &gt; tol:\n        x0 = x1\n        x1 = x0 - lam * fdot(x0)\n    return x1\n\ndef fdot(x):\n    ydot = 1/4 * x**3 - x**2 + 1/4 * x + 1\n    return ydot\n\nx0 = 1.5\nlam = 0.5\nxmin = gradient_descent(fdot, x0, lam)\nprint(xmin)\n\n3.3429230748530196\n\n\nDas Gradient Descent Verfahren wird als eigene Funktion gradient_descent(fdot, x0, lam) implementiert. Dieser Funktion werden die Ableitung \\(f'\\), der Startwert \\(x_0\\), sowie die Schrittweite \\(\\lambda\\) als Argumente übergeben. Sie kann dann im Hauptprogramm aufgerufen werden. Die Ableitung wurde aber immer noch von Hand berechnet.\n\n\n\nfrom math import fabs\n\ndef gradient_descent(f, x0, lam):\n    tol = 1e-6\n    # Erster Schritt berechnen\n    # Ableitung an der Stelle x0 annähern\n    h = 1e-6\n    ydot = ( f(x0 + h) - f(x0) ) / h\n    x1 = x0 - lam * ydot\n    while fabs(x1-x0) &gt; tol:\n        x0 = x1\n        ydot = ( f(x0 + h) - f(x0) ) / h\n        x1 = x0 - lam * ydot\n    return x1\n\ndef f(x):\n    y = 1/16 * x**4 - 1/3 * x**3 + 1/8 * x**2 + x + 2\n    return y\n\nx0 = 1.5\nlam = 0.5\nxmin = gradient_descent(fdot, x0, lam)\nprint(xmin)\n\n2.535183236464121\n\n\nDas Gradient Descent Verfahren wird als eigene Funktion gradient_descent(f, x0, lam) implementiert. Dieser Funktion werden die ursprüngliche Funktion \\(f\\), der Startwert \\(x_0\\), sowie die Schrittweite \\(\\lambda\\) als Argumente übergeben. Die Ableitung wird nicht mehr von Hand berechnet, sondern durch den Differenzenquotienten \\(f'(x_0) \\approx \\frac{f(x_0 + h) - f(x_0)}{h}\\) angenähert. Dabei wird einfach h = 1e-6 gesetzt und gehofft, dass der entstehende Rundungsfehler klein genug ist. Offensichtlich ist diese Annahme jedoch nicht gerechtfertigt.\n\n\n\n\n\n\nDie Übungsaufgabe 1.8 verdeutlicht nochmals das Problem, welches wir bereits in Übungsaufgabe 1.6 gesehen haben. Wir müssen für den Algorithmus die Ableitung \\(f'\\) an mehreren Stellen auswerten. Wir möchten aber die Ableitung einerseits nicht von Hand berechnen und andererseits können wir uns auch nicht mit einer Approximation zufrieden geben.\nWir beschliessen dieses Kapitel mit einer praktischen Anwendung der Gradient Descent Methode.\n\nBeispiel 1.3 (Minimaler Abstand) \nDie Punkte \\(P\\) und \\(Q\\) bewegen sich auf Ellipsen im Raum. Die Position des Punktes \\(P\\) zur Zeit \\(t\\) ist gegeben durch\n\\[\\begin{align*}\n    x_P(t) &= 2 \\cos(t) - 1 \\\\\n    y_P(t) &= 1.5 \\sin(t)   \\\\\n    z_P(t) &= 0             \n\\end{align*}\\]\nund die Position von \\(Q\\) zum Zeitpunkt \\(t\\) lässt sich durch\n\\[\\begin{align*}\n    x_Q(t) &= -3 \\sin(2t)     \\\\\n    y_Q(t) &= 2 \\cos(2t) + 1  \\\\\n    z_Q(t) &= 2 \\sin(2t) + 1  \n\\end{align*}\\]\nbestimmen.\n\n\n\n\nDer Abstand zwischen den beiden Punkten lässt sich zu jedem Zeitpunkt \\(t\\) berechnen durch \\(d = d(t) = |\\overrightarrow{PQ}|\\). Das folgende Programm berechnet diese Funktion und zeichnet ihren Graph.\n\n\nCode\nimport math\nimport matplotlib.pyplot as plt\n\ndef d(t):\n    v0 = t\n    v1 = 2 * math.cos(v0) - 1    # x-Koordinate von P\n    v2 = 1.5 * math.sin(v0)      # y-Koordinate von P\n    v3 = 0                       # z-Koordinate von P\n    v4 = -3 * math.sin(2*v0)     # x-Koordinate von Q\n    v5 = 2 * math.cos(2*v0) + 1  # y-Koordinate von Q\n    v6 = 2 * math.sin(2*v0) + 1  # z-Koordinate von Q\n    y = math.sqrt((v4-v1)**2 + (v5-v2)**2 + (v6-v3)**2)\n    return y\n\n# Graph der Funktion d(t) plotten\nfig = plt.figure()\nax = plt.gca()\nax.set_xlim((0,2*math.pi))\nax.set_ylim((0,6))\nT = [2*math.pi * k / 1000 for k in range(1001)]\nY = [d(t) for t in T]\nplt.plot(T,Y)\nplt.xticks([0, math.pi/2, math.pi, 3*math.pi/2, 2*math.pi],\n           ['0', 'π/2', 'π', '3π/2', '2π'])\nplt.show()  \n\n\n\n\n\nAbbildung 1.4: Graph der Abstandsfunktion \\(d(t)\\).\n\n\n\n\nWir möchten das Minimum der Funktion \\(d(t)\\) mit Hilfe der Gradient Descent Methode finden. Dazu müssen wir aber \\(d\\) ableiten können.\n\n\n\n\n\n\nArens, Tilo, Frank Hettlich, Christian Karpfinger, Ulrich Kockelkorn, Klaus Lichtenegger, und Hellmuth Stachel. 2022. Mathematik. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nGander, Walter. 2015. Learning MATLAB: A Problem Solving Approach. 1. Aufl. UNITEXT. Cham, Switzerland: Springer International Publishing.\n\n\nGriewank, Andreas, und Andrea Walther. 2008. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2. Aufl. Other Titles in Applied Mathematics 105. Philadelphia, PA: SIAM. http://bookstore.siam.org/ot105/.\n\n\nHromkovic, Juraj, Jarka Arnold, Cédric Donner, Urs Hauser, Matthias Hauswirth, Tobias Kohn, Dennis Komm, David Maletinsky, und Nicole Roth. 2021. INFORMATIK, Programmieren und Robotik: Grundlagen der Informatik für Schweizer Maturitätsschulen. Klett.\n\n\nSlater, Max. 2022. „Differentiable programming from scratch“. Juli 2022. https://thenumb.at/Autodiff/.\n\n\nStocker, Hansjürg, Reto Weibel, Marco Schmid, Regula Sourlier-Künzle, und Baoswan Wong Dzung. 2022. Analysis: Aufgaben. hep Verlag."
  }
]