{
  "hash": "8c6ed5120ef5253a9a226cdaca90ef05",
  "result": {
    "markdown": "---\ntbl-cap-location: bottom\n---\n\n# Adjungierte Algorithmische Differentiation {#sec-AAD}\n\nIn @sec-HigherDimFunctions haben wir gesehen, dass wir mit der Standard Algorithmischen Differentiation (SAD) alle $m$ Ableitungen einer Funktion $f : \\mathbb{R} \\rightarrow \\mathbb{R}^m$ mit einem einzigen Funktionsaufruf berechnen können. Die Berechnung des Gradienten einer Funktion $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ benötigt jedoch $n$ Funktionsaufrufe, nämlich einen für jede partielle Ableitung $\\partial f / \\partial x_i$. In diesem Kapitel wollen wir eine Methode entwickeln, die alle $n$ partiellen Ableitungen in einem Funktionsaufruf berechnet. \n\nÄhnlich wie die SAD beruht auch diese Methode darauf, dass wir eine komplizierte Funktion schrittweise mit Hilfe von elementaren Operationen berechnen und in jedem Schritt die Ableitungen in separaten Variablen akkumulieren. Wir führen also wieder unsere Konvention aus @sec-ProgFunc ein. Auch dieses Mal werden wir in jedem Schritt die Kettenregel verwenden. Diesmal fangen wir jedoch am Ende der Funktion an und werden uns dann rückwärts durch alle Ableitungen arbeiten. Aus diesem Grund wird das Verfahren auch Rückwärts-AD ^[Im Englischen spricht man von *reverse mode differentiation* weil *backward differentiation* für bestimmte Methoden zur Integration von Differentialgleichungen verwendet wird.] oder Adjungierte AD (AAD) genannt. \n\n\n## Manuelle Implementation der AAD\n\nWir erläutern die Methode zuerst an einem einfachen Beispiel, welches eine leicht abgeänderte Version des Beispiels von @sidsite2021 ist.\n\n:::{#exm-firstAADbyHand}\n\n## Gradient mit AAD\n<br>\n\nBetrachten wir die Funktion $f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$\n$$\ny = f(x_0, x_1) = (x_0 + x_1) \\cdot x_0 - x_1\n$$\n\nAls Programm können wir die Funktion unter Berücksichtigung der Konvention so schreiben (siehe auch @fig-compTreeMulti):\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"show\"}\ndef f(x0, x1):\n    v0 = x0\n    v1 = x1\n    v2 = v0 + v1\n    v3 = v2 * v0\n    v4 = v3 - v1\n    y = v4\n    return y\n\nx0, x1 = 2, 3\ny0 = f(x0, x1)\nprint(\"f(\" + str(x0) + \",\" + str(x1) + \") = \" + str(y0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nf(2,3) = 7\n```\n:::\n:::\n\n\n```{dot}\n//| label: fig-compTreeMulti\n//| fig-cap: Computational Graph für `y = (x0 + x1) * x0 - x1`.\ndigraph {\n    rankdir = LR\n    fontname = \"Consolas\"\n    node [fontname = \"Cambria\", fontsize=8, width=\".2\", height=\".2\", margin=\".02\"];\n    edge [fontname = \"Cambria\", fontsize=8, arrowsize=0.25, len=minlen];\n    \n    nx0 [label = <x<SUB>0</SUB>>, shape  = none];\n    nx1 [label = <x<SUB>1</SUB>>, shape  = none];\n    nPlus [label = \"+\", shape = circle];\n    nTimes [label = \"*\", shape = circle];\n    nMinus [label = \"-\", shape = circle];\n    ny [label = \"y\", shape = none];\n\n    {rank = same; nx0 nx1}\n    {rank = same; nPlus}\n    {rank = same; nTimes}\n    {rank = same; nMinus}\n    {rank = same; ny}\n\n    nx0 -> nPlus;\n    nx1 -> nPlus;\n    nx0 -> nTimes;\n    nPlus -> nTimes;\n    nx1 -> nMinus;\n    nTimes -> nMinus;\n    nMinus -> ny\n}\n```\n\n\n\n\nDie partiellen Ableitungen von $f$ lauten\n$$\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_0} &= \\frac{\\partial f}{\\partial x_0} = (1+0)\\cdot x_0 + (x_0 + x_1)\\cdot 1 - 0=2x_0 + x_1 \\\\\n    \\frac{\\partial y}{\\partial x_1} &= \\frac{\\partial f}{\\partial x_1} = (0 + 1)\\cdot x_0 - 1 = x_0 - 1\n\\end{align*}\n$$\nwobei für die Ableitung nach $x_0$ die Produktregel verwendet wurde. Die partielle Ableitung $\\partial y / \\partial x_0$ können wir auch berechnen, indem wir bei $y = v_4$ anfangen und jeweils die Definition der Hilfsvariablen einsetzen:\n$$\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_0} &= \\frac{\\partial v_4}{\\partial x_0} \\\\\n    &= \\frac{\\partial (v_3 - v_1)}{\\partial x_0} \\\\\n    &= \\frac{\\partial v_3}{\\partial x_0} - \\frac{\\partial v_1}{\\partial x_0} \\\\\n    &= \\frac{\\partial (v_2 \\cdot v_0)}{\\partial x_0} - \\frac{\\partial x_1}{\\partial x_0} \\\\\n    &= \\frac{\\partial v_2}{\\partial x_0} \\cdot v_0 + v_2 \\cdot \\frac{\\partial v_0}{\\partial x_0} - 0 \\\\\n    &= \\frac{\\partial (v_0 + v_1)}{\\partial x_0} \\cdot v_0 + v_2 \\cdot \\frac{\\partial x_0}{\\partial x_0} \\\\\n    &= \\left( \\frac{\\partial v_0}{\\partial x_0} + \\frac{\\partial v_1}{\\partial x_0} \\right) \\cdot x_0 + (v_0 + v_1) \\cdot 1 \\\\\n    &= \\left( \\frac{\\partial x_0}{\\partial x_0} + \\frac{\\partial x_1}{\\partial x_0} \\right) \\cdot x_0 + (x_0 + x_1) \\\\\n    &= (1 + 0) \\cdot x_0 + (x_0 + x_1) \\\\\n    &= 2x_0 + x_1   \n\\end{align*}\n$$\n\nAnalog findet man $\\partial y / \\partial x_1$ (diesmal lassen wir einige der offensichtlicheren Zwischenschritte weg):\n$$\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_1} &= \\frac{\\partial v_4}{\\partial x_1} \\\\\n    &= \\frac{\\partial v_3}{\\partial x_1} - \\frac{\\partial v_1}{\\partial x_1} \\\\\n    &= \\frac{\\partial (v_2 \\cdot v_0)}{\\partial x_1} - 1 \\\\\n    &= \\frac{\\partial v_2}{\\partial x_1} \\cdot v_0 + v_2 \\cdot \\frac{\\partial v_0}{\\partial x_1} - 1 \\\\\n    &= \\frac{\\partial (v_0 + v_1)}{\\partial x_1} \\cdot x_0 + v_2 \\cdot 0 - 1 \\\\\n    &= \\left( \\frac{\\partial v_0}{\\partial x_1} + \\frac{\\partial v_1}{\\partial x_1} \\right) \\cdot x_0 - 1 \\\\\n    &= ( 0 + 1) \\cdot x_0 - 1 \\\\\n    &= x_0 - 1   \n\\end{align*}\n$$\n\nUm die beiden Rechnungen zusammenzufassen, führen wir nun für jede Hilfsvariable $v_i$ eine neue Variable $\\bar v_i$ ein, welche definiert ist als\n$$\n\\bar v_i = \\frac{\\partial y}{\\partial v_i}\n$$\nÄhnlich wie die $\\dot v_i$ aus der SAD speichern diese Variablen die Werte der Ableitungen. Die neue Notation soll anzeigen, dass es sich um die AAD Methode handelt. Unser Ziel ist es also, $\\bar v_0 = \\partial y / \\partial v_0 = \\partial y / \\partial x_0$ und $\\bar v_1 = \\partial y / \\partial v_1 = \\partial y / \\partial x_1$ zu bestimmen. Beginnen wir in der letzten Zeile des Programms, dann gilt offenbar immer $\\bar v_4 = \\partial y / \\partial v_4 = 1$.  In der Zeile darüber können wir $\\bar v_3$ und $\\bar v_1$ berechnen, indem wir die Kettenregel verwenden.\n\n\\begin{align*}\n    \\bar v_3 &= \\frac{\\partial y}{\\partial v_3} = \\frac{\\partial y}{\\partial v_4} \\cdot \\frac{\\partial v_4}{\\partial v_3} = \\bar v_4 \\cdot (1-0)=\\bar v_4  \\\\\n\n    \\bar v_1 &= \\frac{\\partial y}{\\partial v_1} = \\frac{\\partial y}{\\partial v_4} \\cdot \\frac{\\partial v_4}{\\partial v_1} = \\bar v_4 \\cdot (0-1)= -\\bar v_4\n\\end{align*}\n\n\nAlso sind $\\bar v_3 = 1$ und $\\bar v_1 = -1$. Der Zwischenwert in $\\bar v_1$ wird später ergänzt werden. \nAus der Zeile $v_3 = v_2 \\cdot v_0$ lassen sich als nächstes Ausdrücke für $\\bar v_2$ und $\\bar v_0$ finden.\n\n\\begin{align*}\n    \\bar v_2 &= \\frac{\\partial y}{\\partial v_2} = \\frac{\\partial y}{\\partial v_3} \\cdot \\frac{\\partial v_3}{\\partial v_2} = \\bar v_3 \\cdot v_0 \\\\\n\n    \\bar v_0 &= \\frac{\\partial y}{\\partial v_0} = \\frac{\\partial y}{\\partial v_3} \\cdot \\frac{\\partial v_3}{\\partial v_0} = \\bar v_3 \\cdot v_2 \n\\end{align*}\n\nMit den vorher berechneten Werten erhalten wir also $\\bar v_2 = v_0$ und $\\bar v_0 = v_2$. Beide Werte sind durch die Funktion bereits berechnet worden. Im obigen Beispiel gilt etwa `v0 = x0 = 2` und `v2 = x0 + x1 = 5`. Auch diese Zwischenwerte werden im nächsten Schritt ergänzt. Aus der Zeile $v_2 = v_0 + v_1$ ergibt sich nämlich\n\n\\begin{align*}\n    \\bar v_0 &= \\bar v_0 + \\frac{\\partial y}{\\partial v_0} = \\bar v_0 + \\frac{\\partial y}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial v_0} \\\\\n    &= \\bar v_0 + \\bar v_2 \\cdot (1+0) = \\bar v_0 + \\bar v_2 \\\\ & \\\\\n\n    \\bar v_1 &= \\bar v_1 + \\frac{\\partial y}{\\partial v_1} = \\bar v_1 + \\frac{\\partial y}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial v_1} \\\\\n    &= \\bar v_1 + \\bar v_2 \\cdot (0+1) = \\bar v_1 + \\bar v_2\n\\end{align*}\n\nMit den bereits bekannten Werten erhalten wir $\\bar v_0 = v_2 + v_0$ (bzw. mit den konkreten Werten des Beispiels `v0bar = 5 + 2`) und $\\bar v_1 = -1 + v_0$ (bzw. `v1bar = -1 + 2`).\nNun enthalten die Variablen $\\bar v_0$ und $\\bar v_1$ die Werte der gewünschten Ableitungen, nämlich $\\bar v_0 = (v_0 + v_1) + v_0 = 2x_0 + x_1$ und $\\bar v_1 = -1 + x_0$. Wir können aber die letzten Schritte analog zu den vorherigen ausführen:\n\n\\begin{align*}\n    \\bar x_1 &= \\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial x_1} = \\bar v_1 \\cdot 1  \\\\\n\n    \\bar x_0 &= \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial y}{\\partial v_0} \\cdot \\frac{\\partial v_0}{\\partial x_0} = \\bar v_0 \\cdot 1  \\\\\n\\end{align*}\n\n\nDie Schwierigkeit besteht darin, dass wir nicht wie bei der SAD in jedem Schritt die Variable $v_i$ und gleichzeitig die Variable $\\dot v_i$ berechnen können. Um die $\\bar v_i$ zu bestimmen muss man zuerst die Funktion komplett ausführen, und sich dabei den Aufbau des Computational Graph merken. Erst dann kann man rückwärts die Ableitungswerte berechnen, angefangen bei der letzten Hilfsvariablen $\\bar v_4 = 1$. Das folgende Schema fasst die obigen Rechnungen zusammen.\n\n\\begin{equation*}\n\\left \\downarrow \n    \\begin{aligned}[c] \n        v_0 &= x_0 \\\\ \n        v_1 &= x_1 \\\\\n        v_2 &= v_0 + v_1 \\\\\n        v_3 &= v_2 \\cdot v_0 \\\\\n        v_4 &= v_3 - v_1 \\\\\n        y &= v_4\n    \\end{aligned}  \n\\right .\n\n\\qquad\n\n\\begin{aligned}[c] \n    & \\\\ \n    & \\\\\n    & \\\\\n    & \\\\\n    & \\\\\n    &\\longrightarrow\n\\end{aligned}  \n\n\\qquad\n\n\\left \\uparrow \n    \\begin{aligned}[c] \n        \\bar x_0 &= \\bar v_0 = 2\\cdot x_0 + x_1 \\\\ \n        \\bar x_1 &= \\bar v_1 = -1 + x_0 \\\\\n        \\bar v_0 &= \\bar v_0 + \\bar v_2, \\quad \\bar v_1 = \\bar v_1 + \\bar v_2 \\\\\n        \\bar v_2 &= \\bar v_3 \\cdot v_0, \\quad \\bar v_0 = \\bar v_3 \\cdot v_2 \\\\\n        \\bar v_3 &= \\bar v_4, \\quad \\bar v_1 = -\\bar v_4 \\\\\n        \\bar v_4 &= \\bar y = 1\n    \\end{aligned}  \n\\right . \n\\end{equation*}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\ndef f(x0, x1):\n    v0 = x0\n    v1 = x1\n    v2 = v0 + v1\n    v3 = v2 * v0\n    v4 = v3 - v1\n    y = v4\n    v4bar = 1\n    v3bar = v4bar\n    v1bar = -v4bar\n    v2bar = v3bar * v0\n    v0bar = v3bar * v2\n    v0bar = v0bar + v2bar\n    v1bar = v1bar + v2bar\n    grad = [v0bar, v1bar]\n    return [y, grad]\n\nx0, x1 = 2, 3\n[y0, dy] = f(x0, x1)\nprint(\"Funktionswert: \" + str(y0))\nprint(\"Gradient: \" + str(dy))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFunktionswert: 7\nGradient: [7, 1]\n```\n:::\n:::\n\n\nDie Werte der Ableitungen $\\bar v_i$ lassen sich auch im Computational Graph verfolgen, siehe @fig-compTreeMultiReversed. Der Wert bei der Kante von $v_i$ nach $v_j$ entspricht der partiellen Ableitung $\\partial v_i / \\partial v_j$. Entlang eines Weges werden die Werte multipliziert. Führen mehrere Wege zu einer Variablen $x_i$, so werden die Werte der einzelnen Wege addiert.\n\n\n\n\n```{dot}\n//| label: fig-compTreeMultiReversed\n//| fig-cap: Werte der Hilfsvariablen `vbar`.\ndigraph {\n    rankdir = LR\n    fontname = \"Consolas\"\n    node [fontname = \"Cambria\", fontsize=8, width=\".2\", height=\".2\", margin=\".02\"];\n    edge [fontname = \"Cambria\", fontsize=8, arrowsize=0.25, len=minlen];\n    \n    nx0 [label = <x<SUB>0</SUB>>, shape  = none];\n    nx1 [label = <x<SUB>1</SUB>>, shape  = none];\n    nPlus [label = \"+\", shape = circle];\n    nTimes [label = \"*\", shape = circle];\n    nMinus [label = \"-\", shape = circle];\n    ny [label = \"y\", shape = none];\n    \n    {rank = same; nx0 nx1}\n    {rank = same; nPlus}\n    {rank = same; nTimes}\n    {rank = same; nMinus}\n    {rank = same; ny}\n    \n    nx0 -> nPlus [dir = back, color = red, fontcolor = red, xlabel = 1];\n    nx1 -> nPlus [dir = back, color = red, fontcolor = red, xlabel = 1];\n    nx0 -> nTimes [dir = back, color = red, fontcolor = red, xlabel = <v<SUB>2</SUB>>];\n    nPlus -> nTimes [dir = back, color = red, fontcolor = red, xlabel = <v<SUB>0</SUB>>];\n    nx1 -> nMinus [dir = back, color = red, fontcolor = red, xlabel = -1];\n    nTimes -> nMinus [dir = back, color = red, fontcolor = red, xlabel = 1];\n    nMinus -> ny [dir = back, color = red, fontcolor = red, xlabel = 1];\n}\n```\n\n\n\n\n:::\n\n---\n\n:::{#exr-EigeneAADBeispiele1}\n\n## Eigene Beispiele finden\n<br>\n\nSchreibe eine eigene Funktion $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ für $n\\in\\lbrace 2, 3 \\rbrace$ hin und erstelle den Computational Graph und ein Programm. Leite das Programm nach der oben beschriebenen AAD Methode ab und überzeuge dich an verschiedenen Stellen davon, dass der Gradient korrekt ist.\n\n:::\n\n:::{.callout-tip collapse=\"true\"}\n\n## Lösung\nBeispiele findet man in der Literatur, z.B. bei @sidsite2021, @Slater2022, @Baydin18 (S. 13), @Griewank2008EDP (S. 9, S. 42) oder @Henrard2017ADi (S. 24).\n\n:::\n\n\n\n\n\n## Implementation der AAD mit Operator Overloading {#sec-AADmitOperatorOverloading}\n\nNun wollen wir ähnlich wie im @sec-SadImplementationOperatorOverloading eine Klasse `FloatAad` entwerfen, welche die Berechnung aller Hilfsvariablen `vbar` automatisch ausführt. Wie auch zuvor hat jedes `FloatAad`-Objekt ein Attribut `value` vom Typ `Int` oder `Float`. Allerdings reicht es nicht mehr aus, ein `Float`-Attribut `derivative` zu definieren, um den Wert der Ableitung zu speichern weil auch die Struktur des Computational Graph gespeichert werden muss. Als Attribut `derivatives` wählen wir ein `tuple`, dessen erster Eintrag ein `FloatAad`-Objekt ist, nämlich die Variable, nach der die partielle Ableitung berechnet wird. Der zweite Eintrag ist der Wert dieser partiellen Ableitung. Da das erste Element des Tupels selber auch ein Attribut `derivatives` hat, entsteht so eine rekursive Darstellung des Computational Graph. \n\nDie folgende Implementation lehnt sich stark an @sidsite2021 an. Die Variablennamen wurden angepasst, so dass sie konsistent mit den Bezeichnungen aus @sec-SADforOneDimFunctions sind. Ausserdem werden wir unsere Klasse noch mit einiger zusätzlicher Funktionalität ausstatten, etwa mit Typunterscheidungen, so dass wir z.B. auch wieder `Int`-Zahlen zu `FloatAad`-Objekten addieren können.\n\n### Die Klasse `FloatAad`\n\nWir beginnen unsere Klasse mit einer neuen Datei, welche wir `floataad.py` nennen. Als erstes definieren wir einen Konstruktor, der uns das Umwandeln von `Int`- oder `Float`-Objekten in `FloatAad`-Objekte erlaubt. Ausserdem definieren wir auch gleich eine Funktion `float2FloatAad`, mit der wir eine Liste von solchen `Int` oder `Float` in eine Liste von `FloatAad` umwandeln können und eine Funktion `getValues`, welche aus einer Liste von `FloatAad`-Objekten die Funktionswerte ausliest, siehe dazu @sec-FunktionenMehrereInputs.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"show\"}\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\ngetValues = np.vectorize(lambda x : x.value)\n\nif __name__ == '__main__':\n\n    x = FloatAad(2)\n    print(x.value)\n    print(x.derivatives)\n    print(\"\")\n\n    x = [1,2]\n    v = float2FloatAad(x)\n    print(type(v))\n    print(type(v[0]))\n    print(type(v[0].value))\n    print(type(v[0].derivatives))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2\n()\n\n<class 'numpy.ndarray'>\n<class '__main__.FloatAad'>\n<class 'int'>\n<class 'tuple'>\n```\n:::\n:::\n\n\n### Vorzeichen\n\nWir gehen bei der Implementation der unären und binären Operatoren etwas anders vor als im @sec-SadImplementationOperatorOverloading . Wir definieren zunächst Funktionen für die Operationen und benutzen diese, um die Operatoren zu überladen.\nFür das negative Vorzeichen sieht das so aus:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"show\"}\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\n    def __pos__(self):\n        return self\n\n    def __neg__(self):\n        return neg(self)\n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\ngetValues = np.vectorize(lambda x : x.value)\n\ndef neg(a):\n    newValue = -1 * a.value\n    newDerivative = (\n        (a, -1),\n    )\n    return FloatAad(newValue, newDerivative)\n\n\nif __name__ == '__main__':\n\n    x = FloatAad(2)\n    v = -x\n\n    print(v.value)\n    print(v.derivatives)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-2\n((<__main__.FloatAad object at 0x0000022BF86A08B0>, -1),)\n```\n:::\n:::\n\n\nDer Wert von `v.derivatives` ist ein Tupel, dessen erster Eintrag eine Referenz auf das `FloatAad`-Objekt `x` ist und der Wert des zweiten Eintrags ist `-1` weil $\\partial v / \\partial x = -1$ ist.\n\n### Die Operatoren `+` und `-`\n\nWenn wir zwei `FloatAad`-Objekte `a` und `b` addieren, dann müssen wir zwei Tupel als Ableitung zurückgeben, nämlich für\n$$\n\\frac{\\partial}{\\partial a}(a+b)=1 \\qquad\\textrm{und für}\\qquad \\frac{\\partial}{\\partial b}(a+b)=1\n$$\n\nDie entsprechende Funktion sieht so aus:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"show\"}\ndef add(a, b):\n    newValue = a.value + b.value\n    newDerivative = (\n        (a, 1),  # a+b nach a abgeleitet gibt 1\n        (b, 1)   # a+b nach b abgeleitet gibt 1\n    )\n    return FloatAad(newValue, newDerivative)\n```\n:::\n\n\nFür das Überladen des `+`-Operators geben wir dann einfach `return add(self, other)` zurück. Wir wollen bei dieser Gelegenheit aber gleich noch die Typabfrage implementieren, so dass wir nicht nur zwei `FloatAad`-Objekte addieren können, sondern auch Ausdrücke wie `x + 1` schreiben können. In diesem Fall brauchen wir für die `newDerivative` nur ein Tupel, welches wir direkt in der Funktion `__add__` bestimmen. Der `__radd__`-Operator, mit dem wir einen Ausdruck wie `1 + x` schreiben können, wird analog definiert.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"show\"}\ndef __add__(self, other):\n    if type(other) in [int, float]:\n        newValue = self.value + other\n        newDerivative = (\n            (self, 1),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return add(self, other)\n    \ndef __radd__(self, other):\n    if type(other) in [int, float]:\n        newValue = other + self.value\n        newDerivative = (\n            (self, 1),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return add(other, self)\n```\n:::\n\n\n:::{#exr-AadMinusOp}\n\n## Den Operator `-` implementieren\n<br>\n\nImplementiere die Funktionen `__sub__` und `__rsub__`. Du kannst dafür die Funktionen `neg(a)` und `add(a,b)` verwenden.\n\n:::\n\n:::{.callout-tip collapse=\"true\"}\n\n## Lösung\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\ndef __sub__(self, other):\n    if type(other) in [int, float]:\n        newValue = self.value - other\n        newDerivative = (\n            (self, 1),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return add(self, neg(other))\n        \ndef __rsub__(self, other):\n    if type(other) in [int, float]:\n        newValue = other - self.value\n        newDerivative = (\n            (self, -1),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return add(other, neg(self)) \n```\n:::\n\n\n:::\n\n\n### Gradienten berechnen\n\nHier ist die bisher implementierte Klasse zusammen mit einem kleinen Testprogramm, welches die Funktion $f(x_0, x_1) = 2x_0 - x_1 + 5$ berechnet.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\n    def __pos__(self):\n        return self\n\n    def __neg__(self):\n        return neg(self)\n\n    def __add__(self, other):\n        if type(other) in [int, float]:\n            newValue = self.value + other\n            newDerivative = (\n                (self, 1),\n            )\n            return FloatAad(newValue, newDerivative)\n        else:\n            return add(self, other)\n        \n    def __radd__(self, other):\n        if type(other) in [int, float]:\n            newValue = other + self.value\n            newDerivative = (\n                (self, 1),\n            )\n            return FloatAad(newValue, newDerivative)\n        else:\n            return add(other, self)\n    \n    def __sub__(self, other):\n        if type(other) in [int, float]:\n            newValue = self.value - other\n            newDerivative = (\n                (self, 1),\n            )\n            return FloatAad(newValue, newDerivative)\n        else:\n            return add(self, neg(other))\n        \n    def __rsub__(self, other):\n        if type(other) in [int, float]:\n            newValue = other - self.value\n            newDerivative = (\n                (self, -1),\n            )\n            return FloatAad(newValue, newDerivative)\n        else:\n            return add(other, neg(self)) \n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\ngetValues = np.vectorize(lambda x : x.value)\n\ndef neg(a):\n    newValue = -1 * a.value\n    newDerivative = (\n        (a, -1),\n    )\n    return FloatAad(newValue, newDerivative)\n\ndef add(a, b):\n    newValue = a.value + b.value\n    newDerivative = (\n        (a, 1),  # a+b nach a abgeleitet gibt 1\n        (b, 1)   # a+b nach b abgeleitet gibt 1\n    )\n    return FloatAad(newValue, newDerivative)\n\nif __name__ == '__main__':\n\n    x0 = FloatAad(2)\n    x1 = FloatAad(3)\n    y = x0 + x0 - x1 + 5\n\n    print(y.value)\n    print(y.derivatives)\n    print(y.derivatives[0][0].derivatives)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6\n((<__main__.FloatAad object at 0x0000022BF861FB80>, 1),)\n((<__main__.FloatAad object at 0x0000022BF861FA30>, 1), (<__main__.FloatAad object at 0x0000022BF861FB20>, 1))\n```\n:::\n:::\n\n\nWir sehen, dass der Funktionswert $f(2,3) = 6$ korrekt ist. Als Ableitung sehen wir jedoch nur ein Tupel bestehend aus einer Referenz auf ein `FloatAad`-Objekt und einem Zwischenschritt bei der Berechnung der Ableitung. Auch die Ableitung des referenzierten `FloatAad`-Objekts enthält nur ein weiteres solches Tupel. Mit anderen Worten, wir sehen noch nirgends den Wert der partiellen Ableitungen $\\partial f / \\partial x_0 = 2$ und $\\partial f / \\partial x_1 = -1$. Wir schreiben dafür nun eine Funktion `getDerivatives(y)`, welche aus dem `FloatAad`-Objekt `y` rekursiv die partiellen Ableitungen berechnet. Dies geschieht nach der Regel, dass die Zwischenwerte der Ableitungen entlang eines Weges im Computational Graph multipliziert werden und Werte von verschiedenen Wegen, die zur gleichen Variablen $x_i$ führen, addiert werden. Für diese rekursive Berechnung definieren wir eine lokale Funktion `computeDerivative`. \nDer Rückgabewert soll dann ein Dictionary sein (`defaultdict` aus dem Modul `collections`, welches zu Beginn importiert werden muss), dessen Schlüsselwerte die Variablen `x0, x1` etc. sind und die zugehörigen Werte sind die partiellen Ableitungen $\\partial f / \\partial x_i$. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"show\"}\ndef getDerivatives(y):\n    dy = defaultdict(lambda: 0)\n\n    def computeDerivatives(y, pathValue):\n        for node, localDerivative in y.derivatives:\n            # Multipliziere entlang eines Weges im Graph\n            valueOfPathToNode = pathValue * localDerivative\n            # Addiere entlang unterschiedlicher Wege\n            dy[node] = dy[node] + valueOfPathToNode\n            # Rekursion zum Durchlaufen des ganzen Graphen\n            computeDerivatives(node, valueOfPathToNode)\n\n    # Initialisierung mit 1 (Ableitung von y nach y)\n    computeDerivatives(y, pathValue = 1)\n    return dy\n```\n:::\n\n\nDie partiellen Ableitungen können nun mit `dy = getDerivatives(y)` berechnet und mit `dy[x0]`, bzw. `dy[x1]` ausgegeben werden.\n\nWenn der Input der Funktion eine Liste `x0` ist, dann möchten wir noch eine Funktion `getGradient(x0, y)` haben, welche den Gradienten von `y` in Form einer Liste zurückgibt. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-fold=\"show\"}\ndef getGradient(x0, y):\n    dy = getDerivatives(y)\n    grad = []\n    for i in range(len(x0)):\n        grad.append(dy[x0[i]])\n    return grad\n```\n:::\n\n\nMit diesen Befehlen können wir nun unsere Programme testen.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-fold=\"show\"}\nif __name__ == '__main__':\n\n    x = [2, 3]\n    x = float2FloatAad(x)\n    \n    y = x[0] + x[0] - x[1] + 5\n    dy = getGradient(x, y)\n\n    print(y.value)\n    print(dy)\n```\n:::\n\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\n6\n[2, -1]\n```\n:::\n:::\n\n\n### Die Operatoren `*` und `/`\n\nWenn wir zwei `FloatAad`-Objekte `a` und `b` multiplizieren, dann lauten die partiellen Ableitungen\n$$\n\\frac{\\partial}{\\partial a}(a \\cdot b)=b \\qquad\\textrm{und}\\qquad \\frac{\\partial}{\\partial b}(a \\cdot b)=a\n$$\n\nWir erzeugen also wieder zwei Tupel für die Ableitung.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"show\"}\ndef mul(a, b):\n    newValue = a.value * b.value\n    newDerivative = (\n        (a, b.value),  # a*b nach a abgeleitet gibt b\n        (b, a.value)   # a*b nach b abgeleitet gibt a\n    )\n    return FloatAad(newValue, newDerivative)\n```\n:::\n\n\nDamit überladen wir nun den `*`-Operator:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code code-fold=\"show\"}\ndef __mul__(self, other):\n    if type(other) in [int, float]:\n        newValue = self.value * other\n        newDerivative = (\n            (self, other), \n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return mul(self, other)\n        \ndef __rmul__(self, other):\n    if type(other) in [int, float]:\n        newValue = other * self.value\n        newDerivative = (\n            (self, other), \n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return mul(other, self)\n```\n:::\n\n\n:::{#exr-AadDivOp}\n\n## Den Operator `/` implementieren\n<br>\n\nÄhnlich wie in @exr-AadMinusOp können wir die Division mit Hilfe der Funktion `mul(a,b)` realisieren. Schreibe dafür eine Funktion `inv(a)`, welche in $\\frac{1}{a}$ als `FloatAad`-Objekt berechnet. \nImplementiere damit die Funktionen `__truediv__` und `__rtruediv__`.\n\n:::\n\n:::{.callout-tip collapse=\"true\"}\n\n## Lösung\n\nDie Funktion `inv(a)`:\n\n::: {.cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\ndef inv(a):\n    newValue = 1. / a.value\n    newDerivative = (\n        (a, -1. / a.value**2), \n    )\n    return FloatAad(newValue, newDerivative)\n```\n:::\n\n\nUnd damit der `/`-Operator\n\n::: {.cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\ndef __truediv__(self, other):\n    if type(other) in [int, float]:\n        newValue = self.value / other\n        newDerivative = (\n            (self, 1 / other),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return mul(self, inv(other))\n        \ndef __rtruediv__(self, other):\n    if type(other) in [int, float]:\n        newValue = other / self.value\n        newDerivative = (\n            (self, - other / math.pow(self.value,2)),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return mul(other, inv(self))\n```\n:::\n\n\n:::\n\n### Der Operator `**`\n\nEs fehlt nun nur noch der Potenzoperator. Die partiellen Ableitungen von $a^b$ lauten\n$$\n\\frac{\\partial}{\\partial a}(a^b)=b\\cdot a^{b-1} \\qquad\\textrm{und}\\qquad \\frac{\\partial}{\\partial b}(a^b)=a^b \\cdot \\ln(a)\n$$\n\nWieder definieren wir uns zuerst eine Funktion\n\n::: {.cell execution_count=17}\n``` {.python .cell-code code-fold=\"show\"}\ndef pow(a, b):\n    newValue = math.pow(a.value,b.value)\n    newDerivative = (\n        (a, b.value * math.pow(a.value, b.value-1)), # a^b nach a abgeleitet gibt b*a^(b-1)\n        (b, math.pow(a.value, b.value) * math.log(a.value))  # a^b nach b abgeleitet gibt a^b * ln(a)\n    )\n    return FloatAad(newValue, newDerivative)\n```\n:::\n\n\nund benutzen diese zur Überladung des `**`-Operators:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code code-fold=\"show\"}\ndef __pow__(self, other):\n    if type(other) in [int, float]:\n        newValue = math.pow(self.value, other)\n        newDerivative = (\n            (self, other * math.pow(self.value, other - 1)),\n        ) \n        return FloatAad(newValue, newDerivative)           \n    else:\n        return pow(self, other)\n    \ndef __rpow__(self, other):\n    if type(other) in [int, float]:\n        newValue = math.pow(other, self.value)\n        newDerivative = (\n            (self, math.pow(other, self.value) * math.log(other)),\n        )\n        return FloatAad(newValue, newDerivative)\n    else:\n        return pow(other, self)\n```\n:::\n\n\nDie fertige Klasse `FloatAad` von [hier](floataad.py) kopiert werden.\n\n\n## Die Klasse `FloatAad` im Einsatz\n\n\n\nWir sind nun in der Lage, Gradienten von Funktionen zu berechnen, solange darin noch keine Ausdrücke wie $\\sin(x)$ oder $\\ln(x)$ etc. vorkommen.\n\n:::{#exm-gradientsWithAAD}\n\n## Gradient mit `FloatAad`\n<br>\n\nBetrachte die Funktion\n$$\nf(x_0, x_1, x_2) = x_0 \\cdot x_1^2 + \\frac{2 ^{x_1}}{x_2} - \\frac{2}{x_2^2}\n$$\nDer Gradient lautet\n$$\n\\nabla f = \\begin{pmatrix} \n    x_1^2  , \\;\n    2x_0x_1 + \\frac{2^{x_1}\\cdot\\ln(2)}{x_2} , \\;\n    -\\frac{2^{x_1}}{x_2^2}+\\frac{4}{x_2^3}\n    \\end{pmatrix}\n$$\n\nWerten wir die Funktion an der Stelle $(x_0, x_1, x_2) = (3, 2, -1)$ aus, dann erhalten wir $f(3, 2, -1) = 6$ und \n\n\\begin{align*}\n\\nabla f|_{(3, 2 -1)} &= \\begin{pmatrix} \n    4  , \\;\n    12-4\\cdot \\ln(2) ,\\;\n     -8\n    \\end{pmatrix} \\\\\n    &\\approx \\begin{pmatrix} \n    4  , \\;\n    9.23 , \\;\n     -8\n    \\end{pmatrix}\n\\end{align*}\n\nTesten wir dies mit unserer Klasse:\n\n::: {.cell execution_count=20}\n``` {.python .cell-code code-fold=\"show\"}\nfrom floataad import float2FloatAad, getGradient\n\ndef f(x):\n    v1 = x[0] * x[1]**2\n    v2 = 2**x[1] / x[2]\n    v3 = 2 / x[2]**2\n    y = v1 + v2 - v3\n    return y\n\nx0 = [3,2,-1]\nx0 = float2FloatAad(x0)\n\ny = f(x0)\ny0 = y.value\ndy = getGradient(x0, y)\n\nprint(\"Funktionswert: \" + str(y0))\nprint(\"Gradient: \" + str(dy))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFunktionswert: 6.0\nGradient: [4.0, 9.227411277760218, -8.0]\n```\n:::\n:::\n\n\n:::\n\n---\n\nWir können mit der Klasse `FloatAad` auch die Jacobi Matrix einer Funktion $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ bestimmen. Ähnlich wie in @sec-FuncRnToRm benötigen wir dazu jedoch mehrere Durchgänge. Diesmal muss jede *Zeile* der Matrix $Jf$ einzeln berechnet werden. Dafür müssen wir die Methode `getGradient` $m$ Mal aufrufen, nämlich einmal für jedes `y[i]`. Um dies zu automatisieren definieren wir uns wieder zwei Funktionen `getValues` und `getJacobian` (welche wir natürlich auch in die Klasse `FloatAad` schreiben könnten). Während `FloatSad` günstig war, solange $n<m$ war, so ist  `FloatAad` günstig, wenn $n>m$ ist.\n\n:::{#exm-JacobianWithAad}\n\n## Jacobi Matrix mit `FloatAad`\n<br>\n\nWir betrachten die Funktion $f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^2$ mit\n$$\nf(x_0, x_1, x_2) = \n    \\begin{pmatrix}\n        y_0 \\\\ y_1\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n        x_0 + x_1^2 + \\frac{1}{x_2} \\\\\n        x_0 \\cdot x_1 \\cdot x_2\n    \\end{pmatrix}\n$$\n\nDie Jacobi Matrix lautet\n\\begin{align*}\nJf &= \n    \\begin{pmatrix}\n        \\frac{\\partial y_0}{\\partial x_0} & \\frac{\\partial y_0}{\\partial x_1} & \\frac{\\partial y_0}{\\partial x_2} \\\\\n        \\frac{\\partial y_1}{\\partial x_0} & \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2}\n    \\end{pmatrix} \\\\\n    &=\n    \\begin{pmatrix}\n        1 & 2x_1 & -\\frac{1}{x_2 ^2} \\\\\n        x_1 x_2 & x_0 x_2 & x_0 x_1\n    \\end{pmatrix}\n\\end{align*}\n\nAusgewertet an der Stelle $(x_0, x_1, x_2) = (-2, -4, 0.5)$ erhalten wir $f(-2, -4, 0.5) = (16, 4)^\\intercal$ und \n$$\nJf|_{(-2,-4,0.5)} = \n\\begin{pmatrix}\n    1 & -8 & -4 \\\\\n    -2 & -1 & 8\n\\end{pmatrix}\n$$\n\nAls Programm mit den oben beschriebenen Funktionen erhalten wir\n\n::: {.cell execution_count=21}\n``` {.python .cell-code code-fold=\"show\"}\nfrom floataad import float2FloatAad, getGradient\nimport numpy as np\n\ndef f(x):\n    y0 = x[0] + x[1]**2 + 1/x[2]\n    y1 = x[0] * x[1] * x[2]\n    return [y0, y1]\n\nx0 = [-2,-4,0.5]\nx0 = float2FloatAad(x0)\n\ngetValues = np.vectorize(lambda y : y.value)\ngetJacobian = lambda x,y : np.array([getGradient(x, y[i]) for i in range(len(y))])\n\ny = f(x0)\n\nval = getValues(y)\nJacobian = getJacobian(x0, y)\n\nprint(\"Funktionswert: \" +str(val))\nprint(\"Jf =\")\nprint(Jacobian)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFunktionswert: [16.  4.]\nJf =\n[[ 1. -8. -4.]\n [-2. -1.  8.]]\n```\n:::\n:::\n\n\n:::\n\n---\n\n\n### Gradient Descent zum Auffinden lokaler Minima\n\nEine Funktion $f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ kann man sich als eine i.A. gekrümmte Fläche im $\\mathbb{R}^3$ vorstellen. Der Gradient $\\nabla f|_{(x_0, x_1)}$ ist ein Vektor in $\\mathbb{R}^2$, der gerade in die Richtung des steilsten Anstiegs der Fläche beim Punkt $(x_0, x_1)$ zeigt. Entsprechend zeigt $-\\nabla f$ in die Richtung des steilsten Abstiegs, siehe @Arens2022, S. 871. Damit lässt sich die Gradient Descent Methode zur Bestimmung eines lokalen Minimums, die wir im @sec-gradientDescent kennen gelernt haben, auch zum Auffinden eines lokalen Minimums einer Fläche verwenden. Sie lässt sich sogar auf Funktionen $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ anwenden, siehe @Arens2022, S. 1324.\n\nIm Fall einer 2-dimensionalen Fläche starten wir an einem Punkt $(x_{0,0}, x_{1,0})$, welchen wir durch seinen Ortsvektor $\\vec{x}_0$ beschreiben. Dann berechnen wir rekursiv eine Folge von (Ortsvektoren zu) Punkten $\\vec{x}_n$, welche im Idealfall zu einem lokalen Minimum der Funktion konvergieren, gemäss der Vorschrift\n$$\n\\vec{x}_{n+1} = \\vec{x}_n - \\lambda\\cdot(\\nabla f(\\vec{x}_n))^\\intercal\n$$\n\n$\\lambda\\in\\mathbb{R}^+$ beeinflusst wie im @sec-gradientDescent die Schrittweite. Ist $\\lambda$ zu klein, dann konvergiert die Iteration nur sehr langsam, wird $\\lambda$ hingegen zu gross gewählt, dann kann es passieren, dass sich die Iteration von einem lokalen Minimum wieder weg bewegt. \n\n:::{#exm-gradientDescentWithAAD}\n\n## Gradient Descent auf einer Fläche\n<br>\n\nBetrachte die Funktion $f(x_0, x_1) = x_0^4 + x_1^4 + x_0 x_1^3 - x_0^2 x_1 - x_1^2$.\n\n::::{.content-visible unless-format=\"pdf\"}\n\nHier wird diese Funktion einmal als 2-dimensionale Fläche und einmal als mit Hilfe von Konturlinien dargestellt. Im linken Feld kann man den Startpunkt und $\\lambda$ wählen und die ersten 30 Iterationen darstellen.\n\n:::::{.fig-GradientDescent2DGeoGebra}\n\n<iframe scrolling=\"no\" title=\"Gradient descent 2d\" src=\"https://www.geogebra.org/material/iframe/id/c2zjuxef/width/700/height/600/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/true/rc/false/ld/false/sdz/true/ctl/false\" width=\"700px\" height=\"600px\" style=\"border:0px;\"> </iframe>\n\n:::::\n\nMan erkennt, dass die Funktion drei lokale Minima hat.\n\n::::\n\nNun wollen wir ein lokales Minimum mit Hilfe der Gradient Descent Methode finden. Damit wir $\\lambda \\cdot (\\nabla f)$ als `lam * df` berechnen können, wandeln wir die Liste, die wir mit `getGradient` erhalten, in ein `numpy`-array um. Die oben beschriebene Iteration wird so lange ausgeführt, bis $|\\vec{x}_{n+1}-\\vec{x}_n|\\le 10^{-6}$ ist.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code code-fold=\"show\"}\nfrom floataad import float2FloatAad, getGradient\nimport numpy as np\n\ndef f(x):\n    y = x[0]**4 + x[1]**4 + x[0] * x[1]**3\n    y = y - x[0]**2 * x[1] - x[1]**2\n    return y\n\ngetValues = np.vectorize(lambda y : y.value)\n\n# Startwert und Lambda für Gradient Descent\nx0 = [0.5, 0]\nlam = 0.1\ntol = 1e-6 # Toleranz für Abbruchbedingung\n\nx0 = float2FloatAad(x0)\ny0 = f(x0)\ndy = np.array(getGradient(x0, y0))\n\n# Erster Schritt\nx1 = x0 - lam * dy\n\n# Iteration bis die Distanz zwischen zwei\n# aufeinanderfolgenden Punkten kleiner ist als tol.\nwhile np.linalg.norm(getValues(x0) - getValues(x1)) > tol:\n    x0 = x1\n    y0 = f(x0)\n    dy = np.array(getGradient(x0, y0))\n    x1 = x0 - lam * dy\n\nprint(\"Lokales Minimum gefunden in der Nähe von\")\nprint(getValues(x1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLokales Minimum gefunden in der Nähe von\n[0.40427048 0.6160611 ]\n```\n:::\n:::\n\n\n:::\n\n---\n\nDie folgenden drei Abschnitte beschreiben verschiedene Anwendungen des Gradient Descent Verfahrens, die wir mit Hilfe von `FloatAad` programmieren können.\n\n\n\n\n### Lineare Regression\n\nBeim klassischen Ausgleichsproblem sind $n$ Datenpunkte $(x_i , y_i)$ (Messwerte) gegeben, zwischen denen ein linearer Zusammenhang vermutet wird, d.h. $y_i \\approx g(x_i) = a\\cdot x_i + b$. Auf Grund von Messfehlern und anderen Einflüssen können wir jedoch nicht erwarten, dass der Zusammenhang exakt einer linearen Funktion entspricht, d.h. der Fehler $y_i - g(x_i)$ ist im Allgemeinen nicht Null. Das Ziel ist nun, die Parameter $a$ und $b$ so zu bestimmen, dass die Summe der Quadrate dieser Fehler möglichst klein wird. Mit anderen Worten: wir suchen das Minimum der Funktion $\\Phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$\n$$\n\\Phi(a, b) = \\sum_{i = 1}^n (y_i - (a\\cdot x_i + b))^2\n$$\nDieser Ansatz stammt von Gauss und wird auch die Methode der kleinsten Fehlerquadrate genannt. Die Funktion $\\Phi$ wird insbesondere im Kontext des maschinellen Lernens auch *Loss Funktion* genannt. Da es sich bei $\\Phi(a,b)$ um eine quadratische Funktion in $a$ und $b$ handelt, besitzt sie ein eindeutiges Minimum, welches auch rein analytisch gefunden werden kann, siehe z.B. @Arens2022, S. 1526. Wir wollen das Minimum aber mit dem Gradient Descent Verfahren bestimmen. Als Startwert verwenden wir einfach $(a, b) = (0, 0)$.\n\n:::{#exm-LinearRegressionWithGradientDescent}\n\n## Lineare Regression mit Gradient Descent und `AAD`\n<br>\n\nIn diesem Beispiel gehen wir davon aus, dass der korrekte, aber unbekannte, lineare Zusammenhang durch $y = f(x) = 2x+3$ gegeben ist. Wir erzeugen zunächst eine Anzahl von `anz = 50` Datenpunkten, die zufällig um diese Gerade streuen. Danach versuchen wir, die Parameter $a$ und $b$ mittels linearer Regression aus diesen Datenpunkten zu rekonstruieren. Die Funktion `loss(a, b)` berechnet die Funktion $\\Phi(a,b)$, deren Ableitung automatisch mittels `FloatAad` berechnet wird. Der so berechnete Gradient von $\\Phi$ wird für das Gradient Descent Verfahren verwendet. Da die Funktion nur zwei unabhängige Variablen hat, verwenden wir direkt den Konstruktor `FloatAad` und die Methode `getDerivatives` anstelle der vektorisierten Methoden `float2FloatAad` und `getGradient`. Schliesslich werden die Datenpunkte zusammen mit der korrekten Funktion $f$ und der Ausgleichsgeraden $g$ grafisch dargestellt. \n\n::: {.cell execution_count=23}\n``` {.python .cell-code code-fold=\"show\"}\nfrom floataad import FloatAad, getDerivatives\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef loss(a, b):\n    # X und Y werden im global space gefunden\n    sum = 0\n    for i in range(len(X)):\n        d = float(Y[i]) - (a * float(X[i]) + b)\n        sum += d**2\n    return np.sum(sum)\n    \n\n# Parameter zur Erzeugung der Datenpunkte\nanz = 50 # Anzahl Datenpunkte\nxmin, xmax = 0, 10\ns = 2    # Streuung\n\n# Erzeugende Funktion\nf = lambda x: 2 * x + 3\n\n# Daten erzeugen\nX = np.linspace(xmin, xmax, anz)\nY = f(X)\n# Rauschen hinzufügen\nY = Y + s * np.random.randn(anz)\n\n\n# Gradient Descent\nlam = 0.0005\n\na0, b0 = FloatAad(0), FloatAad(0) # Startwerte\n\nPhi = loss(a0, b0)\ndPhi = getDerivatives(Phi)\n\na1 = a0 - lam * dPhi[a0]\nb1 = b0 - lam * dPhi[b0]\n\nwhile (a1.value-a0.value)**2 + (b1.value-b0.value)**2 > 1e-9:\n    a0, b0 = a1, b1\n    Phi = loss(a0, b0)\n    dPhi = getDerivatives(Phi)\n    a1 = a0 - lam * dPhi[a0]\n    b1 = b0 - lam * dPhi[b0]\n\n# Regressionsgerade\na, b = a1.value, b1.value\ng = np.vectorize(lambda x : a * x + b)\nprint(\"y = g(x) = \" + str(a) + \"x + \" + str(b))\n\n# Daten darstellen\nplt.plot(X,Y, 'b.', X, f(X), 'r--', X, g(X), 'g-.')\nplt.legend([\"Datenpunkte\", \"Erzeugende Funktion\", \"Regressionsgerade\"])\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ny = g(x) = 2.153885166329361x + 2.1180405938819957\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Ausgleichsgerade.](aad_files/figure-docx/fig-graphoflinearregression-output-2.png){#fig-graphoflinearregression}\n:::\n:::\n\n\n:::\n\n---\n\n\n\n\n\n### Bilder schärfen {#sec-deblurringImages}\n\nDie Idee zu dieser Anwendung stammt von @Slater2022. Beim Fotografieren kann es passieren, dass die Linse der Kamera nicht richtig fokussiert ist und das Bild dadurch unscharf wirkt. Es ist jedoch möglich, ein Bild bis zu einem gewissen Grad nachträglich zu schärfen.\n\nAls Testbild verwenden wir das folgende Bild eines Teddybären, welches unter der Creative Commons 4.0 Lizenz auf [https://www.pngall.com/toy-png/download/55843](https://www.pngall.com/toy-png/download/55843) [Letzter Zugriff 02.04.2023] zur Verfügung gestellt wird. Allerdings müssen wir die Auflösung von ursprünglich 180 x 180 Pixel auf 30 x 30 Pixel reduzieren, was mit jeder Bildbearbeitungssoftware gemacht werden kann. Der Grund dafür ist, dass die Anzahl Pixel darüber entscheidet, wie viele `FloatAad`-Objekte wir erzeugen müssen und während des Gradient Descent Verfahrens wird der Computational Graph bei vielen Variablen sehr gross, so dass beim Berechnen der Ableitungen die maximale Rekursionstiefe überschritten würde. Wir verwenden also das in  rechts dargestellte Bild `Bear30.jpg`.\n\n::: {#fig-teddys layout=\"[1,6]\" layout-ncol=2 layout-valign=\"bottom\"}\n\n![Original 180 x 180](Bear_original.png){#fig-originalTeddy}\n\n![Testbild 30 x 30](Bear30.jpg){width=180  #fig-smallTeddy}\n\nTestbild eines Teddybärs.\n:::\n\nEs stellt sich heraus, dass selbst diese kleine Auflösung noch zu viele `FloatAad`-Objekte benötigt weil jedes Pixel drei Farbkanäle hat. Daher wandeln wir das Bild nach dem Laden zuerst in ein Graustufenbild um.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code code-fold=\"show\"}\nfrom matplotlib.image import imread\nfrom floataad import float2FloatAad, getGradient\n\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\n\noriginal = imread('bear30.jpg') / 255\n# Bild in Graustufenbild umwandeln\nimage = 1/3 * (original[:,:,0] + original[:,:,1] + original[:,:,2])\n[length, width] = np.shape(image)\n```\n:::\n\n\nUm die Unschärfe einer schlecht fokussierten Linse zu simulieren, wenden wir einen *Gaussian Blur* an. Wie das genau funktioniert wird z.B. in diesem Video von Grant Sanderson von [3blue1brown](https://www.3blue1brown.com) [Letzter Aufruf 02.04.2023] erklärt.\n\n\n{{< video https://youtu.be/KuXjwB4LzSA start=\"512\" >}}\n\n\n\n\n\nDie folgende Funktion wendet einen solchen Gaussian Blur auf ein Bild an. Die Vorlage des Codes stammt aus dem Forumsbeitrag [How to gauss-filter (blur) a floating point numpy array](https://stackoverflow.com/questions/29920114/how-to-gauss-filter-blur-a-floating-point-numpy-array) von stackoverflow [Letzter Zugriff 02.04.2023] und wurde so abgeändert, dass die kernel-Grösse selber bestimmt werden kann.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code code-fold=\"show\"}\ndef blur(a): \n    # kernel erzeugen\n    kernel_size = 4\n    k1 = [np.array([math.comb(kernel_size, k) for k in range(kernel_size)])]\n    kernel = np.dot(np.transpose(k1), k1)\n    kernel = kernel / np.sum(kernel)\n    \n    # Faltung (Convolution) ausführen\n    arraylist = []\n    for y in range(kernel_size):\n        temparray = np.copy(a)\n        temparray = np.roll(temparray, y - 1, axis=0)\n        for x in range(kernel_size):\n            temparray_X = np.copy(temparray)\n            temparray_X = np.roll(temparray_X, x - 1, axis=1)*kernel[y,x]\n            arraylist.append(temparray_X)\n\n    arraylist = np.array(arraylist)\n    arraylist_sum = np.sum(arraylist, axis=0)\n    return arraylist_sum\n```\n:::\n\n\nNun wenden wir diesen Filter auf unser Testbild an.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code code-fold=\"show\"}\n# Blur erzeugen\nblurredimage = blur(image)\nblurrarray = np.reshape(blurredimage, length*width)\n\n# Plot\nax = plt.subplot(1,2,1)\nax.set_title(\"Original\")\nax.set_axis_off()\nplt.imshow(image, cmap = \"gray\")\n\nax = plt.subplot(1,2,2)\nax.set_title(\"Blurred\")\nax.set_axis_off()\nplt.imshow(blurredimage, cmap = \"gray\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Testbild und Blur.](aad_files/figure-docx/fig-testbildmitblur-output-1.png){#fig-testbildmitblur}\n:::\n:::\n\n\nUnser Ziel ist wie gesagt, mit Hilfe des Gradient Descent Verfahrens aus dem unscharfen Bild rechts wieder so nahe wie möglich an das Original heranzukommen. Dazu beginnen wir mit einem \"Startwert\", das ein Bild ist, welches aus 30 x 30 Pixeln besteht, die alle den Wert 0.5 haben, d.h. mit einem grauen Bild. \n\n::: {.cell execution_count=27}\n``` {.python .cell-code code-fold=\"show\"}\n# Startwert\nguessimage = np.full(shape = [length, width], fill_value=0.5)\n```\n:::\n\n\nDiese $30\\times30=900$ Pixelwerte sind nun der Input unserer Loss Funktion und werden im Laufe der Gradient Descent Iteration so verändert, dass sie diese Funktion minimieren. Diese Loss Funktion definieren wir folgendermassen: Auf das `guessimage` wird zuerst ein Gaussian Blur angewendet. Danach betrachten wir die Differenzen `blur(guessimage) - blurredimage` in jedem Pixel. Nach dem Gauss'schen Ansatz der kleinsten Fehlerquadrate definieren wir die Loss Funktion als Summe der Quadrate aller Fehler in den einzelnen Pixeln. Dahinter steckt die Idee, dass wenn die Differenzen zwischen den unscharfen Bildern `blur(guessimage) - blur(original)` klein ist, dann sollten auch die Differenzen `guessimage - original` klein sein, d.h. `testimage` sollte in etwa dem `original` entsprechen.\n\nFür die konkrete Umsetzung wandeln wir das Bild mit den $30\\times30$ Pixeln in einen Vektor der Länge 900 um. Das haben wir für für das `blurredimage` bereits in der Zeile `blurrarray = np.reshape(blurredimage, length*width)` gemacht. Für das `guessimage` müssen wir nach der Umwandlung die 900 Einträge zunächst in `FloatAad`-Objekte umwandeln. All das geschieht in der folgenden Funktion.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code code-fold=\"show\"}\ndef loss(x):\n    # Input x ist ein Bild, auf welches der Gauss Filter angewendet wird\n    # Danach wird das Bild als 1-dim. Array gespeichert\n    [length, width] = np.shape(x)\n    temp = blur(x)\n    temparray = np.reshape(temp, length * width)\n\n    # Umwandeln in FloatAad\n    temparray = float2FloatAad(temparray)\n\n    y = sum((temparray - blurrarray) ** 2)\n    g = getGradient(temparray, y)\n    return [y.value, g]\n```\n:::\n\n\nNun können wir das Gradient Descent Verfahren anwenden.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code code-fold=\"show\"}\n# Gradient Descent Parameter\nlam = 0.01\ntol = 0.5\n\n[lossval, grad] = loss(guessimage)\n\nwhile lossval > tol:\n    [lossval, grad] = loss(guessimage) \n    diff = np.reshape(grad, [length, width])\n    guessimage = guessimage - lam * diff\n\n# Plot\nax = plt.subplot(2,2,1)\nax.set_title(\"Guess\")\nax.set_axis_off()\nplt.imshow(guessimage, cmap = \"gray\")\n\nax = plt.subplot(2,2,2)\nax.set_title(\"Blurred Guess\")\nax.set_axis_off()\nblurredguess = blur(guessimage)\nplt.imshow(blurredguess, cmap = \"gray\")\n\nax = plt.subplot(2,2,3)\nax.set_title(\"Original - Guess\")\nax.set_axis_off()\ndiffOrig = 0.5 * (image - guessimage + 1)\nplt.imshow(diffOrig, cmap = \"gray\")\n\nax = plt.subplot(2,2,4)\nax.set_title(\"Blurred - Blurred Guess\")\nax.set_axis_off()\ndiffBlurred = 0.5 * (blurredimage - blurredguess + 1)\nplt.imshow(diffBlurred, cmap = \"gray\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Resultat des Schärfens und Differenz zum Original](aad_files/figure-docx/fig-imagereconstructionresult-output-1.png){#fig-imagereconstructionresult}\n:::\n:::\n\n\nDas vollständige Programm kann auch [hier](deblurImage.py) heruntergeladen werden.\n\n\n\n\n\n\n## Das Modul `mathaad` {#sec-modulMathAad}\n\nIn diesem Kapitel schreiben wir ein Modul `mathaad`, welches eine Auswahl an mathematischen Funktionen beinhaltet, die auf `FloatAad`-Objekte angewendet werden können. Wir gehen dabei analog zum @sec-modulMathSad vor, beschränken uns aber auf die Funktionen `sqrt`, `exp`, `log` und die drei trigonometrischen Funktionen. Ausserdem verwenden wir Funktionen aus `numpy` weil wir als Argumente auch Arrays von `FloatAad`-Objekte übergeben wollen. Die Funktion soll in diesem Fall elementweise angewendet werden, wofür der Decorator `@np.vectorize` sorg. Der folgende Code sollte in einer Datei `mathaad.py` gespeichert und im gleichen Ordner wie die anderen Dateien abgelegt werden.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code code-fold=\"show\"}\nimport numpy as np\nfrom floataad import FloatAad\n\n@np.vectorize\ndef sqrt(x):\n    newValue = np.sqrt(x.value)\n    newDerivative = (\n        (x, 1. / (2 * np.sqrt(x.value))),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef exp(x):\n    newValue = np.exp(x.value)\n    newDerivative = (\n        (x, newValue),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef log(x):\n    newValue = np.log(x.value)\n    newDerivative = (\n        (x, 1. / x.value),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef sin(x):\n    newValue = np.sin(x.value)\n    newDerivative = (\n        (x, np.cos(x.value)),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef cos(x):\n    newValue = np.cos(x.value)\n    newDerivative = (\n        (x, -np.sin(x.value)),\n    )\n    return FloatAad(newValue, newDerivative)\n\n@np.vectorize\ndef tan(x):\n    return sin(x) / cos(x)\n```\n:::\n\n\nDie Datei kann [hier](mathaad.py) heruntergeladen werden.\n\n\n## Das Modul `mathaad` im Einsatz\n\nZum Schluss wollen wir uns ein einfaches neuronales Netz zur Lösung eines berühmten Klassifikationsproblems programmieren.\n\n### Ein einaches neuronales Netz zur Klassifikation von Lilien\n\nWir verwenden für dieses Beispiel einen der bekanntesten Datensätze, nämlich Fisher's Datensatz zu Lilien. Er wurde bereits 1936 vom britischen Biologen und Statistier Ronald Fisher verwendet und ist heute nach ihm benannt. Auch in @Hromkovic2022 (S. 160) wird auf diesen Datensatz Bezug genommen.\nDie Datei [iris.data](iris.data) kann von @Fisher1936 heruntergeladen werden. Über diesen Datensatz liest man dort\n\n> [It is] A small classic dataset from Fisher, 1936. One of the earliest datasets used for evaluation of classification methodologies. [...]\n> This is perhaps the best known database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day.\n\nDie Datei enthält 150 Datensätze (Zeilen) mit je 5 Spalten. Die 1. Spalte gibt die Länge des Kelchblattes (Sepalum) an, die 2. Spalte die Breite des Kelchblattes, die 3. Spalte enthält die Länge des Kornblattes (Petalum) und die 4. Spalte enthält die Breite des Kornblattes. Die 5. Spalte schliesslich gibt an, von welcher Lilienart die Daten stammen. Im Datensatz gibt es drei Arten von Lilien (Iris setosa, Iris versicolor und Iris virginica) und von jeder Art sind 50 Messungen enthalten.  \n\nUnser Ziel wird es sein, auf Grund der vier gemessenen Grössen (Länge und Breite des Kelch- bzw. Kornblattes) die Art vorher zu sagen. Als erstes wollen wir die Daten grafisch als Scatterplot darstellen (@Frochte2021, S. 71). Zunächst ändern wir aber die Label in der 5. Spalte noch zu `0` (Iris setosa), `1` (Iris versicolor), bzw. `2` (Iris virginica)\n\n::: {.cell execution_count=31}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\n\nfrom floataad import float2FloatAad, getValues, getGradient\nimport mathaad\n\n# Daten einlesen und Labels ändern\nfString = open('iris.data','r')\nfFloat  = open('iris.csv','w')\n\nfor line in fString:\n    line = line.replace('Iris-setosa', '0')\n    line = line.replace('Iris-versicolor', '1')\n    line = line.replace('Iris-virginica', '2')\n    fFloat.write(line)\n\nfString.close()\nfFloat.close()\n\nfFloat = open('iris.csv','r')\ndataset = np.loadtxt(fFloat, delimiter = ',')\nfFloat.close()\n\n# Daten plotten\nfig = plt.figure(1)\n\nax = fig.add_subplot(2,2,1)\nax.scatter(dataset[0:50,0], dataset[0:50,1], \n            c = 'red', s = 20, alpha = 0.6)\nax.scatter(dataset[50:100,0], dataset[50:100,1], \n            c = 'green', marker = '^', s = 20, alpha = 0.6)\nax.scatter(dataset[100:150,0], dataset[100:150,1], \n            c = 'blue', marker = '*', s = 20, alpha = 0.6)\nax.set_xlabel('Kelchblattlaenge (cm)')\nax.set_ylabel('Kelchblattbreite (cm)')\n\nax = fig.add_subplot(2,2,2)\nax.scatter(dataset[0:50,2], dataset[0:50,3], \n            c = 'red', s = 20, alpha = 0.6)\nax.scatter(dataset[50:100,2], dataset[50:100,3], \n            c = 'green', marker = '^', s = 20, alpha = 0.6)\nax.scatter(dataset[100:150,2], dataset[100:150,3], \n            c = 'blue', marker = '*', s = 20, alpha = 0.6)\nax.set_xlabel('Kronblattlaenge (cm)')\nax.set_ylabel('Kronblattbreite (cm)')\n\nax = fig.add_subplot(2,2,3)\nax.scatter(dataset[0:50,0], dataset[0:50,2], \n            c = 'red', s = 20, alpha = 0.6)\nax.scatter(dataset[50:100,0], dataset[50:100,2], \n            c = 'green', marker = '^', s = 20, alpha = 0.6)\nax.scatter(dataset[100:150,0], dataset[100:150,2], \n            c = 'blue', marker = '*', s = 20, alpha = 0.6)\nax.set_xlabel('Kelchblattlaenge (cm)')\nax.set_ylabel('Kronblattlaenge (cm)')\n\nax = fig.add_subplot(2,2,4)\nax.scatter(dataset[0:50,1], dataset[0:50,3], \n            c = 'red', s = 20, alpha = 0.6)\nax.scatter(dataset[50:100,1], dataset[50:100,3], \n            c = 'green', marker = '^', s = 20, alpha = 0.6)\nax.scatter(dataset[100:150,1], dataset[100:150,3], \n            c = 'blue', marker = '*', s = 20, alpha = 0.6)\nax.set_xlabel('Kelchblattbreite (cm)')\nax.set_ylabel('Kronblattbreite (cm)')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Scatter Plots der Fisher Iris Daten](aad_files/figure-docx/fig-imageirisscatterplot-output-1.png){#fig-imageirisscatterplot}\n:::\n:::\n\n\nNun wählen wir uns aus den 150 Einträgen des Datensatzes zufällig 30 heraus, welche wir als Testdaten verwenden. Die übrigen 120 Einträge dienen uns als Trainingsdaten, mit denen wir unser neuronales Netz trainieren werden.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code code-fold=\"show\"}\n# Daten in Trainings- und Testdaten aufteilen\nX = dataset[:, 0:4] # Messwerte\nY = dataset[:, 4]   # Label\nallData = np.arange(0, X.shape[0])\ntestIndices = np.random.choice(X.shape[0], size = 30, replace = False)\ntrainIndices = np.delete(allData, testIndices)\ndataRecords = len(testIndices)\nXTrain = X[trainIndices, :]\nYTrain = np.array(Y[trainIndices], dtype = np.int32)\nXTest = X[testIndices, :]\nYTest = Y[testIndices]\n```\n:::\n\n\nUnser Netz soll aus 4 Inputneuronen und 3 Outputneuronen bestehen. Die vier Inputs stehen für die vier Messwerte `X[0], ..., X[3]`. Wenn dort die entsprechenden Messwerte eingegeben wurden, dann werden bei den Outputneuronen `Y[0], ..., Y[2]` vier Werte zwischen $0$ und $1$ generiert, welche die Wahrscheinlichkeiten darstellen, dass es sich um die entsprechende Lilienart handelt. Das Neuron mit dem grössten Wert stellt unsere Vorhersage dar. Das neuronale Netz hat also die folgende Architektur:\n\n\n\n```{dot}\n//| label: fig-FisherNNArchitecture\n//| fig-cap: Neuronales Netz für den Lilienklassifikator.\n//| fig-width: 4    \ngraph {\n    rankdir = LR\n    fontname = \"Consolas\"\n    node [fontname = \"Cambria\", fontsize=8, width=\".2\", height=\".2\", margin=\".02\"];\n    edge [fontname = \"Cambria\", fontsize=8, arrowsize=0.25, len=minlen];\n    \n    x0 [label = \"\", shape = circle];\n    x1 [label = \"\", hape = circle];\n    x2 [label = \"\", hape = circle];\n    x3 [label = \"\", hape = circle];\n    y0 [label = \"\", hape = circle];\n    y1 [label = \"\", hape = circle];\n    y2 [label = \"\", shape = circle];\n\n    {rank = same; x0, x1, x2, x3};\n    {rank = same; y0, y1, y2};\n\n    x0 -- y0; x0 -- y1; x0 -- y2; \n    x1 -- y0; x1 -- y1; x1 -- y2; \n    x2 -- y0; x2 -- y1; x2 -- y2; \n    x3 -- y0; x3 -- y1; x3 -- y2; \n}\n```\n\n\n\nEntlang jeder Kante multiplizeren wir den Wert $x_i$ mit einem Gewicht $w_{ij}$ und addieren die vier Werte. Zu dieser Summe addieren wir noch einen Bias $b_i$ so entstehen drei Zwischenwerte\n$$\nz_i = \\sum_{j=0}^3 w_{ij}x_j + b_i \\quad\\textrm{für}\\quad i\\in\\lbrace0, 1, 2\\rbrace\n$$\nIn Matrixschreibweise können wir das ausdrücken als\n$$\n\\vec{z} = W\\cdot \\vec{x} + \\vec{b}\n$$\nmit $W\\in \\mathbb{R}^{3\\times 4}$, $\\vec{x}\\in\\mathbb{R}^4$ und $\\vec{b}, \\vec{z} \\in \\mathbb{R}^3$.\nDie Zwischenwerten $z_i$ müssen nun noch so skaliert werden, dass sie eine Wahrscheinlichkeitsverteilung darstellen. Das erreichen wir, die Softmax Funktion anwenden:\n$$\n\\hat y_i = \\sigma_i(\\vec z) = \\frac{e^{z_i}}{\\sum_{k=0}^2 e^{z_k}} \\quad\\textrm{für}\\quad i\\in\\lbrace0, 1, 2\\rbrace\n$$\nMehr zur Softmax Funktion findet man z.B. in @Frochte2021 (S. 240). Im folgenden Programm unterscheiden wir noch, ob wir die Funktion auf einen Vektor aus Zahlen oder einen Vektor aus `FloatAad`-Objekten anwenden.\n\n::: {.cell execution_count=33}\n``` {.python .cell-code code-fold=\"show\"}\n# Softmax Funktion für Vektor z\ndef softmax(z):\n    if z.dtype == \"object\":\n        return [mathaad.exp(x) / sum(mathaad.exp(z)) for x in z]\n    else:\n        return [np.exp(x) / sum(np.exp(z)) for x in z]\n```\n:::\n\n\nUnsere Vorhersage ist dann der Wert\n$$\ny = \\operatorname{argmax} \\hat y_i \\in \\lbrace0, 1, 2\\rbrace \n$$\n\nDie Frage ist also, wie wir die 12 Gewichte $w_{ij}$ und die Bias $b_i$ wählen. Letztere setzen wir einfach auf $b_i = 1$ für alle $i$. Bei der Bestimmung der Gewichte kommen unsere Trainingsdaten ins Spiel. Wir definieren uns eine Loss Funktion $J : \\mathbb{R}^{12} \\rightarrow \\mathbb{R}$, welche als Input die Gewichte $W$ erhält und als Output die so genannte Cross-Entropy liefert, welche ein Mass für die Abweichung von der korrekten Klassifikation ist. Sie wird bestimmt, indem man zuerst\n$$\nD(\\vec y, \\vec {\\hat y}) = - \\sum_i y_i\\cdot \\ln(\\hat y_i)\n$$\nberechnet, wobei die $\\hat y_i$ wie oben definiert sind und $\\vec y = (y_i)_{i=0, 1, 2}$ die One-Hot Codierung der korrekten Labels ist, d.h. \n\\begin{align*}\n    \\vec y &= (\\matrix{1, 0, 0})^\\intercal \\quad\\textrm{falls der korrekte Label 0 (Iris setosa) ist,} \\\\\n    \\vec y &= (\\matrix{0, 1, 0})^\\intercal \\quad\\textrm{falls der korrekte Label 1 (Iris versicolor) ist,} \\\\ \n    \\vec y &= (\\matrix{0, 0, 1})^\\intercal \\quad\\textrm{falls der korrekte Label 2 (Iris virginica) ist.}\n\\end{align*}\n\nDie Cross-Entropy über alle $N=120$ Beispiele erhält man dann als Mittelwert dieser Grössen\n$$\nJ(W) = \\frac{1}{N}\\sum_{n=0}^{N-1} D(\\vec y^{(n)}, \\vec{\\hat y}^{(n)})\n$$\nWeitere Details zur Cross-Entropy findet man ebenfalls in @Frochte2021 (S. 241). In der Python Funktion wandeln wir die Gewichte zuerst in `FloatAad`-Objete um und geben am Schluss den Wert $J(W)$ und die den Gradienten $\\nabla J (W)$ zurück.\n\n::: {.cell execution_count=34}\n``` {.python .cell-code code-fold=\"show\"}\ndef loss(Weights, bias, XTrain, YTrain):\n    \n    N = len(YTrain) # Anzahl Trainingsdaten\n    \n    # Gewichte als FloatAad-Matrix\n    Wtemp = float2FloatAad(Weights)\n    WtempMatrix = np.reshape(Wtemp, [4, 3])\n    \n    \n    # Labels als One-Hot Encoding\n    YOneHot = np.zeros([N, 3])\n    YOneHot[range(N), YTrain] = 1\n\n    Z = XTrain @ WtempMatrix + bias\n    # Softmax auf jede Zeile anwenden\n    Yhat = np.apply_along_axis(softmax, 1, Z)\n    \n    # Cross Entropy\n    D = [- YOneHot[i] @ mathaad.log(Yhat[i]) for i in range(N) ]\n    J = sum(D) / N\n    LossValue = getValues(J)\n    LossGrad = np.array(getGradient(Wtemp, J))\n    return [LossValue, LossGrad]\n```\n:::\n\n\nMan beachte, dass das Matrixprodukt in der Zeile `Z = XTrain @ WtempMatrix + bias` eigentlich $X\\cdot W^\\intercal + \\vec b^\\intercal$ ist, mit $X\\in\\mathbb{R}^{120\\times 4}$ und $W^\\intercal \\in \\mathbb{R}^{4\\times 3}$. Die Addition von `bias` wird dann auf jede Zeile $X\\cdot W^\\intercal \\in \\mathbb{R}^{120\\times 3}$ angewendet.\n\nNun verwenden wir wieder das Gradient Descent Verfahren, um ein lokales Minimum der Loss Funktion zu finden und damit die Gewichte zu optimieren. Wir initialisieren die Gewichte mit zufälligen Werten.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code code-fold=\"show\"}\n# Gewichte initialisieren\nW = np.random.random(4 * 3)\nb = np.ones(3)  # bias\n\n# Fit mit Gradient Descent\nlam = 0.5 # Lernrate\ntol = 1e-2\nstart = time()\n[Lval, Lgrad] = loss(W, b, XTrain, YTrain)\nwhile np.linalg.norm(Lgrad) > tol:\n    W1 = W - lam * Lgrad\n    [Lval, Lgrad] = loss(W1, b, XTrain, YTrain)\n    W = W1\nend = time()\nzeit = end - start\nprint(\"Der Lernprozess dauerte %1.2f Sekunden.\" %zeit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDer Lernprozess dauerte 12.32 Sekunden.\n```\n:::\n:::\n\n\nZum Schluss wenden wir die Matrix mit den optimierten Gewichten auf die 30 Testdaten an, welche wir im Trainingsprozess noch nicht verwendet hatten, und zählen, wie viele davon durch unser Netz korrekt klassifiziert werden.\n\n::: {.cell execution_count=36}\n``` {.python .cell-code code-fold=\"show\"}\n# Test des Modells\nW = np.reshape(W, [4, 3])\nZ = XTest @ W + b\nYp = Yp = np.apply_along_axis(softmax, 1, Z)\nY = np.apply_along_axis(np.argmax, 1, Yp)\n\n# Vergleich mit Resultaten\nnCorrect = sum(Y == YTest)\nprint(\"%d von %d wurden korrekt klassifiziert.\" %(nCorrect, dataRecords))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n28 von 30 wurden korrekt klassifiziert.\n```\n:::\n:::\n\n\nDas vollständige Program kann [hier](FisherClassification.py) heruntergeladen werden.\n\n",
    "supporting": [
      "aad_files\\figure-docx"
    ],
    "filters": []
  }
}