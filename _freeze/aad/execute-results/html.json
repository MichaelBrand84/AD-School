{
  "hash": "ea21235cdb95717039a1a4ed9e103300",
  "result": {
    "markdown": "---\ntbl-cap-location: bottom\n---\n\n# Adjungierte Algorithmische Differentiation {#sec-AAD}\n\nIn @sec-HigherDimFunctions haben wir gesehen, dass wir mit der Standard Algorithmischen Differentiation (SAD) alle $m$ Ableitungen einer Funktion $f : \\mathbb{R} \\rightarrow \\mathbb{R}^m$ mit einem einzigen Funktionsaufruf berechnen können. Die Berechnung des Gradienten einer Funktion $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ benötigt jedoch $n$ Funktionsaufrufe, nämlich einen für jede partielle Ableitung $\\partial f / \\partial x_i$. In diesem Kapitel wollen wir eine Methode entwickeln, die alle $n$ partiellen Ableitungen in einem Funktionsaufruf berechnet. \n\nÄhnlich wie die SAD beruht auch diese Methode darauf, dass wir eine komplizierte Funktion schrittweise mit Hilfe von elementaren Operationen berechnen und in jedem Schritt die Ableitungen in separaten Variablen akkumulieren. Wir führen also wieder unsere Konvention aus Kapitel @sec-ProgFunc ein. Auch dieses Mal werden wir in jedem Schritt die Kettenregel verwenden. Diesmal fangen wir jedoch am Ende der Funktion an und werden uns dann rückwärts durch alle Ableitungen arbeiten. Aus diesem Grund wird das Verfahren auch Rückwärts-AD ^[Im Englischen spricht man von *reverse mode differentiation* weil *backward differentiation* für bestimmte Methoden zur Integration von Differentialgleichungen verwendet wird.] oder Adjungierte AD (AAD) genannt. \n\n\n## Manuelle Implementation der AAD\n\nWir erläutern die Methode zuerst an einem einfachen Beispiel.\n\n:::{#exm-firstAADbyHand}\n\n## Gradient mit AAD\n<br>\n\nDieses Beispiel ist eine leicht abgeänderte Version von @sidsite2021 .\nBetrachten wir die Funktion $f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}$\n$$\ny = f(x_0, x_1) = (x_0 + x_1) \\cdot x_0 - x_1\n$$\n\nAls Programm können wir die Funktion unter Berücksichtigung der Konvention so schreiben:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"show\"}\ndef f(x0, x1):\n    v0 = x0\n    v1 = x1\n    v2 = v0 + v1\n    v3 = v2 * v0\n    v4 = v3 - v1\n    y = v4\n    return y\n\nx0, x1 = 2, 3\ny0 = f(x0, x1)\nprint(\"f(\" + str(x0) + \",\" + str(x1) + \") = \" + str(y0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nf(2,3) = 7\n```\n:::\n:::\n\n\n```{dot}\n//| label: fig-compTreeMulti\n//| fig-cap: Computational Graph für `y = (x0 + x1) * x0 - x1`.\ndigraph {\n    rankdir = LR\n    fontname = \"Consolas\"\n    node [fontname = \"Cambria\", fontsize=8, width=\".2\", height=\".2\", margin=\".02\"];\n    edge [fontname = \"Cambria\", fontsize=8, arrowsize=0.25, len=minlen];\n    \n    nx0 [label = <x<SUB>0</SUB>>, shape  = none];\n    nx1 [label = <x<SUB>1</SUB>>, shape  = none];\n    nPlus [label = \"+\", shape = circle];\n    nTimes [label = \"*\", shape = circle];\n    nMinus [label = \"-\", shape = circle];\n    ny [label = \"y\", shape = none];\n\n    {rank = same; nx0 nx1}\n    {rank = same; nPlus}\n    {rank = same; nTimes}\n    {rank = same; nMinus}\n    {rank = same; ny}\n\n    nx0 -> nPlus;\n    nx1 -> nPlus;\n    nx0 -> nTimes;\n    nPlus -> nTimes;\n    nx1 -> nMinus;\n    nTimes -> nMinus;\n    nMinus -> ny\n}\n```\n\n\nDie partiellen Ableitungen von $f$ lauten\n$$\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_0} &= \\frac{\\partial f}{\\partial x_0} = (1+0)\\cdot x_0 + (x_0 + x_1)\\cdot 1 - 0=2x_0 + x_1 \\\\\n    \\frac{\\partial y}{\\partial x_1} &= \\frac{\\partial f}{\\partial x_1} = (0 + 1)\\cdot x_0 - 1 = x_0 - 1\n\\end{align*}\n$$\nwobei für die Ableitung nach $x_0$ die Produktregel verwendet wurde. Die partielle Ableitung $\\partial y / \\partial x_0$ können wir auch berechnen, indem wir bei $y = v_4$ anfangen und jeweils die Definition der Hilfsvariablen einsetzen:\n$$\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_0} &= \\frac{\\partial v_4}{\\partial x_0} \\\\\n    &= \\frac{\\partial (v_3 - v_1)}{\\partial x_0} \\\\\n    &= \\frac{\\partial v_3}{\\partial x_0} - \\frac{\\partial v_1}{\\partial x_0} \\\\\n    &= \\frac{\\partial (v_2 \\cdot v_0)}{\\partial x_0} - \\frac{\\partial x_1}{\\partial x_0} \\\\\n    &= \\frac{\\partial v_2}{\\partial x_0} \\cdot v_0 + v_2 \\cdot \\frac{\\partial v_0}{\\partial x_0} - 0 \\\\\n    &= \\frac{\\partial (v_0 + v_1)}{\\partial x_0} \\cdot v_0 + v_2 \\cdot \\frac{\\partial x_0}{\\partial x_0} \\\\\n    &= \\left( \\frac{\\partial v_0}{\\partial x_0} + \\frac{\\partial v_1}{\\partial x_0} \\right) \\cdot x_0 + (v_0 + v_1) \\cdot 1 \\\\\n    &= \\left( \\frac{\\partial x_0}{\\partial x_0} + \\frac{\\partial x_1}{\\partial x_0} \\right) \\cdot x_0 + (x_0 + x_1) \\\\\n    &= (1 + 0) \\cdot x_0 + (x_0 + x_1) \\\\\n    &= 2x_0 + x_1   \n\\end{align*}\n$$\n\nAnalog findet man $\\partial y / \\partial x_1$ (diesmal lassen wir einige der offensichtlicheren Zwischenschritte weg):\n$$\n\\begin{align*}\n    \\frac{\\partial y}{\\partial x_1} &= \\frac{\\partial v_4}{\\partial x_1} \\\\\n    &= \\frac{\\partial v_3}{\\partial x_1} - \\frac{\\partial v_1}{\\partial x_1} \\\\\n    &= \\frac{\\partial (v_2 \\cdot v_0)}{\\partial x_1} - 1 \\\\\n    &= \\frac{\\partial v_2}{\\partial x_1} \\cdot v_0 + v_2 \\cdot \\frac{\\partial v_0}{\\partial x_1} - 1 \\\\\n    &= \\frac{\\partial (v_0 + v_1)}{\\partial x_1} \\cdot x_0 + v_2 \\cdot 0 - 1 \\\\\n    &= \\left( \\frac{\\partial v_0}{\\partial x_1} + \\frac{\\partial v_1}{\\partial x_1} \\right) \\cdot x_0 - 1 \\\\\n    &= ( 0 + 1) \\cdot x_0 - 1 \\\\\n    &= x_0 - 1   \n\\end{align*}\n$$\n\nUm die beiden Rechnungen zusammenzufassen, führen nun für jede Hilfsvariable $v_i$ eine neue Variable $\\bar v_i$ ein, welche definiert ist als\n$$\n\\bar v_i = \\frac{\\partial y}{\\partial v_i}\n$$\nÄhnlich wie die $\\dot v_i$ aus der SAD speichern diese Variablen die Werte der Ableitungen. Die neue Notation soll anzeigen, dass es sich um die AAD Methode handelt. Unser Ziel ist es also, $\\bar v_0 = \\partial y / \\partial v_0 = \\partial y / \\partial x_0$ und $\\bar v_1 = \\partial y / \\partial v_1 = \\partial y / \\partial x_1$ zu bestimmen. Beginnen wir in der letzten Zeile des Programms, dann gilt offenbar immer $\\bar v_4 = \\partial y / \\partial v_4 = 1$.  In der Zeile darüber können wir $\\bar v_3$ und $\\bar v_1$ berechnen, indem wir die Kettenregel verwenden.\n\n\\begin{align*}\n    \\bar v_3 &= \\frac{\\partial y}{\\partial v_3} = \\frac{\\partial y}{\\partial v_4} \\cdot \\frac{\\partial v_4}{\\partial v_3} = \\bar v_4 \\cdot (1-0)=\\bar v_4  \\\\\n\n    \\bar v_1 &= \\frac{\\partial y}{\\partial v_1} = \\frac{\\partial y}{\\partial v_4} \\cdot \\frac{\\partial v_4}{\\partial v_1} = \\bar v_4 \\cdot (0-1)= -\\bar v_4\n\\end{align*}\n\n\nAlso sind $\\bar v_3 = 1$ und $\\bar v_1 = -1$. Der Zwischenwert in $\\bar v_1$ wird später ergänzt werden. \nAus der Zeile $v_3 = v_2 \\cdot v_0$ lassen sich als nächstes Ausdrücke für $\\bar v_2$ und $\\bar v_0$ finden.\n\n\\begin{align*}\n    \\bar v_2 &= \\frac{\\partial y}{\\partial v_2} = \\frac{\\partial y}{\\partial v_3} \\cdot \\frac{\\partial v_3}{\\partial v_2} = \\bar v_3 \\cdot v_0 \\\\\n\n    \\bar v_0 &= \\frac{\\partial y}{\\partial v_0} = \\frac{\\partial y}{\\partial v_3} \\cdot \\frac{\\partial v_3}{\\partial v_0} = \\bar v_3 \\cdot v_2 \n\\end{align*}\n\nMit den vorher berechneten Werten erhalten wir also $\\bar v_2 = v_0$ und $\\bar v_0 = v_2$. Beide Werte sind durch die Funktion bereits berechnet worden. Im obigen Beispiel gilt etwa `v0 = x0 = 2` und `v2 = x0 + x1 = 5`. Auch diese Zwischenwerte werden im nächsten Schritt ergänzt. Aus der Zeile $v_2 = v_0 + v_1$ ergibt sich nämlich\n\n\\begin{align*}\n    \\bar v_0 &= \\bar v_0 + \\frac{\\partial y}{\\partial v_0} = \\bar v_0 + \\frac{\\partial y}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial v_0} \\\\\n    &= \\bar v_0 + \\bar v_2 \\cdot (1+0) = \\bar v_0 + \\bar v_2 \\\\ & \\\\\n\n    \\bar v_1 &= \\bar v_1 + \\frac{\\partial y}{\\partial v_1} = \\bar v_1 + \\frac{\\partial y}{\\partial v_2} \\cdot \\frac{\\partial v_2}{\\partial v_1} \\\\\n    &= \\bar v_1 + \\bar v_2 \\cdot (0+1) = \\bar v_1 + \\bar v_2\n\\end{align*}\n\nMit den bereits bekannten Werten erhalten wir $\\bar v_0 = v_2 + v_0$ (bzw. mit den konkreten Werten des Beispiels `v0bar = 5 + 2`) und $\\bar v_1 = -1 + v_0$ (bzw. `v1bar = -1 + 2`).\nNun enthalten die Variablen $\\bar v_0$ und $\\bar v_1$ die Werte der gewünschten Ableitungen, nämlich $\\bar v_0 = (v_0 + v_1) + v_0 = 2x_0 + x_1$ und $\\bar v_1 = -1 + x_0$. Wir können aber die letzten Schritte analog zu den vorherigen ausführen:\n\n\\begin{align*}\n    \\bar x_1 &= \\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial v_1} \\cdot \\frac{\\partial v_1}{\\partial x_1} = \\bar v_1 \\cdot 1  \\\\\n\n    \\bar x_0 &= \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial y}{\\partial v_0} \\cdot \\frac{\\partial v_0}{\\partial x_0} = \\bar v_0 \\cdot 1  \\\\\n\\end{align*}\n\n\nDie Schwierigkeit besteht darin, dass wir nicht wie bei der SAD in jedem Schritt die Variable $v_i$ und gleichzeitig die Variable $\\dot v_i$ berechnen können. Um die $\\bar v_i$ zu bestimmen muss man zuerst die Funktion komplett ausführen, und sich dabei den Aufbau des Computational Graph merken. Erst dann kann man rückwärts die Ableitungswerte berechnen, angefangen bei der letzten Hilfsvariablen $\\bar v_4 = 1$. Das folgende Schema fasst die obigen Rechnungen zusammen.\n\n\\begin{equation*}\n\\left \\downarrow \n    \\begin{aligned}[c] \n        v_0 &= x_0 \\\\ \n        v_1 &= x_1 \\\\\n        v_2 &= v_0 + v_1 \\\\\n        v_3 &= v_2 \\cdot v_0 \\\\\n        v_4 &= v_3 - v_1 \\\\\n        y &= v_4\n    \\end{aligned}  \n\\right .\n\n\\qquad\n\n\\begin{aligned}[c] \n    & \\\\ \n    & \\\\\n    & \\\\\n    & \\\\\n    & \\\\\n    &\\longrightarrow\n\\end{aligned}  \n\n\\qquad\n\n\\left \\uparrow \n    \\begin{aligned}[c] \n        \\bar x_0 &= \\bar v_0 = 2\\cdot x_0 + x_1 \\\\ \n        \\bar x_1 &= \\bar v_1 = -1 + x_0 \\\\\n        \\bar v_0 &= \\bar v_0 + \\bar v_2, \\quad \\bar v_1 = \\bar v_1 + \\bar v_2 \\\\\n        \\bar v_2 &= \\bar v_3 \\cdot v_0, \\quad \\bar v_0 = \\bar v_3 \\cdot v_2 \\\\\n        \\bar v_3 &= \\bar v_4, \\quad \\bar v_1 = -\\bar v_4 \\\\\n        \\bar v_4 &= \\bar y = 1\n    \\end{aligned}  \n\\right . \n\\end{equation*}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\ndef f(x0, x1):\n    v0 = x0\n    v1 = x1\n    v2 = v0 + v1\n    v3 = v2 * v0\n    v4 = v3 - v1\n    y = v4\n    v4bar = 1\n    v3bar = v4bar\n    v1bar = -v4bar\n    v2bar = v3bar * v0\n    v0bar = v3bar * v2\n    v0bar = v0bar + v2bar\n    v1bar = v1bar + v2bar\n    grad = [v0bar, v1bar]\n    return [y, grad]\n\nx0, x1 = 2, 3\n[y0, dy] = f(x0, x1)\nprint(\"Funktionswert: \" + str(y0))\nprint(\"Gradient: \" + str(dy))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFunktionswert: 7\nGradient: [7, 1]\n```\n:::\n:::\n\n\nDie Werte der Ableitungen $\\bar v_i$ lassen sich auch im Computational Graph verfolgen. Der Wert bei der Kante von $v_i$ nach $v_j$ entspricht der partiellen Ableitung $\\partial v_i / \\partial v_j$. Entlang eines Weges werden die Werte multipliziert. Führen mehrere Wege zu einer Variablen $x_i$, so werden die Werte der einzelnen Wege addiert.\n\n\n```{dot}\n//| label: fig-compTreeMultiReversed\n//| fig-cap: Werte der `vbar`.\ndigraph {\n    rankdir = LR\n    fontname = \"Consolas\"\n    node [fontname = \"Cambria\", fontsize=8, width=\".2\", height=\".2\", margin=\".02\"];\n    edge [fontname = \"Cambria\", fontsize=8, arrowsize=0.25, len=minlen];\n    \n    nx0 [label = <x<SUB>0</SUB>>, shape  = none];\n    nx1 [label = <x<SUB>1</SUB>>, shape  = none];\n    nPlus [label = \"+\", shape = circle];\n    nTimes [label = \"*\", shape = circle];\n    nMinus [label = \"-\", shape = circle];\n    ny [label = \"y\", shape = none];\n    \n    {rank = same; nx0 nx1}\n    {rank = same; nPlus}\n    {rank = same; nTimes}\n    {rank = same; nMinus}\n    {rank = same; ny}\n    \n    nx0 -> nPlus [dir = back, color = red, fontcolor = red, xlabel = 1];\n    nx1 -> nPlus [dir = back, color = red, fontcolor = red, xlabel = 1];\n    nx0 -> nTimes [dir = back, color = red, fontcolor = red, xlabel = <v<SUB>2</SUB>>];\n    nPlus -> nTimes [dir = back, color = red, fontcolor = red, xlabel = <v<SUB>0</SUB>>];\n    nx1 -> nMinus [dir = back, color = red, fontcolor = red, xlabel = -1];\n    nTimes -> nMinus [dir = back, color = red, fontcolor = red, xlabel = 1];\n    nMinus -> ny [dir = back, color = red, fontcolor = red, xlabel = 1];\n}\n```\n\n\n:::\n\n---\n\n:::{#exr-EigeneAADBeispiele1}\n\n## Eigene Beispiele finden\n<br>\n\nSchreibe eigene Funktion $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ für $n\\in\\lbrace 2, 3 \\rbrace$ hin und erstelle den Computational Graph und ein Programm. Leite das Programm nach der oben beschriebenen AAD Methode ab und überzeuge dich an verschiedenen Stellen davon, dass der Gradient korrekt ist.\n\n:::\n\n:::{.callout-tip collapse=\"true\"}\n\n## Lösung\nBeispiele können in der Literatur gefunden werden, z.B. bei @sidsite2021, @Slater2022, @Baydin18 (S. 13), @Griewank2008EDP (S. 9, S. 42) oder @Henrard2017ADi (S. 24).\n\n:::\n\n\n\n\n\n## Implementation der AAD mit Operator Overlaoding\n\nNun wollen wir ähnlich wie im @sec-SadImplementationOperatorOverloading eine Klasse `FloatAad` entwerfen, welche die Berechnung aller Hilfsvariablen `vbar` automatisch ausführt. Wie auch zuvor hat jedes `FloatAad`-Objekt ein Attribut `value` vom Typ `Int`. Allerdings reicht es nicht mehr aus, ein `Int`-Attribut `derivative` zu definieren, um den Wert der Ableitung zu speichern weil auch die Struktur des Computational Graph gespeichert werden muss. Als Attribut `derivatives` wählen wir ein `tuple`, dessen erster Eintrag ein `FloatAad`-Objekt ist, nämlich die Variable, nach der die partielle Ableitung berechnet wird, und der zweite Eintrag ist der Wert dieser partiellen Ableitung. Da das erste Element des Tupels selber auch ein Attribut `derivatives` hat, entsteht so eine rekursive Darstellung des Computational Graph. \n\nDie folgende Implementation ist stark von @sidsite2021 inspiriert. Die Variablennamen wurden angepasst, so dass sie konsistent mit den Bezeichnungen aus @sec-SADforOneDimFunctions sind. Ausserdem werden wir unsere Klasse noch mit einiger zusätzlicher Funktionalität ausstatten, etwa mit Typunterscheidungen, so dass wir `Int`-Zahlen zu `FloatAad`-Objekten addieren können.\n\n### Die Klasse `FloatAad`\n\nWir beginnen unsere Klasse mit einer neuen Datei, welche wir `floataad.py` nennen. Als erstes definieren wir einen Konstruktor, der uns das Umwandeln von `Int`- oder `Float`-Objekten in `FloatAad`-Objekte erlaubt. Ausserdem definieren wir auch gleich eine Funktion, mit der wir eine Liste von solchen `Int` oder `Float` in eine Liste von `FloatAad` umwandeln können, siehe dazu @sec-FunktionenMehrereInputs.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"show\"}\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\n\n\nif __name__ == '__main__':\n\n    x = FloatAad(2)\n    print(x.value)\n    print(x.derivatives)\n    print(\"\")\n\n    x = [1,2]\n    v = float2FloatAad(x)\n    print(type(v))\n    print(type(v[0]))\n    print(type(v[0].value))\n    print(type(v[0].derivatives))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2\n()\n\n<class 'numpy.ndarray'>\n<class '__main__.FloatAad'>\n<class 'int'>\n<class 'tuple'>\n```\n:::\n:::\n\n\n### Vorzeichen\n\nWir gehen bei der Implementation der unären und binären Operatoren etwas anders vor als im @sec-SadImplementationOperatorOverloading . Wir definieren zunächst Funktionen für die Operationen und benutzen diese, um die Operatoren zu überladen.\nFür das negative Vorzeichen sieht das so aus:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"show\"}\nimport numpy as np\n\nclass FloatAad:\n\n    def __init__(self, value, derivatives = ()):\n        self.value = value\n        self.derivatives = derivatives\n\n    def __pos__(self):\n        return self\n\n    def __neg__(self):\n        return neg(self)\n\nfloat2FloatAad = np.vectorize(lambda x: FloatAad(x))\n\ndef neg(a):\n    newValue = -1 * a.value\n    newDerivative = (\n        (a, -1),\n    )\n    return FloatAad(newValue, newDerivative)\n\n\nif __name__ == '__main__':\n\n    x = FloatAad(2)\n    v = -x\n\n    print(v.value)\n    print(v.derivatives)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-2\n((<__main__.FloatAad object at 0x0F478280>, -1),)\n```\n:::\n:::\n\n\nDer Wert von `v.derivatives` ist ein Tupel, dessen erster Eintrag eine Referenz auf `x` ist und der Wert des zweiten Eintrags ist `-1` weil $\\partial v / \\partial x = -1$ ist.\n\n### Die Operatoren `+` und `-`\n\nWenn wir zwei `FloatAad`-Objekte `a` und `b` addieren, dann müssen wir zwei Tupel als Ableitung zurückgeben, nämlich für\n$$\n\\frac{\\partial}{\\partial a}(a+b)=1 \\qquad\\textrm{und für}\\qquad \\frac{\\partial}{\\partial b}(a+b)=1\n$$\n\nDie entsprechende Funktion sieht so aus:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"show\"}\ndef add(a, b):\n    newValue = a.value + b.value\n    newDerivative = (\n        (a, 1),  # a+b nach a abgeleitet gibt 1\n        (b, 1)   # a+b nach b abgeleitet gibt 1\n    )\n    return FloatAad(newValue, newDerivative)\n```\n:::\n\n\nFür das Überladen des `+`-Operators geben wir dann einfach `return add(self, other)` zurück. Wir wollen bei dieser Gelegenheit aber gleich noch die Typabfrage implementieren, so dass wir nicht nur zwei `FloatAad`-Objekte addieren können, sondern auch `x + 1` schreiben können. In diesem Fall wandeln wir die Zahl einfach in ein `FloatAad`-Objekt um und wenden die Funktion `add` an. Der `__radd__`-Operator, mit dem wir einen Ausdruck wie `1 + x` schreiben können, wird analog definiert.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"show\"}\ndef __add__(self, other):\n    if type(other) in [int, float]:\n        return add(self, FloatAad(other))\n    else:\n        return add(self, other)\n\ndef __radd__(self, other):\n    if type(other) in [int, float]:\n        return add(FloatAad(other), self)\n    else:\n        return add(other, self)\n```\n:::\n\n\n:::{#exr-AadMinusOp}\n\n## Den Operator `-` implementieren\n<br>\n\nImplementiere die Funktionen `__sub__` und `__rsub__`. Du kannst dafür die Funktion `neg(a)` verwenden.\n\n:::\n\n:::{.callout-tip collapse=\"true\"}\n\n## Lösung\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\ndef __sub__(self, other):\n    if type(other) in [int, float]:\n        return add(self, FloatAad(-other))\n    else:\n        return add(self, neg(other))\n        \ndef __rsub__(self, other):\n    if type(other) in [int, float]:\n        return add(FloatAad(other), neg(self))\n    else:\n        return add(neg(other), self)\n```\n:::\n\n\n:::\n\n",
    "supporting": [
      "aad_files"
    ],
    "filters": [],
    "includes": {}
  }
}