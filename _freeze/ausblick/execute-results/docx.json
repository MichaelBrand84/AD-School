{
  "hash": "2a5ed698843e07b43969348d047948b4",
  "result": {
    "markdown": "---\ntbl-cap-location: bottom\n---\n\n# Ausblick: Moderne Bibliotheken für automatische Differentiation {#sec-ausblick}\n\nZum Schluss werfen wir einen kurzen Blick auf PyTorch, einer Open Source Machine Learning Bibliothek (@PyTorch). Diese kann von [PyTorch](https://pytorch.org/) heruntergeladen werden. Von den zahlreichen verfügbaren Modulen beschränken wir uns hier auf *Autograd*, das zur Berechnung von Ableitungen benutzt wird.\n\nNeben der Dokumentation auf [PyTorch 2.0 documentation](https://pytorch.org/docs/stable/autograd.html) gibt auch das folgende Video von [Elliot Waite](https://www.youtube.com/@elliotwaite) [letzter Aufruf 06.06.2023] eine gute Einführung in die Funktionsweise und Bedienung von Autograd.\n\n\n\n {{< video https://www.youtube.com/watch?v=MswxJw-8PvE&t=1s&ab_channel=ElliotWaite >}}\n\n\n\n\n\n\n\n\n\n## Vergleich von `pytorch.autograd` mit `FloatSad` und `FloatAad`\n\nIn einem ersten Beispiel berechnen wir einen mathematischen Ausdruck ohne dafür eine Funktion zu definieren. \n\n:::{#exm-FunctionEvaluationPyTorch}\n\nBetrachten wir als erstes die Funktion von @exr-FunToGraphProg, nämlich $y = f(x) = \\frac{\\ln(x^2 + 1)}{\\sqrt{x^2 + 1 + x}}$. Wir wollen an der Stelle $x_0 = 2$ den Funktionswert und den Wert der Ableitung berechnen und verwenden dazu einmal unsere Klassen `FloatSad` bzw. `FloatAad` und einmal `pytorch.autgrad`.\n\n\n\n::::{.panel-tabset}\n\n## `FloatSad`\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nfrom floatsad import FloatSad\nimport mathsad\n\nx0 = FloatSad(2)\ny0 = mathsad.log(x0**2 + 1) / mathsad.sqrt(x0**2 + 1 + x0)\n\nprint(y0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n< 0.6083103524142255 ; 0.08511788111658697 >\n```\n:::\n:::\n\n\nFunktionen $f:\\mathbb{R} \\rightarrow \\mathbb{R}$ können direkt evaluiert werden.\n\n## `FloatAad`\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nfrom floataad import float2FloatAad, getGradient\nimport mathaad\n\nx = float2FloatAad([2])\ny = mathaad.log(x[0]**2 + 1) / mathaad.sqrt(x[0]**2 + 1 + x[0])\n\n# Werte extrahieren\ny0 = y.value\ndy = getGradient(x,y)\n\nprint(y0)\nprint(dy[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.6083103524142255\n0.08511788111658698\n```\n:::\n:::\n\n\nDie Funktion `getGradient` erwartet Listen, deshalb muss man auch für Funktionen in einer Variablen dem Konstruktor eine Liste übergeben. \n\n## `autograd`\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\n\nx0 = torch.tensor(2., requires_grad=True)\ny = torch.log(x0**2 + 1) / torch.sqrt(x0**2 + 1 + x0)\n\ny.backward() # Ableitungen mit AAD berechnen\ndy = x0.grad # Ableitung von dy nach dx0\n\n# Werte aus Tensor extrahieren\ny0 = y.item()\ndy0 = dy.item()\n\nprint(y0)\nprint(dy0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.6083104014396667\n0.08511786162853241\n```\n:::\n:::\n\n\nDer Datentyp in `PyTorch` heisst `tensor` und entspricht in etwa unserem `FloatAad`. Beachte, dass dem Konstruktor ein `Float` übergeben werden muss, also z.B. `2.0` und nicht `2`. \nMit Hilfe der `tensor`-Variablen `x` und den speziell implementierten mathematischen Funktionen berechnen wir den Audruck `y`. Jede `tensor`-Variable hat unter anderem ein Attribut  `requires_grad`, welches speichert, ob die Ableitung von `y` nach dieser Variablen, also $\\partial y / \\partial x$ benötigt wird. Der Standardwert dafür ist `False`. \nBei der Berechnung von `y` wird der computational graph kreiert und mit `y.backward()` werden die Ableitungen nach der AAD-Methode berechnet. Dabei wird in jeder Variable, die zur Berechnung von `y` benötigt wird und deren Attribut `requires_grad=True` ist, der Wert der partiellen Ableitung in einem Attribut `grad` gespeichert. \n\n::::\n\n:::\n\n---\n\nIm nächsten Beispiel betrachten wir die Berechnung des Gradienten einer Funktion $f : \\mathbb{R}^3 \\rightarrow \\mathbb{R}$. Wieder vergleichen wir die Implementation in `autograd` mit unserer Klasse `FloatAad`. \n\n:::{#exm-GradientPyTorchVsFloatAad}\n\nWir betrachten nochmals die Funktion aus @exm-gradientsWithAAD. \n\n::::{.panel-tabset}\n\n## `FloatAad`\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nfrom floataad import float2FloatAad, getGradient\n\ndef f(x):\n    v1 = x[0] * x[1]**2\n    v2 = 2**x[1] / x[2]\n    v3 = 2 / x[2]**2\n    y = v1 + v2 - v3\n    return y\n\nx0 = [3,2,-1]\nx0 = float2FloatAad(x0)\n\ny = f(x0)\ny0 = y.value\ndy = getGradient(x0, y)\n\nprint(y0)\nprint(dy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6.0\n[4.0, 9.227411277760218, -8.0]\n```\n:::\n:::\n\n\n## `autograd`\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\n\ndef f(x):\n    v1 = x[0] * x[1]**2\n    v2 = 2**x[1] / x[2]\n    v3 = 2 / x[2]**2\n    y = v1 + v2 - v3\n    return y\n\nx0 = [3., 2., -1.]\nx0 = torch.tensor(x0, requires_grad=True)\n\ny = f(x0)\ny0 = y.item() # Funktionswert aus Tensor extrahieren\n\ny.backward()  # AAD anwenden\ndy = x0.grad\n\nprint(y0)\nprint(dy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6.0\ntensor([ 4.0000,  9.2274, -8.0000])\n```\n:::\n:::\n\n\n::::\n\n:::\n\n---\n\nAls letztes Beispiel vergleichen wir die Berechnung der Jacobi Matrix einer Funktion $f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$.\n\n:::{#exm-JacobianAutogradVsFloatSad}\n\nBetrachte die Funktion aus @exm-ExFunctionR2ToR3. Zur Berechnung der Jacobi Matrix $Jf \\in \\mathbb{R}^{3\\times 2}$ benötigt man mit `FloatSad` zwei Funktionsaufrufe (je einen für jede Spalte) und mit `FloatAad` drei Funktionsaufrufe (je einen für jede Zeile). Wir verwenden daher `FloatSad`, beschränken uns aber auf die Berechnung der ersten Spalte von $Jf$. In `PyTorch` gibt es eine spezielle Funktion zur Berechnung der Jacobi Matrix.\n\n::::{.panel-tabset}\n\n## `FloatSad`\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\nfrom floatsad import *\nimport mathsad\n\ndef f(x):\n    xdot = [1, 0]\n    x = float2FloatSad(x, xdot)\n    y1 = x[0]*mathsad.sqrt(x[1]) + 3*x[1]\n    y2 = mathsad.cos(x[0]) / x[1]\n    y3 = mathsad.exp(x[0]**2 * x[1])\n    return [y1, y2, y3]    \n\nx0 = (2, 1)\ny0 = f(x0)\nprint(getValues(y0))\nprint(getDerivatives(y0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 5.         -0.41614684 54.59815003]\n[  1.          -0.90929743 218.39260013]\n```\n:::\n:::\n\n\n## `autograd`\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\nimport torch\nfrom torch.autograd.functional import jacobian\n\ndef f(x):\n    y1 = x[0]*torch.sqrt(x[1]) + 3*x[1]\n    y2 = torch.cos(x[0]) / x[1]\n    y3 = torch.exp(x[0]**2 * x[1])\n    return torch.stack([y1, y2, y3])\n\nx0 = torch.tensor([2., 1.], requires_grad=True)\n\ny0 = f(x0)\nj = jacobian(f, x0)\n\nprint(y0)\nprint(j)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([ 5.0000, -0.4161, 54.5981], grad_fn=<StackBackward0>)\ntensor([[  1.0000,   4.0000],\n        [ -0.9093,   0.4161],\n        [218.3926, 218.3926]])\n```\n:::\n:::\n\n\n::::\n\n:::\n\n---\n\n",
    "supporting": [
      "ausblick_files\\figure-docx"
    ],
    "filters": []
  }
}