{
  "hash": "adbac63089fa48f8e29b2ac6d9630e4f",
  "result": {
    "markdown": "# AD ist nicht ... {#sec-ADisnot}\nBevor wir uns mit den konkreten Implementationen von algorithmischer Differentiation beschäftigen, wollen wir herausstellen, was AD *nicht* ist.\n\n\n## AD ist nicht numerisches Ableiten {#sec-ADnotNumDiff}\nEine Funktion $y = f(x)$ ist bekanntlich differenzierbar an der Stelle $x_0 \\in \\mathbb{D}$, wenn der Grenzwert\n$$ \\lim_{h\\rightarrow 0} \\frac{f(x_0 + h) - f(x_0)}{h} $$\nexistiert. In dem Fall ist $f'(x_0)$ einfach der Wert dieses Grenzwerts.\n\nEin erster Ansatz zur numerischen Berechnung könnte also sein, den Differenzenquotienten für kleine $h$ auszuwerten^[Dieser Ansatz kann verbessert werden indem man z.B. $f'(x_0) \\approx \\frac{f(x_0 + h) - f(x_0 - h)}{2h}$ verwendet. Die im Beispiel beschriebenen Probleme bleiben aber auch dann bestehen.].\n\n\n\n:::{#exm-numDiff}\n\n## Numerische Ableitung\n<br>\n\nLeite die Funktion $f(x) = x^2$ an der Stelle $x_0 = 0.2$ ab.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"show\"}\ndef f(x):\n    y = x ** 2\n    return y\n\ndef fdot(f, x0, h):\n    df = (f(x0 + h) - f(x0)) / h\n    return df\n\nx0 = 0.2\nH = [0.1, 0.01, 0.001, 0.0001]\nfor h in H:\n    ydot = fdot(f, x0, h)\n    print(\"h = \" + str(h) + \" \\t=> f'(x0) = \" + str(ydot))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nh = 0.1 \t=> f'(x0) = 0.5000000000000001\nh = 0.01 \t=> f'(x0) = 0.4099999999999999\nh = 0.001 \t=> f'(x0) = 0.4009999999999986\nh = 0.0001 \t=> f'(x0) = 0.40009999999993107\n```\n:::\n:::\n\n\nEs scheint zunächst, als ob die Werte für kleiner werdende $h$ zum korrekten Wert $f'(0.2)=0.4$ konvergieren. Wenn wir aber an sehr genauen Werten interessiert sind und entsprechend $h$ sehr klein wählen, beobachten wir folgendes:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef f(x):\n    y = x ** 2\n    return y\n\ndef fdot(f, x0, h):\n    df = (f(x0 + h) - f(x0)) / h\n    return df\n\nx0 = 0.2\nH = [10 ** -8, 10 ** -9, 10 ** -10, 10 ** -11]\nfor h in H:\n    ydot = fdot(f, x0, h)\n    print(\"h = \" + str(h) + \"\\t=> f'(x0) = \" + str(ydot))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nh = 1e-08\t=> f'(x0) = 0.4000000095039091\nh = 1e-09\t=> f'(x0) = 0.3999999984016789\nh = 1e-10\t=> f'(x0) = 0.4000000330961484\nh = 1e-11\t=> f'(x0) = 0.3999994779846361\n```\n:::\n:::\n\n\nMit kleiner werdendem $h$ scheint sich der Näherungswert für die Ableitung zu verschlechtern. Das Phänomen wird noch deutlicher, wenn wir den Fehler $E(h) = \\lvert\\frac{f(x_0+h)-f(x_0)}{h} - f'(x_0)\\rvert$ als Funktion von $h$ plotten. Beachte die doppelt logarithmische Skala.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport math\n\ndef f(x):\n    y = x ** 2\n    return y\n\ndef fdot(f, x0, h):\n    df = (f(x0 + h) - f(x0)) / h\n    return df\n\nx0 = 0.2\nH = [10**(k/100) for k in range(-1800, -300)]\nE = [math.fabs(fdot(f, x0, h) - 2*x0) for h in H]\n\n# Plot\nfig = plt.figure()\nax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\nax.set(xlim=(10**-18, 10**-3), ylim=(10**-12, 10**0))\nax.set_xscale('log')\nax.set_xlabel('h')\nax.set_yscale('log')\nax.set_ylabel('Fehler E(h)')\nplt.plot(H,E)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Grösse des Fehlers $E(h)$ als Funktion der Schrittweite $h$. Ist $h$ zu gross, dann ist der Näherungswert für $f'(x_0)$ ungenau. Bei kleiner werdendem $h$ nimmt der Fehler zunächst ab, aber ab einem gewissen Wert dominiert die Auslöschung und der Fehler nimmt wieder zu.](notAD_files/figure-pdf/fig-numdiffproblem-output-1.pdf){#fig-numdiffproblem}\n:::\n:::\n\n\n:::\n\n---\n\n### Auslöschung\nIm vorherigen Beispiel haben wir das Phänomen der [Auslöschung](https://de.wikipedia.org/wiki/Ausl%C3%B6schung_(numerische_Mathematik)) beobachtet. Zunächst ist dir sicher aufgefallen, dass der Näherungswert für $f'(x_0)$ mit $h=0.01$ nicht \n$$ \\frac{f(x_0 + h) - f(x_0)}{h} = \\frac{0.21^2 - 0.2^2}{0.01}=0.41$$\nergab, sondern $f'(x_0)\\approx 0.40999...$. Das liegt daran, dass Dezimalzahlen nicht exakt als Binärzahl dargestellt werden können. Da nun die Werte von $f(x_0 + h)$ und $f(x_0)$ für kleine $h$ fast gleich sind, besteht ihre Differenz $f(x_0 + h) - f(x_0)$ nur noch aus diesen Rundungsfehlern. Diese (sinnlose) Differenz ist zwar sehr klein, wird aber im nächsten Schritt mit der sehr grossen Zahl $\\frac{1}{h}$ multipliziert, wodurch die Rundungsfehler die gleiche Grössenordnung annehmen, wie die ursprünglichen Funktionswerte. Mehr über Rundungsfehler und Auslöschung kann in @Weitz2021 ab S. 117 nachgelesen werden.\n\n\n## AD ist nicht symbolisches Ableiten {#sec-ADnotSymbDiff}\nComputer Algebra Systeme (CAS) sind Programme zur Bearbeitung algebraischer Ausdrücke. Mit solchen Programmen lassen sich auch Ableitungen symbolisch bestimmen. Wie das funktioniert, wird in @Slater2022 kurz angedeutet. \nBekannte Beispiele für CAS sind etwa [Wolfram Alpha](https://www.wolframalpha.com/), [Maxima](https://maxima.sourceforge.io/download.html) oder [Sage](https://www.sagemath.org/). Letzteres kann man [hier](https://sagecell.sagemath.org/) auch online ausprobieren. Gib z.B. den folgenden Code ein, welcher die Ableitung der Funktion aus @exr-LoopProgToFun bestimmt:\n```{}\nl(x) = x^2 + 1\nf(x) = l(l(l(x)))\nfdot = diff(f,x)\nexpand(fdot)\n```\n\n:::{.callout-tip}\n\nAuf der Website kannst du rechts unter `Language` auch `Maxima` auswählen und Maxima-Code ausführen. Maxima ist in Sage integriert.\n\n:::\n\nFür Python gibt es die Bibliothek `sympy`, die ein CAS für Python zur Verfügung stellt. Damit können wir die Funktion aus @exr-LoopProgToFun direkt in Python ableiten:\n\n:::{#exm-symbDiff}\n\n## Symbolische Ableitung\n<br>\nLeite die Funktion $f(x) = l(l(l(x)))$, wobei $l(x) = x^2 + 1$ ist, an der Stelle $x_0 = 1$ ab.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"show\"}\nfrom sympy import symbols, diff\n\ndef f(x):\n    v0 = x\n    v1 = v0 ** 2 + 1\n    v2 = v1 ** 2 + 1\n    v3 = v2 ** 2 + 1\n    y = v3\n    return y\n\nx = symbols('x')\nprint(\"f(x) =\", f(x))\n\ndf = diff(f(x),x)\nprint(\"f'(x) =\", df)\n\nx0 = 1\nprint(\"f'(\" + str(x0) + \") =\", df.evalf(subs={x:x0}))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nf(x) = ((x**2 + 1)**2 + 1)**2 + 1\nf'(x) = 8*x*(x**2 + 1)*((x**2 + 1)**2 + 1)\nf'(1) = 80.0000000000000\n```\n:::\n:::\n\n\n:::\n\n---\n\nDamit erhält man die (bis auf Maschinengenauigkeit) exakten Werte der Ableitungen. Der Grund, warum wir nicht auf symbolische Ausdrücke für Ableitungen zurückgreifen wollen, liegt darin, dass diese Methode bei komplizierten Funktionsausdrücken sehr ineffizient ist, insbesondere dann, wenn wir auch Ableitungen von Funktionen $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ berechnen wollen.\n\n",
    "supporting": [
      "notAD_files\\figure-pdf"
    ],
    "filters": []
  }
}