---
tbl-cap-location: bottom
---


# Adjungierte Algorithmische Differentiation {#sec-AAD}

In @sec-HigherDimFunctions haben wir gesehen, dass wir mit der Standard Algorithmischen Differentiation (SAD) alle $m$ Ableitungen einer Funktion $f : \mathbb{R} \rightarrow \mathbb{R}^m$ mit einem einzigen Funktionsaufruf berechnen können. Die Berechnung des Gradienten einer Funktion $f : \mathbb{R}^n \rightarrow \mathbb{R}$ benötigt jedoch $$n$ Funktionsaufrufe, nämlich einen für jede partielle Ableitung $\partial f / \partial x_i$. In diesem Kapitel wollen wir eine Methode entwickeln, die alle $n$ partiellen Ableitungen in einem Funktionsaufruf berechnet. 

Ähnlich wie die SAD beruht auch diese Methode darauf, dass wir eine komplizierte Funktion Schrittweise mit Hilfe von elementaren Operationen berechnen und dann in jedem Rechenschritt die Ableitung in einer separaten Variablen speichern. Wir führen also wieder unsere Konvention aus Kapitel @sec-ProgFunc ein. Auch dieses Mal werden wir in jedem Schritt die Kettenregel verwenden. Diesmal fangen wir jedoch am Ende der Funktion an und werden uns dann rückwärts durch alle Ableitungen arbeiten. Aus diesem Gründ wird das Verfahren auch Rückwärts-AD ^[Im Englischen spricht man von *reverse mode differentiation* weil *backward differentiation* für bestimmte Methoden zur Integration von Differentialgleichungen verwendet wird.] oder Adjungierte AD (AAD) genannt. 

:::{#exm-firstAADbyHand}

## Gradient mit AAD
<br>

Dieses Beispiel ist eine leicht abgeänderte Version von @sidsite2021 .
Betrachten wir die Funktion $f : \mathbb{R}^2 \rightarrow \mathbb{R}$
$$
y = f(x_0, x_1) = (x_0 + x_1) \cdot x_0 - x_1
$$

Als Programm können wir die Funktion unter Berücksichtigung der Konvention so schreiben:
```{python}
#| code-fold: show
def f(x0, x1):
    v0 = x0
    v1 = x1
    v2 = v0 + v1
    v3 = v2 * v0
    v4 = v3 - v1
    y = v4
    return y

x0, x1 = 2, 3
y0 = f(x0, x1)
print("f(" + str(x0) + "," + str(x1) + ") = " + str(y0))
```

```{dot}
//| label: fig-compTreeMulti
//| fig-cap: Computational Graph für `y = (x0 + x1) * x0 - x1`.
digraph {
    rankdir = LR
    fontname = "Consolas"
    node [fontname = "Cambria", fontsize=8, width=".2", height=".2", margin=".02"];
    edge [fontname = "Cambria", fontsize=8, arrowsize=0.25, len=minlen];
    
    nx0 [label = <x<SUB>0</SUB>>, shape  = none];
    nx1 [label = <x<SUB>1</SUB>>, shape  = none];
    nPlus [label = "+", shape = circle];
    nTimes [label = "*", shape = circle];
    nMinus [label = "-", shape = circle];
    ny [label = "y", shape = none];

    {rank = same; nx0 nx1}
    {rank = same; nPlus}
    {rank = same; nTimes}
    {rank = same; nMinus}
    {rank = same; ny}

    nx0 -> nPlus;
    nx1 -> nPlus;
    nx0 -> nTimes;
    nPlus -> nTimes;
    nx1 -> nMinus;
    nTimes -> nMinus;
    nMinus -> ny
}
```

Die partiellen Ableitungen von $f$ lauten
$$
\begin{align*}
    \frac{\partial y}{\partial x_0} &= \frac{\partial f}{\partial x_0} = (1+0)\cdot x_0 + (x_0 + x_1)\cdot 1 - 0=2x_0 + x_1 \\
    \frac{\partial y}{\partial x_1} &= \frac{\partial f}{\partial x_1} = (0 + 1)\cdot x_0 - 1 = x_0 - 1
\end{align*}
$$
wobei für die Ableitung nach $x_0$ die Produktregel verwendet wurde. Die partielle Ableitung $\partial y / \partial x_0$ können wir auch berechnen, indem wir bei $y = v_4$ anfangen und jeweils die Definition der Hilfsvariablen einsetzen:
$$
\begin{align*}
    \frac{\partial y}{\partial x_0} &= \frac{\partial v_4}{\partial x_0} \\
    &= \frac{\partial (v_3 - v_1)}{\partial x_0} \\
    &= \frac{\partial v_3}{\partial x_0} - \frac{\partial v_1}{\partial x_0} \\
    &= \frac{\partial (v_2 \cdot v_0)}{\partial x_0} - \frac{\partial x_1}{\partial x_0} \\
    &= \frac{\partial v_2}{\partial x_0} \cdot v_0 + v_2 \cdot \frac{\partial v_0}{\partial x_0} - 0 \\
    &= \frac{\partial (v_0 + v_1)}{\partial x_0} \cdot v_0 + v_2 \cdot \frac{\partial x_0}{\partial x_0} \\
    &= \left( \frac{\partial v_0}{\partial x_0} + \frac{\partial v_1}{\partial x_0} \right) \cdot x_0 + (v_0 + v_1) \cdot 1 \\
    &= \left( \frac{\partial x_0}{\partial x_0} + \frac{\partial x_1}{\partial x_0} \right) \cdot x_0 + (x_0 + x_1) \\
    &= (1 + 0) \cdot x_0 + (x_0 + x_1) \\
    &= 2x_0 + x_1   
\end{align*}
$$

Analog findet man (diesmal lassen wir einige der offensichtlicheren Zwischenschritte weg)
$$
\begin{align*}
    \frac{\partial y}{\partial x_1} &= \frac{\partial v_4}{\partial x_1} \\
    &= \frac{\partial v_3}{\partial x_1} - \frac{\partial v_1}{\partial x_1} \\
    &= \frac{\partial (v_2 \cdot v_0)}{\partial x_1} - 1 \\
    &= \frac{\partial v_2}{\partial x_1} \cdot v_0 + v_2 \cdot \frac{\partial v_0}{\partial x_1} - 1 \\
    &= \frac{\partial (v_0 + v_1)}{\partial x_1} \cdot x_0 + v_2 \cdot 0 - 1 \\
    &= \left( \frac{\partial v_0}{\partial x_1} + \frac{\partial v_1}{\partial x_1} \right) \cdot x_0 - 1 \\
    &= ( 0 + 1) \cdot x_0 - 1 \\
    &= x_0 - 1   
\end{align*}
$$

Um die beiden Rechnungen zusammenzufassen, führen nun für jede Hilfsvariable $v_i$ eine neue Variable $\bar v_i$ ein, welche definiert ist als
$$
\bar v_i = \frac{\partial y}{\partial v_i}
$$
Ähnlich wie die $\dot v_i$ aus der SAD speichern diese Variablen die Werte der Ableitungen. Die neue Notation soll anzeigen, dass es sich um die AAD Methode handelt. Unser Ziel ist es also, $\bar v_0$ und $\bar v_1$ zu bestimmen. Beginnen wir in der letzten Zeile des Programms, dann gilt offenbar immer $\bar v_4 = \partial y / \partial v_4 = 1$.  In der Zeile darüber können wir $\bar v_3$ und $\bar v_1$ berechnen, indem wir die Kettenregel verwenden.

\begin{align*}
    \bar v_3 &= \frac{\partial y}{\partial v_3} = \frac{\partial y}{\partial v_4} \cdot \frac{\partial v_4}{\partial v_3} = \bar v_4 \cdot (1-0)=\bar v_4 = 1 \\

    \bar v_1 &= \frac{\partial y}{\partial v_1} = \frac{\partial y}{\partial v_4} \cdot \frac{\partial v_4}{\partial v_1} = \bar v_4 \cdot (0-1)= -1
\end{align*}


Aus der Zeile $v_3 = v_2 \cdot v_0$ lassen sich als nächstes Ausdrücke für $\bar v_2$ und $\bar v_0$ finden.

\begin{align*}
    \bar v_2 &= \frac{\partial y}{\partial v_2} = \frac{\partial y}{\partial v_3} \cdot \frac{\partial v_3}{\partial v_2} = \bar v_3 \cdot v_0 = v_0 \\

    \bar v_0 &= \frac{\partial y}{\partial v_0} = \frac{\partial y}{\partial v_3} \cdot \frac{\partial v_3}{\partial v_0} = \bar v_3 \cdot v_0 = v_2
\end{align*}


In die Variable $v_0$ enthält beim Programmaufruf aber einen konkreten Wert (im obigen Beispiel ist etwa `v0 = x0 = 2`) und ebenso die Variable $v_2$ (nämlich `v2 = x0 + x1 = 5`). Dies sind jedoch erst Zwischenwerte. Wie bei der SAD werden diese im nächsten Schritt aktualisiert. Aus der Zeile $v_2 = v_0 + v_1$ ergibt sich nämlich

\begin{align*}
    \bar v_0 &= \bar v_0 + \frac{\partial y}{\partial v_0} = \bar v_0 + \frac{\partial y}{\partial v_2} \cdot \frac{\partial v_2}{\partial v_0} \\
    &= \bar v_0 + \bar v_2 \cdot (1+0) = v_2 + v_0 \\ & \\

    \bar v_1 &= \bar v_1 + \frac{\partial y}{\partial v_1} = \bar v_1 + \frac{\partial y}{\partial v_2} \cdot \frac{\partial v_2}{\partial v_1} \\
    &= \bar v_1 + \bar v_2 \cdot (0+1) = -1 + v_0
\end{align*}


Nun enthalten die Variablen $\bar v_0$ und $\bar v_1$ die Werte der gewünschten Ableitungen, nämlich $\bar v_0 = (v_0 + v_1) + v_0 = 2x_0 + x_1$ und $\bar v_1 = -1 + x_0$. Wir können aber die letzten Schritte analog zu den vorherigen ausführen:

\begin{align*}
    \bar x_1 &= \frac{\partial y}{\partial x_1} = \frac{\partial y}{\partial v_1} \cdot \frac{\partial v_1}{\partial x_1} = \bar v_1 \cdot 1 = -1 + v_0 \\

    \bar x_0 &= \frac{\partial y}{\partial x_0} = \frac{\partial y}{\partial v_0} \cdot \frac{\partial v_0}{\partial x_0} = \bar v_0 \cdot 1 = v_2 + v_0 \\
\end{align*}


Die Schwierigkeit besteht darin, dass wir nicht wie bei der SAD in jedem Schritt die Variable $v_i$ und gleichzeitig die Variable $\dot v_i$ berechnen können. Um die $\bar v_i$ zu bestimmen muss man zuerst die Funktion komplett ausführen, und sich dabei den Aufbau des Computational Graph merken. Erst dann kann man rückwärts die Ableitungswerte berechnen, angefangen bei der letzten Hilfsvariablen $\bar v_4 = 1$. Das folgende Schema fasst die obigen Rechnungen zusammen.

\begin{equation*}
\left \downarrow 
    \begin{aligned}[c] 
        v_0 &= x_0 \\ 
        v_1 &= x_1 \\
        v_2 &= x_0 + x_1 \\
        v_3 &= v_2 \cdot v_0 \\
        v_4 &= v_3 - v_1 \\
        y &= v_4
    \end{aligned}  
\right .

\qquad

\begin{aligned}[c] 
    & \\ 
    & \\
    & \\
    & \\
    & \\
    &\longrightarrow
\end{aligned}  

\qquad

\left \uparrow 
    \begin{aligned}[c] 
        \bar x_0 &= 2\cdot x_0 + x_1 \\ 
        \bar x_1 &= -1 + x_0 \\
        \bar v_0 &= v_2 + v_0, \quad \bar v_1 = -1 + v_0 \\
        \bar v_2 &= v_0, \quad \bar v_0 = v_2 \\
        \bar v_3 &= 1, \quad \bar v_1 = -1 \\
        \bar v_4 &= \bar y = 1
    \end{aligned}  
\right . 
\end{equation*}

```{python}
#| code-fold: true
def f(x0, x1):
    v0 = x0
    v1 = x1
    v2 = v0 + v1
    v3 = v2 * v0
    v4 = v3 - v1
    y = v4
    v4bar = 1
    v3bar = 1
    v1bar = -1
    v2bar = v0
    v0bar = v2
    v0bar = v0bar + v2bar
    v1bar = v1bar + v2bar
    grad = [v0bar, v1bar]
    return [y, grad]

x0, x1 = 2, 3
[y0, dy] = f(x0, x1)
print("Funktionswert: " + str(y0))
print("Gradient: " + str(dy))
```

Die Werte der Ableitungen $\bar v_i$ lassen sich auch im Computational Graph verfolgen. Entlang eines Weges werden die Werte multipliziert. Führen mehrere Wege zu einer Variablen $x_i$, so werden die Werte der einzelnen Wege addiert.

```{dot}
//| label: fig-compTreeMultiReversed
//| fig-cap: Werte der `vbar`.
digraph {
    rankdir = LR
    fontname = "Consolas"
    node [fontname = "Cambria", fontsize=8, width=".2", height=".2", margin=".02"];
    edge [fontname = "Cambria", fontsize=8, arrowsize=0.25, len=minlen];
    
    nx0 [label = <x<SUB>0</SUB>>, shape  = none];
    nx1 [label = <x<SUB>1</SUB>>, shape  = none];
    nPlus [label = "+", shape = circle];
    nTimes [label = "*", shape = circle];
    nMinus [label = "-", shape = circle];
    ny [label = "y", shape = none];
    
    {rank = same; nx0 nx1}
    {rank = same; nPlus}
    {rank = same; nTimes}
    {rank = same; nMinus}
    {rank = same; ny}
    
    nx0 -> nPlus [dir = back, color = red, fontcolor = red, xlabel = 1];
    nx1 -> nPlus [dir = back, color = red, fontcolor = red, xlabel = 1];
    nx0 -> nTimes [dir = back, color = red, fontcolor = red, xlabel = <v<SUB>2</SUB>>];
    nPlus -> nTimes [dir = back, color = red, fontcolor = red, xlabel = <v<SUB>0</SUB>>];
    nx1 -> nMinus [dir = back, color = red, fontcolor = red, xlabel = -1];
    nTimes -> nMinus [dir = back, color = red, fontcolor = red, xlabel = 1];
    nMinus -> ny [dir = back, color = red, fontcolor = red, xlabel = 1];
}
```

:::
---