---
tbl-cap-location: bottom
---


# Adjungierte Algorithmische Differentiation {#sec-AAD}

In @sec-HigherDimFunctions haben wir gesehen, dass wir mit der Standard Algorithmischen Differentiation (SAD) alle $m$ Ableitungen einer Funktion $f : \mathbb{R} \rightarrow \mathbb{R}^m$ mit einem einzigen Funktionsaufruf berechnen können. Die Berechnung des Gradienten einer Funktion $f : \mathbb{R}^n \rightarrow \mathbb{R}$ benötigt jedoch $n$ Funktionsaufrufe, nämlich einen für jede partielle Ableitung $\partial f / \partial x_i$. In diesem Kapitel wollen wir eine Methode entwickeln, die alle $n$ partiellen Ableitungen in einem Funktionsaufruf berechnet. 

Ähnlich wie die SAD beruht auch diese Methode darauf, dass wir eine komplizierte Funktion schrittweise mit Hilfe von elementaren Operationen berechnen und in jedem Schritt die Ableitungen in separaten Variablen akkumulieren. Wir führen also wieder unsere Konvention aus Kapitel @sec-ProgFunc ein. Auch dieses Mal werden wir in jedem Schritt die Kettenregel verwenden. Diesmal fangen wir jedoch am Ende der Funktion an und werden uns dann rückwärts durch alle Ableitungen arbeiten. Aus diesem Grund wird das Verfahren auch Rückwärts-AD ^[Im Englischen spricht man von *reverse mode differentiation* weil *backward differentiation* für bestimmte Methoden zur Integration von Differentialgleichungen verwendet wird.] oder Adjungierte AD (AAD) genannt. 


## Manuelle Implementation der AAD

Wir erläutern die Methode zuerst an einem einfachen Beispiel, welches eine leicht abgeänderte Version des Beispiels von @sidsite2021 ist.

:::{#exm-firstAADbyHand}

## Gradient mit AAD
<br>

Betrachten wir die Funktion $f : \mathbb{R}^2 \rightarrow \mathbb{R}$
$$
y = f(x_0, x_1) = (x_0 + x_1) \cdot x_0 - x_1
$$

Als Programm können wir die Funktion unter Berücksichtigung der Konvention so schreiben (siehe auch @fig-compTreeMulti):
```{python}
#| code-fold: show
def f(x0, x1):
    v0 = x0
    v1 = x1
    v2 = v0 + v1
    v3 = v2 * v0
    v4 = v3 - v1
    y = v4
    return y

x0, x1 = 2, 3
y0 = f(x0, x1)
print("f(" + str(x0) + "," + str(x1) + ") = " + str(y0))
```

```{dot}
//| label: fig-compTreeMulti
//| fig-cap: Computational Graph für `y = (x0 + x1) * x0 - x1`.
digraph {
    rankdir = LR
    fontname = "Consolas"
    node [fontname = "Cambria", fontsize=8, width=".2", height=".2", margin=".02"];
    edge [fontname = "Cambria", fontsize=8, arrowsize=0.25, len=minlen];
    
    nx0 [label = <x<SUB>0</SUB>>, shape  = none];
    nx1 [label = <x<SUB>1</SUB>>, shape  = none];
    nPlus [label = "+", shape = circle];
    nTimes [label = "*", shape = circle];
    nMinus [label = "-", shape = circle];
    ny [label = "y", shape = none];

    {rank = same; nx0 nx1}
    {rank = same; nPlus}
    {rank = same; nTimes}
    {rank = same; nMinus}
    {rank = same; ny}

    nx0 -> nPlus;
    nx1 -> nPlus;
    nx0 -> nTimes;
    nPlus -> nTimes;
    nx1 -> nMinus;
    nTimes -> nMinus;
    nMinus -> ny
}
```

Die partiellen Ableitungen von $f$ lauten
$$
\begin{align*}
    \frac{\partial y}{\partial x_0} &= \frac{\partial f}{\partial x_0} = (1+0)\cdot x_0 + (x_0 + x_1)\cdot 1 - 0=2x_0 + x_1 \\
    \frac{\partial y}{\partial x_1} &= \frac{\partial f}{\partial x_1} = (0 + 1)\cdot x_0 - 1 = x_0 - 1
\end{align*}
$$
wobei für die Ableitung nach $x_0$ die Produktregel verwendet wurde. Die partielle Ableitung $\partial y / \partial x_0$ können wir auch berechnen, indem wir bei $y = v_4$ anfangen und jeweils die Definition der Hilfsvariablen einsetzen:
$$
\begin{align*}
    \frac{\partial y}{\partial x_0} &= \frac{\partial v_4}{\partial x_0} \\
    &= \frac{\partial (v_3 - v_1)}{\partial x_0} \\
    &= \frac{\partial v_3}{\partial x_0} - \frac{\partial v_1}{\partial x_0} \\
    &= \frac{\partial (v_2 \cdot v_0)}{\partial x_0} - \frac{\partial x_1}{\partial x_0} \\
    &= \frac{\partial v_2}{\partial x_0} \cdot v_0 + v_2 \cdot \frac{\partial v_0}{\partial x_0} - 0 \\
    &= \frac{\partial (v_0 + v_1)}{\partial x_0} \cdot v_0 + v_2 \cdot \frac{\partial x_0}{\partial x_0} \\
    &= \left( \frac{\partial v_0}{\partial x_0} + \frac{\partial v_1}{\partial x_0} \right) \cdot x_0 + (v_0 + v_1) \cdot 1 \\
    &= \left( \frac{\partial x_0}{\partial x_0} + \frac{\partial x_1}{\partial x_0} \right) \cdot x_0 + (x_0 + x_1) \\
    &= (1 + 0) \cdot x_0 + (x_0 + x_1) \\
    &= 2x_0 + x_1   
\end{align*}
$$

Analog findet man $\partial y / \partial x_1$ (diesmal lassen wir einige der offensichtlicheren Zwischenschritte weg):
$$
\begin{align*}
    \frac{\partial y}{\partial x_1} &= \frac{\partial v_4}{\partial x_1} \\
    &= \frac{\partial v_3}{\partial x_1} - \frac{\partial v_1}{\partial x_1} \\
    &= \frac{\partial (v_2 \cdot v_0)}{\partial x_1} - 1 \\
    &= \frac{\partial v_2}{\partial x_1} \cdot v_0 + v_2 \cdot \frac{\partial v_0}{\partial x_1} - 1 \\
    &= \frac{\partial (v_0 + v_1)}{\partial x_1} \cdot x_0 + v_2 \cdot 0 - 1 \\
    &= \left( \frac{\partial v_0}{\partial x_1} + \frac{\partial v_1}{\partial x_1} \right) \cdot x_0 - 1 \\
    &= ( 0 + 1) \cdot x_0 - 1 \\
    &= x_0 - 1   
\end{align*}
$$

Um die beiden Rechnungen zusammenzufassen, führen nun für jede Hilfsvariable $v_i$ eine neue Variable $\bar v_i$ ein, welche definiert ist als
$$
\bar v_i = \frac{\partial y}{\partial v_i}
$$
Ähnlich wie die $\dot v_i$ aus der SAD speichern diese Variablen die Werte der Ableitungen. Die neue Notation soll anzeigen, dass es sich um die AAD Methode handelt. Unser Ziel ist es also, $\bar v_0 = \partial y / \partial v_0 = \partial y / \partial x_0$ und $\bar v_1 = \partial y / \partial v_1 = \partial y / \partial x_1$ zu bestimmen. Beginnen wir in der letzten Zeile des Programms, dann gilt offenbar immer $\bar v_4 = \partial y / \partial v_4 = 1$.  In der Zeile darüber können wir $\bar v_3$ und $\bar v_1$ berechnen, indem wir die Kettenregel verwenden.

\begin{align*}
    \bar v_3 &= \frac{\partial y}{\partial v_3} = \frac{\partial y}{\partial v_4} \cdot \frac{\partial v_4}{\partial v_3} = \bar v_4 \cdot (1-0)=\bar v_4  \\

    \bar v_1 &= \frac{\partial y}{\partial v_1} = \frac{\partial y}{\partial v_4} \cdot \frac{\partial v_4}{\partial v_1} = \bar v_4 \cdot (0-1)= -\bar v_4
\end{align*}


Also sind $\bar v_3 = 1$ und $\bar v_1 = -1$. Der Zwischenwert in $\bar v_1$ wird später ergänzt werden. 
Aus der Zeile $v_3 = v_2 \cdot v_0$ lassen sich als nächstes Ausdrücke für $\bar v_2$ und $\bar v_0$ finden.

\begin{align*}
    \bar v_2 &= \frac{\partial y}{\partial v_2} = \frac{\partial y}{\partial v_3} \cdot \frac{\partial v_3}{\partial v_2} = \bar v_3 \cdot v_0 \\

    \bar v_0 &= \frac{\partial y}{\partial v_0} = \frac{\partial y}{\partial v_3} \cdot \frac{\partial v_3}{\partial v_0} = \bar v_3 \cdot v_2 
\end{align*}

Mit den vorher berechneten Werten erhalten wir also $\bar v_2 = v_0$ und $\bar v_0 = v_2$. Beide Werte sind durch die Funktion bereits berechnet worden. Im obigen Beispiel gilt etwa `v0 = x0 = 2` und `v2 = x0 + x1 = 5`. Auch diese Zwischenwerte werden im nächsten Schritt ergänzt. Aus der Zeile $v_2 = v_0 + v_1$ ergibt sich nämlich

\begin{align*}
    \bar v_0 &= \bar v_0 + \frac{\partial y}{\partial v_0} = \bar v_0 + \frac{\partial y}{\partial v_2} \cdot \frac{\partial v_2}{\partial v_0} \\
    &= \bar v_0 + \bar v_2 \cdot (1+0) = \bar v_0 + \bar v_2 \\ & \\

    \bar v_1 &= \bar v_1 + \frac{\partial y}{\partial v_1} = \bar v_1 + \frac{\partial y}{\partial v_2} \cdot \frac{\partial v_2}{\partial v_1} \\
    &= \bar v_1 + \bar v_2 \cdot (0+1) = \bar v_1 + \bar v_2
\end{align*}

Mit den bereits bekannten Werten erhalten wir $\bar v_0 = v_2 + v_0$ (bzw. mit den konkreten Werten des Beispiels `v0bar = 5 + 2`) und $\bar v_1 = -1 + v_0$ (bzw. `v1bar = -1 + 2`).
Nun enthalten die Variablen $\bar v_0$ und $\bar v_1$ die Werte der gewünschten Ableitungen, nämlich $\bar v_0 = (v_0 + v_1) + v_0 = 2x_0 + x_1$ und $\bar v_1 = -1 + x_0$. Wir können aber die letzten Schritte analog zu den vorherigen ausführen:

\begin{align*}
    \bar x_1 &= \frac{\partial y}{\partial x_1} = \frac{\partial y}{\partial v_1} \cdot \frac{\partial v_1}{\partial x_1} = \bar v_1 \cdot 1  \\

    \bar x_0 &= \frac{\partial y}{\partial x_0} = \frac{\partial y}{\partial v_0} \cdot \frac{\partial v_0}{\partial x_0} = \bar v_0 \cdot 1  \\
\end{align*}


Die Schwierigkeit besteht darin, dass wir nicht wie bei der SAD in jedem Schritt die Variable $v_i$ und gleichzeitig die Variable $\dot v_i$ berechnen können. Um die $\bar v_i$ zu bestimmen muss man zuerst die Funktion komplett ausführen, und sich dabei den Aufbau des Computational Graph merken. Erst dann kann man rückwärts die Ableitungswerte berechnen, angefangen bei der letzten Hilfsvariablen $\bar v_4 = 1$. Das folgende Schema fasst die obigen Rechnungen zusammen.

\begin{equation*}
\left \downarrow 
    \begin{aligned}[c] 
        v_0 &= x_0 \\ 
        v_1 &= x_1 \\
        v_2 &= v_0 + v_1 \\
        v_3 &= v_2 \cdot v_0 \\
        v_4 &= v_3 - v_1 \\
        y &= v_4
    \end{aligned}  
\right .

\qquad

\begin{aligned}[c] 
    & \\ 
    & \\
    & \\
    & \\
    & \\
    &\longrightarrow
\end{aligned}  

\qquad

\left \uparrow 
    \begin{aligned}[c] 
        \bar x_0 &= \bar v_0 = 2\cdot x_0 + x_1 \\ 
        \bar x_1 &= \bar v_1 = -1 + x_0 \\
        \bar v_0 &= \bar v_0 + \bar v_2, \quad \bar v_1 = \bar v_1 + \bar v_2 \\
        \bar v_2 &= \bar v_3 \cdot v_0, \quad \bar v_0 = \bar v_3 \cdot v_2 \\
        \bar v_3 &= \bar v_4, \quad \bar v_1 = -\bar v_4 \\
        \bar v_4 &= \bar y = 1
    \end{aligned}  
\right . 
\end{equation*}

```{python}
#| code-fold: true
def f(x0, x1):
    v0 = x0
    v1 = x1
    v2 = v0 + v1
    v3 = v2 * v0
    v4 = v3 - v1
    y = v4
    v4bar = 1
    v3bar = v4bar
    v1bar = -v4bar
    v2bar = v3bar * v0
    v0bar = v3bar * v2
    v0bar = v0bar + v2bar
    v1bar = v1bar + v2bar
    grad = [v0bar, v1bar]
    return [y, grad]

x0, x1 = 2, 3
[y0, dy] = f(x0, x1)
print("Funktionswert: " + str(y0))
print("Gradient: " + str(dy))
```

Die Werte der Ableitungen $\bar v_i$ lassen sich auch im Computational Graph verfolgen, siehe @fig-compTreeMultiReversed. Der Wert bei der Kante von $v_i$ nach $v_j$ entspricht der partiellen Ableitung $\partial v_i / \partial v_j$. Entlang eines Weges werden die Werte multipliziert. Führen mehrere Wege zu einer Variablen $x_i$, so werden die Werte der einzelnen Wege addiert.

```{dot}
//| label: fig-compTreeMultiReversed
//| fig-cap: Werte der Hilfsvariablen `vbar`.
digraph {
    rankdir = LR
    fontname = "Consolas"
    node [fontname = "Cambria", fontsize=8, width=".2", height=".2", margin=".02"];
    edge [fontname = "Cambria", fontsize=8, arrowsize=0.25, len=minlen];
    
    nx0 [label = <x<SUB>0</SUB>>, shape  = none];
    nx1 [label = <x<SUB>1</SUB>>, shape  = none];
    nPlus [label = "+", shape = circle];
    nTimes [label = "*", shape = circle];
    nMinus [label = "-", shape = circle];
    ny [label = "y", shape = none];
    
    {rank = same; nx0 nx1}
    {rank = same; nPlus}
    {rank = same; nTimes}
    {rank = same; nMinus}
    {rank = same; ny}
    
    nx0 -> nPlus [dir = back, color = red, fontcolor = red, xlabel = 1];
    nx1 -> nPlus [dir = back, color = red, fontcolor = red, xlabel = 1];
    nx0 -> nTimes [dir = back, color = red, fontcolor = red, xlabel = <v<SUB>2</SUB>>];
    nPlus -> nTimes [dir = back, color = red, fontcolor = red, xlabel = <v<SUB>0</SUB>>];
    nx1 -> nMinus [dir = back, color = red, fontcolor = red, xlabel = -1];
    nTimes -> nMinus [dir = back, color = red, fontcolor = red, xlabel = 1];
    nMinus -> ny [dir = back, color = red, fontcolor = red, xlabel = 1];
}
```

:::
---

:::{#exr-EigeneAADBeispiele1}

## Eigene Beispiele finden
<br>

Schreibe eine eigene Funktion $f : \mathbb{R}^n \rightarrow \mathbb{R}$ für $n\in\lbrace 2, 3 \rbrace$ hin und erstelle den Computational Graph und ein Programm. Leite das Programm nach der oben beschriebenen AAD Methode ab und überzeuge dich an verschiedenen Stellen davon, dass der Gradient korrekt ist.

:::

:::{.callout-tip collapse="true"}

## Lösung
Beispiele findet man in der Literatur, z.B. bei @sidsite2021, @Slater2022, @Baydin18 (S. 13), @Griewank2008EDP (S. 9, S. 42) oder @Henrard2017ADi (S. 24).

:::





## Implementation der AAD mit Operator Overloading

Nun wollen wir ähnlich wie im @sec-SadImplementationOperatorOverloading eine Klasse `FloatAad` entwerfen, welche die Berechnung aller Hilfsvariablen `vbar` automatisch ausführt. Wie auch zuvor hat jedes `FloatAad`-Objekt ein Attribut `value` vom Typ `Int` oder `Float`. Allerdings reicht es nicht mehr aus, ein `Float`-Attribut `derivative` zu definieren, um den Wert der Ableitung zu speichern weil auch die Struktur des Computational Graph gespeichert werden muss. Als Attribut `derivatives` wählen wir ein `tuple`, dessen erster Eintrag ein `FloatAad`-Objekt ist, nämlich die Variable, nach der die partielle Ableitung berechnet wird. Der zweite Eintrag ist der Wert dieser partiellen Ableitung. Da das erste Element des Tupels selber auch ein Attribut `derivatives` hat, entsteht so eine rekursive Darstellung des Computational Graph. 

Die folgende Implementation lehnt sich stark an @sidsite2021 an. Die Variablennamen wurden angepasst, so dass sie konsistent mit den Bezeichnungen aus @sec-SADforOneDimFunctions sind. Ausserdem werden wir unsere Klasse noch mit einiger zusätzlicher Funktionalität ausstatten, etwa mit Typunterscheidungen, so dass wir z.B. auch wieder `Int`-Zahlen zu `FloatAad`-Objekten addieren können.

### Die Klasse `FloatAad`

Wir beginnen unsere Klasse mit einer neuen Datei, welche wir `floataad.py` nennen. Als erstes definieren wir einen Konstruktor, der uns das Umwandeln von `Int`- oder `Float`-Objekten in `FloatAad`-Objekte erlaubt. Ausserdem definieren wir auch gleich eine Funktion, mit der wir eine Liste von solchen `Int` oder `Float` in eine Liste von `FloatAad` umwandeln können, siehe dazu @sec-FunktionenMehrereInputs.
```{python}
#| code-fold: show
import numpy as np

class FloatAad:

    def __init__(self, value, derivatives = ()):
        self.value = value
        self.derivatives = derivatives

float2FloatAad = np.vectorize(lambda x: FloatAad(x))


if __name__ == '__main__':

    x = FloatAad(2)
    print(x.value)
    print(x.derivatives)
    print("")

    x = [1,2]
    v = float2FloatAad(x)
    print(type(v))
    print(type(v[0]))
    print(type(v[0].value))
    print(type(v[0].derivatives))
```

### Vorzeichen

Wir gehen bei der Implementation der unären und binären Operatoren etwas anders vor als im @sec-SadImplementationOperatorOverloading . Wir definieren zunächst Funktionen für die Operationen und benutzen diese, um die Operatoren zu überladen.
Für das negative Vorzeichen sieht das so aus:

```{python}
#| code-fold: show
import numpy as np

class FloatAad:

    def __init__(self, value, derivatives = ()):
        self.value = value
        self.derivatives = derivatives

    def __pos__(self):
        return self

    def __neg__(self):
        return neg(self)

float2FloatAad = np.vectorize(lambda x: FloatAad(x))

def neg(a):
    newValue = -1 * a.value
    newDerivative = (
        (a, -1),
    )
    return FloatAad(newValue, newDerivative)


if __name__ == '__main__':

    x = FloatAad(2)
    v = -x

    print(v.value)
    print(v.derivatives)
```
Der Wert von `v.derivatives` ist ein Tupel, dessen erster Eintrag eine Referenz auf das `FloatAad`-Objekt `x` ist und der Wert des zweiten Eintrags ist `-1` weil $\partial v / \partial x = -1$ ist.

### Die Operatoren `+` und `-`

Wenn wir zwei `FloatAad`-Objekte `a` und `b` addieren, dann müssen wir zwei Tupel als Ableitung zurückgeben, nämlich für
$$
\frac{\partial}{\partial a}(a+b)=1 \qquad\textrm{und für}\qquad \frac{\partial}{\partial b}(a+b)=1
$$

Die entsprechende Funktion sieht so aus:
```{python}
#| code-fold: show
#| eval: false
def add(a, b):
    newValue = a.value + b.value
    newDerivative = (
        (a, 1),  # a+b nach a abgeleitet gibt 1
        (b, 1)   # a+b nach b abgeleitet gibt 1
    )
    return FloatAad(newValue, newDerivative)
```

Für das Überladen des `+`-Operators geben wir dann einfach `return add(self, other)` zurück. Wir wollen bei dieser Gelegenheit aber gleich noch die Typabfrage implementieren, so dass wir nicht nur zwei `FloatAad`-Objekte addieren können, sondern auch Ausdrücke wie `x + 1` schreiben können. In diesem Fall brauchen wir für die `newDerivative` nur ein Tupel, welches wir direkt in der Funktion `__add__` bestimmen. Der `__radd__`-Operator, mit dem wir einen Ausdruck wie `1 + x` schreiben können, wird analog definiert.
```{python}
#| code-fold: show
#| eval: false
def __add__(self, other):
    if type(other) in [int, float]:
        newValue = self.value + other
        newDerivative = (
            (self, 1),
        )
        return FloatAad(newValue, newDerivative)
    else:
        return add(self, other)
    
def __radd__(self, other):
    if type(other) in [int, float]:
        newValue = other + self.value
        newDerivative = (
            (self, 1),
        )
        return FloatAad(newValue, newDerivative)
    else:
        return add(other, self)
```

:::{#exr-AadMinusOp}

## Den Operator `-` implementieren
<br>

Implementiere die Funktionen `__sub__` und `__rsub__`. Du kannst dafür die Funktionen `neg(a)` und `add(a,b)` verwenden.

:::

:::{.callout-tip collapse="true"}

## Lösung

```{python}
#| code-fold: true
def __sub__(self, other):
    if type(other) in [int, float]:
        newValue = self.value - other
        newDerivative = (
            (self, 1),
        )
        return FloatAad(newValue, newDerivative)
    else:
        return add(self, neg(other))
        
def __rsub__(self, other):
    if type(other) in [int, float]:
        newValue = other - self.value
        newDerivative = (
            (self, -1),
        )
        return FloatAad(newValue, newDerivative)
    else:
        return add(other, neg(self)) 
```

:::


### Gradienten berechnen

Hier ist die bisher implementierte Klasse zusammen mit einem kleinen Testprogramm, welches die Funktion $f(x_0, x_1) = 2x_0 - x_1 + 5$ berechnet.

```{python}
#| code-fold: true
import numpy as np

class FloatAad:

    def __init__(self, value, derivatives = ()):
        self.value = value
        self.derivatives = derivatives

    def __pos__(self):
        return self

    def __neg__(self):
        return neg(self)

    def __add__(self, other):
        if type(other) in [int, float]:
            newValue = self.value + other
            newDerivative = (
                (self, 1),
            )
            return FloatAad(newValue, newDerivative)
        else:
            return add(self, other)
        
    def __radd__(self, other):
        if type(other) in [int, float]:
            newValue = other + self.value
            newDerivative = (
                (self, 1),
            )
            return FloatAad(newValue, newDerivative)
        else:
            return add(other, self)
    
    def __sub__(self, other):
        if type(other) in [int, float]:
            newValue = self.value - other
            newDerivative = (
                (self, 1),
            )
            return FloatAad(newValue, newDerivative)
        else:
            return add(self, neg(other))
        
    def __rsub__(self, other):
        if type(other) in [int, float]:
            newValue = other - self.value
            newDerivative = (
                (self, -1),
            )
            return FloatAad(newValue, newDerivative)
        else:
            return add(other, neg(self)) 

float2FloatAad = np.vectorize(lambda x: FloatAad(x))

def neg(a):
    newValue = -1 * a.value
    newDerivative = (
        (a, -1),
    )
    return FloatAad(newValue, newDerivative)

def add(a, b):
    newValue = a.value + b.value
    newDerivative = (
        (a, 1),  # a+b nach a abgeleitet gibt 1
        (b, 1)   # a+b nach b abgeleitet gibt 1
    )
    return FloatAad(newValue, newDerivative)

if __name__ == '__main__':

    x0 = FloatAad(2)
    x1 = FloatAad(3)
    y = x0 + x0 - x1 + 5

    print(y.value)
    print(y.derivatives)
    print(y.derivatives[0][0].derivatives)
```

Wir sehen, dass der Funktionswert $f(2,3) = 6$ korrekt ist. Als Ableitung sehen wir jedoch nur ein Tupel bestehend aus einer Referenz auf ein `FloatAad`-Objekt und einem Zwischenschritt bei der Berechnung der Ableitung. Auch die Ableitung des referenzierten `FloatAad`-Objekts enthält nur ein weiteres solches Tupel. Mit anderen Worten, wir sehen noch nirgends den Wert der partiellen Ableitungen $\partial f / \partial x_0 = 2x_0 = 4$ und $\partial f / \partial x_1 = -1$. Wir schreiben dafür nun eine Funktion `getDerivatives(y)`, welche aus dem `FloatAad`-Objekt `y` rekursiv die partiellen Ableitungen berechnet. Dies geschieht nach der Regel, dass die Zwischenwerte der Ableitungen entlang eines Weges im Computational Graph multipliziert werden und Werte von verschiedenen Wegen, die zur gleichen Variablen $x_i$ führen, addiert werden. Für diese rekursive Berechnung definieren wir eine lokale Funktion `computeDerivative`. 
Der Rückgabewert soll dann ein Dictionary sein (`defaultdict` aus dem Modul `collections`, welches zu Beginn importiert werden muss), dessen Schlüsselwerte die Variablen `x0, x1` etc. sind und die zugehörigen Werte sind die partiellen Ableitungen $\partial f / \partial x_i$. 

```{python}
#| code-fold: show
#| eval: false
def getDerivatives(y):
    dy = defaultdict(lambda: 0)

    def computeDerivatives(y, pathValue):
        for node, localDerivative in y.derivatives:
            # Multipliziere entlang eines Weges im Graph
            valueOfPathToNode = pathValue * localDerivative
            # Addiere entlang unterschiedlicher Wege
            dy[node] = dy[node] + valueOfPathToNode
            # Rekursion zum Durchlaufen des ganzen Graphen
            computeDerivatives(node, valueOfPathToNode)

    # Initialisierung mit 1 (Ableitung von y nach y)
    computeDerivatives(y, pathValue = 1)
    return dy
```

Die partiellen Ableitungen können nun mit `dy = getDerivatives(y)` berechnet und mit `dy[x0]`, bzw. `dy[x1]` ausgegeben werden.

Wenn der Input der Funktion eine Liste `x0` ist, dann möchten wir noch eine Funktion `getGradient(x0, y)` haben, welche den Gradienten von `y` in Form einer Liste zurückgibt. 
```{python}
#| code-fold: show
#| eval: false
def getGradient(x0, y):
    dy = getDerivatives(y)
    grad = []
    for i in range(len(x0)):
        grad.append(dy[x0[i]])
    return grad
```

Mit diesen Befehlen können wir nun unsere Programme testen.

```{python}
#| code-fold: show
#| echo: true
#| eval: false
if __name__ == '__main__':

    x = [2, 3]
    x = float2FloatAad(x)
    
    y = x[0] + x[0] - x[1] + 5
    dy = getGradient(x, y)

    print(y.value)
    print(dy)
```


```{python}
#| echo: false
#| eval: true
import numpy as np
from collections import defaultdict

class FloatAad:

    def __init__(self, value, derivatives = ()):
        self.value = value
        self.derivatives = derivatives

    def __pos__(self):
        return self

    def __neg__(self):
        return neg(self)

    def __add__(self, other):
        if type(other) in [int, float]:
            newValue = self.value + other
            newDerivative = (
                (self, 1),
            )
            return FloatAad(newValue, newDerivative)
        else:
            return add(self, other)
        
    def __radd__(self, other):
        if type(other) in [int, float]:
            newValue = other + self.value
            newDerivative = (
                (self, 1),
            )
            return FloatAad(newValue, newDerivative)
        else:
            return add(other, self)
    
    def __sub__(self, other):
        if type(other) in [int, float]:
            newValue = self.value - other
            newDerivative = (
                (self, 1),
            )
            return FloatAad(newValue, newDerivative)
        else:
            return add(self, neg(other))
        
    def __rsub__(self, other):
        if type(other) in [int, float]:
            newValue = other - self.value
            newDerivative = (
                (self, -1),
            )
            return FloatAad(newValue, newDerivative)
        else:
            return add(other, neg(self)) 

float2FloatAad = np.vectorize(lambda x: FloatAad(x))

def neg(a):
    newValue = -1 * a.value
    newDerivative = (
        (a, -1),
    )
    return FloatAad(newValue, newDerivative)
    
def add(a, b):
    newValue = a.value + b.value
    newDerivative = (
        (a, 1),  # a+b nach a abgeleitet gibt 1
        (b, 1)   # a+b nach b abgeleitet gibt 1
    )
    return FloatAad(newValue, newDerivative)

def getDerivatives(y):
    dy = defaultdict(lambda: 0)

    def computeDerivatives(y, pathValue):
        for node, localDerivative in y.derivatives:
            # Multipliziere entlang eines Weges im Graph
            valueOfPathToNode = pathValue * localDerivative
            # Addiere entlang unterschiedlicher Wege
            dy[node] = dy[node] + valueOfPathToNode
            # Rekursion zum Durchlaufen des ganzen Graphen
            computeDerivatives(node, valueOfPathToNode)

    # Initialisierung mit 1 (Ableitung von y nach y)
    computeDerivatives(y, pathValue = 1)
    return dy

def getGradient(x0, y):
    dy = getDerivatives(y)
    grad = []
    for i in range(len(x0)):
        grad.append(dy[x0[i]])
    return grad

if __name__ == '__main__':

    x = [2, 3]
    x = float2FloatAad(x)
    
    y = x[0] + x[0] - x[1] + 5
    dy = getGradient(x, y)

    print(y.value)
    print(dy)
```


### Die Operatoren `*` und `/`

Wenn wir zwei `FloatAad`-Objekte `a` und `b` multiplizieren, dann lauten die partiellen Ableitungen
$$
\frac{\partial}{\partial a}(a \cdot b)=b \qquad\textrm{und}\qquad \frac{\partial}{\partial b}(a \cdot b)=a
$$

Wir erzeugen also wieder zwei Tupel für die Ableitung.
```{python}
#| code-fold: show
#| eval: false
def mul(a, b):
    newValue = a.value * b.value
    newDerivative = (
        (a, b.value),  # a*b nach a abgeleitet gibt b
        (b, a.value)   # a*b nach b abgeleitet gibt a
    )
    return FloatAad(newValue, newDerivative)
```

Damit überladen wir nun den `*`-Operator:
```{python}
#| code-fold: show
#| eval: false
def __mul__(self, other):
    if type(other) in [int, float]:
        newValue = self.value * other
        newDerivative = (
            (self, other), 
        )
        return FloatAad(newValue, newDerivative)
    else:
        return mul(self, other)
        
def __rmul__(self, other):
    if type(other) in [int, float]:
        newValue = other * self.value
        newDerivative = (
            (self, other), 
        )
        return FloatAad(newValue, newDerivative)
    else:
        return mul(other, self)
```

:::{#exr-AadDivOp}

## Den Operator `/` implementieren
<br>

Ähnlich wie in @exr-AadMinusOp können wir die Division mit Hilfe der Funktion `mul(a,b)` realisieren. Schreibe dafür eine Funktion `inv(a)`, welche in $\frac{1}{a}$ als `FloatAad`-Objekt berechnet. 
Implementiere damit die Funktionen `__truediv__` und `__rtruediv__`.

:::

:::{.callout-tip collapse="true"}

## Lösung

Die Funktion `inv(a)`:
```{python}
#| code-fold: true
def inv(a):
    newValue = 1. / a.value
    newDerivative = (
        (a, -1. / a.value**2), 
    )
    return FloatAad(newValue, newDerivative)
```
Und damit der `/`-Operator
```{python}
#| code-fold: true
def __truediv__(self, other):
    if type(other) in [int, float]:
        newValue = self.value / other
        newDerivative = (
            (self, 1 / other),
        )
        return FloatAad(newValue, newDerivative)
    else:
        return mul(self, inv(other))
        
def __rtruediv__(self, other):
    if type(other) in [int, float]:
        newValue = other / self.value
        newDerivative = (
            (self, - other / math.pow(self.value,2)),
        )
        return FloatAad(newValue, newDerivative)
    else:
        return mul(other, inv(self))
```

:::

### Der Operator `**`

Es fehlt nun nur noch der Potenzoperator. Die partiellen Ableitungen von $a^b$ lauten
$$
\frac{\partial}{\partial a}(a^b)=b\cdot a^{b-1} \qquad\textrm{und}\qquad \frac{\partial}{\partial b}(a^b)=a^b \cdot \ln(a)
$$

Wieder definieren wir uns zuerst eine Funktion
```{python}
#| code-fold: show
#| eval: false
def pow(a, b):
    newValue = math.pow(a.value,b.value)
    newDerivative = (
        (a, b.value * math.pow(a.value, b.value-1)), # a^b nach a abgeleitet gibt b*a^(b-1)
        (b, math.pow(a.value, b.value) * math.log(a.value))  # a^b nach b abgeleitet gibt a^b * ln(a)
    )
    return FloatAad(newValue, newDerivative)
```
und benutzen diese zur Überladung des `**`-Operators:
```{python}
#| code-fold: show
#| eval: false
def __pow__(self, other):
    if type(other) in [int, float]:
        newValue = math.pow(self.value, other)
        newDerivative = (
            (self, other * math.pow(self.value, other - 1)),
        ) 
        return FloatAad(newValue, newDerivative)           
    else:
        return pow(self, other)
    
def __rpow__(self, other):
    if type(other) in [int, float]:
        newValue = math.pow(other, self.value)
        newDerivative = (
            (self, math.pow(other, self.value) * math.log(other)),
        )
        return FloatAad(newValue, newDerivative)
    else:
        return pow(other, self)
```

Die fertige Klasse `FloatAad` von [hier](floataad.py) kopiert werden.


## Die Klasse `FloatAad` im Einsatz

{{< include _floataadForExecution.qmd >}}

Wir sind nun in der Lage, Gradienten von Funktionen zu berechnen, solange darin noch keine Ausdrücke mit $\sin$ oder $\ln$ etc. vorkommen.

:::{#exm-gradientsWithAAD}

## Gradient mit `FloatAad`
<br>

Betrachte die Funktion
$$
f(x_0, x_1, x_2) = x_0 \cdot x_1^2 + \frac{2 ^{x_1}}{x_2} - \frac{2}{x_2^2}
$$
Der Gradient lautet
$$
\nabla f = \begin{pmatrix} 
    x_1^2  , \;
    2x_0x_1 + \frac{2^{x_1}\cdot\ln(2)}{x_2} , \;
    -\frac{2^{x_1}}{x_2^2}+\frac{4}{x_2^3}
    \end{pmatrix}
$$

Werten wir die Funktion an der Stelle $(x_0, x_1, x_2) = (3, 2, -1)$ aus, dann erhalten wir $f(3, 2, -1) = 6$ und 

\begin{align*}
\nabla f|_{(3, 2 -1)} &= \begin{pmatrix} 
    4  , \;
    12-4\cdot \ln(2) ,\;
     -8
    \end{pmatrix} \\
    &\approx \begin{pmatrix} 
    4  , \;
    9.23 , \;
     -8
    \end{pmatrix}
\end{align*}

Testen wir dies mit unserer Klasse:

```{python}
#| code-fold: show
from floataad import float2FloatAad, getGradient

def f(x):
    v1 = x[0] * x[1]**2
    v2 = 2**x[1] / x[2]
    v3 = 2 / x[2]**2
    y = v1 + v2 - v3
    return y

x0 = [3,2,-1]
x0 = float2FloatAad(x0)

y = f(x0)
y0 = y.value
dy = getGradient(x0, y)

print("Funktionswert: " + str(y0))
print("Gradient: " + str(dy))
```

:::
---

Wir können mit der Klasse `FloatAad` auch die Jacobi Matrix einer Funktion $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$ bestimmen. Ähnlich wie in @sec-FuncRnToRm benötigen wir dazu jedoch mehrere Durchgänge. Diesmal muss jede Zeile der Matrix $Jf$ einzeln berechnet werden. Dafür müssen wir die Methode `getGradient` $m$ Mal aufrufen, nämlich einmal für jedes `y[i]`. Um dies zu automatisieren definieren wir uns wieder zwei Funktionen `getValues` und `getJacobian` (welche wir natürlich auch in die Klasse `FloatAad` schreiben könnten). Während `FloatSad` günstig war, solange $n<m$ war, so ist  `FloatAad` günstig, wenn $n>m$ ist.

:::{#exm-JacobianWithAad}

## Jacobi Matrix mit `FloatAad`
<br>

Wir betrachten die Funktion $f : \mathbb{R}^3 \rightarrow \mathbb{R}^2$ mit
$$
f(x_0, x_1, x_2) = 
    \begin{pmatrix}
        y_0 \\ y_1
    \end{pmatrix}
    =
    \begin{pmatrix}
        x_0 + x_1^2 + \frac{1}{x_2} \\
        x_0 \cdot x_1 \cdot x_2
    \end{pmatrix}
$$

Die Jacobi Matrix lautet
\begin{align*}
Jf &= 
    \begin{pmatrix}
        \frac{\partial y_0}{\partial x_0} & \frac{\partial y_0}{\partial x_1} & \frac{\partial y_0}{\partial x_2} \\
        \frac{\partial y_1}{\partial x_0} & \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2}
    \end{pmatrix} \\
    &=
    \begin{pmatrix}
        1 & 2x_1 & -\frac{1}{x_2 ^2} \\
        x_1 x_2 & x_0 x_2 & x_0 x_1
    \end{pmatrix}
\end{align*}

Ausgewertet an der Stelle $(x_0, x_1, x_2) = (-2, -4, 0.5)$ erhalten wir $f(-2, -4, 0.5) = (16, 4)^\intercal$ und 
$$
Jf|_{(-2,-4,0.5)} = 
\begin{pmatrix}
    1 & -8 & -4 \\
    -2 & -1 & 8
\end{pmatrix}
$$

Als Programm mit den oben beschriebenen Funktionen erhalten wir
```{python}
#| code-fold: show
from floataad import float2FloatAad, getGradient
import numpy as np

def f(x):
    y0 = x[0] + x[1]**2 + 1/x[2]
    y1 = x[0] * x[1] * x[2]
    return [y0, y1]

x0 = [-2,-4,0.5]
x0 = float2FloatAad(x0)

getValues = np.vectorize(lambda y : y.value)
getJacobian = lambda x,y : np.array([getGradient(x, y[i]) for i in range(len(y))])

y = f(x0)

val = getValues(y)
Jacobian = getJacobian(x0, y)

print("Funktionswert: " +str(val))
print("Jf =")
print(Jacobian)
```

:::
---

### Gradient Descent zum Auffinden lokaler Minima

Eine Funktion $f : \mathbb{R}^2 \rightarrow \mathbb{R}$ kann man sich als eine i.A. gekrümmte Fläche im $\mathbb{R}^3$ vorstellen. Ist die Funktion differenzierbar, dann diese Fläche in einer Umgebung eines Punktes $(x_0, x_1, f(x_0, x_1))$ beliebig gut durch eine Tangentialebene angenähert werden. Der Gradient $\nabla f|_{(x_0, x_1)}$ stellt ein Vektor in dieser Tangentialebene dar, und zwar zeigt er gerade in die Richtung des steilsten Anstiegs der Fläche. Entsprechend zeigt $-\nabla f$ in die Richtung des steilsten Abstiegs, siehe @Arens2022, S. 871. Damit lässt sich die Gradient Descent Methode zur Berechnung eines lokalen Minimums, die wir im @sec-gradientDescent kennen gelernt haben, auch zum Auffinden eines lokalen Minimums einer Fläche verwenden. Sie lässt sich sogar auf Funktionen $f : \mathbb{R}^n \rightarrow \mathbb{R}$ anwenden, siehe @Arens2022, S. 1324.

Im Fall einer 2-dimensionalen Fläche starten wir an einem Punkt $(x_{0,0}, x_{1,0})$, welchen wir durch seinen Ortsvektor $\vec{x}_0$ beschreiben. Dann berechnen wir rekursiv eine Folge von (Ortsvektoren zu) Punkten $\vec{x}_n$, welche im Idealfall zu einem lokalen Minimum der Funktion konvergieren, gemäss der Vorschrift
$$
\vec{x}_{n+1} = \vec{x}_n - \lambda\cdot(\nabla f(\vec{x}_n))^\intercal
$$

$\lambda\in\mathbb{R}$ beeinflusst wie im @sec-gradientDescent die Schrittweite. Ist $\lambda$ zu klein, dann konvergiert die Iteration nur sehr langsam, wird $\lambda$ hingegen zu gross gewählt, dann kann es passieren, dass sich die Iteration von einem lokalen Minimum weg bewegt. 

:::{#exm-gradientDescentWithAAD}

## Gradient Descent auf einer Fläche
<br>

Betrachte die Funktion $f(x_0, x_1) = x_0^4 + x_1^4 + x_0 x_1^3 - x_0^2 x_1 - x_1^2$.

::::{.content-visible unless-format="pdf"}

Hier wird diese Funktion einmal als 2-dimensionale Fläche und einmal als Konturlinien Plot dargestellt. Im linken Feld kann man den Startpunkt und $\lambda$ wählen und die ersten 30 Iterationen darstellen.

:::::{.fig-AbstandEllipseGerade}

<iframe scrolling="no" title="Gradient descent 2d" src="https://www.geogebra.org/material/iframe/id/c2zjuxef/width/700/height/600/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/true/rc/false/ld/false/sdz/true/ctl/false" width="700px" height="600px" style="border:0px;"> </iframe>

:::::

Man erkennt, dass die Funktion drei lokale Minima hat.

::::

Nun wollen wir die lokalen Minima mit Hilfe der Gradient Descent Methode berechnen. Damit wir $\lambda \cdot (\nabla f)$ als `lam * df` berechnen können, wandeln wir die Liste, die wir mit `getGradient` erhalten, in ein `numpy`-array um. Die oben beschriebene Iteration wird so lange ausgeführt, bis $|\vec{x}_{n+1}-\vec{x}_n|\le 10^{-6}$ ist.

```{python}
#| code-fold: show
from floataad import float2FloatAad, getGradient
import numpy as np

def f(x):
    y = x[0]**4 + x[1]**4 + x[0] * x[1]**3
    y = y - x[0]**2 * x[1] - x[1]**2
    return y

getValues = np.vectorize(lambda y : y.value)

# Startwert und Lambda für Gradient Descent
x0 = [0.5, 0]
lam = 0.1
tol = 1e-6 # Toleranz für Abbruchbedingung

x0 = float2FloatAad(x0)
y0 = f(x0)
dy = np.array(getGradient(x0, y0))

# Erster Schritt
x1 = x0 - lam * dy

# Iteration bis die Distanz zwischen zwei
# aufeinanderfolgenden Punkten kleiner ist als tol.
while np.linalg.norm(getValues(x0) - getValues(x1)) > tol:
    x0 = x1
    y0 = f(x0)
    dy = np.array(getGradient(x0, y0))
    x1 = x0 - lam * dy

print("Lokales Minimum gefunden in der Nähe von")
print(getValues(x1))
```

:::
---

Die folgenden drei Abschnitte beschreiben verschiedene Anwendungen des Gradient Descent Verfahrens, die wir mit Hilfe von `FloatAad` programmieren können.

### Lineare Regression



### Bilder schärfen


### Ein einaches neuronales Netz